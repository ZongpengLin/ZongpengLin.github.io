<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>冯·诺依曼体系结构</title>
    <url>/posts/9ece874e/</url>
    <content><![CDATA[<h1 id="“存储程序”的计算机金字塔"><a href="#“存储程序”的计算机金字塔" class="headerlink" title="“存储程序”的计算机金字塔"></a>“存储程序”的计算机金字塔</h1><hr>
<h2 id="早期的计算机组成"><a href="#早期的计算机组成" class="headerlink" title="早期的计算机组成"></a>早期的计算机组成</h2><ul>
<li>早期的基本硬件组成：CPU、内存和主板。 </li>
<li>存放在内存中的程序和数据，需要被CPU读取，CPU 计算完之后，还要把数据写回到内存</li>
<li>主板是一个有着各种各样，有时候多达数十乃至上百个插槽的配件。我们的 CPU 要插在主板上，内存也要插在主板上。主板的<strong>芯片组</strong>（Chipset）和<strong>总线</strong>（Bus）解决了 CPU 和内存之间如何通信的问题。<span id="more"></span>芯片组控制了数据传输的流转，也就是数据从哪里到哪里的问题。总线则是实际数据传输的高速公路。因此，总线速度（Bus Speed）决定了数据能传输得多快。</li>
</ul>
 <img src="/img/computer_img/主板各种插槽.webp" alt="主板各种插槽" style="zoom: 75%;"/>  
- 有了以上三件，只要配上电源，计算机差不多就可以跑起来了，现在还缺少的是各类输入（Input）/ 输出（Output）设备，也就是 *I/O 设备*。最后，还可以配上一个硬盘。这样各种数据才能持久地保存下来。
- 另外，显卡（Graphics Card）对于使用图形界面操作系统的计算机，无论是 Windows、Mac OS 还是 Linux，都是必不可少的。显卡之所以特殊，是因为显卡里有除了 CPU 之外的另一个“处理器”，也就是 GPU（Graphics Processing Unit，图形处理器），GPU 一样可以做各种“计算”的工作。

<img src="/img/computer_img/主板设计图.webp" alt="主板设计图" style="zoom: 33%;" />

<h2 id="主板"><a href="#主板" class="headerlink" title="主板"></a>主板</h2><p>鼠标、键盘以及硬盘，这些都是插在主板上的。作为外部 I&#x2F;O 设备，它们是通过主板上的南桥（SouthBridge）芯片组，来控制和 CPU 之间的通信的。“南桥”芯片的名字很直观，一方面，它在主板上的位置，通常在主板的“南面”。另一方面，它的作用就是作为“桥”，来连接鼠标、键盘以及硬盘这些外部设备和 CPU 之间的通信。</p>
<p>有了南桥，自然对应着也有“北桥”。是的，以前的主板上通常也有“北桥”芯片，用来作为“桥”，连接 CPU 和内存、显卡之间的通信。不过，随着时间的变迁，现在的主板上的“北桥”芯片的工作，已经被移到了 CPU 的内部，所以你在主板上，已经看不到北桥芯片了。</p>
<h2 id="冯·诺依曼体系结构"><a href="#冯·诺依曼体系结构" class="headerlink" title="冯·诺依曼体系结构"></a>冯·诺依曼体系结构</h2><p>类似的手机也有这样的组成。我们手机里只有 SD 卡（Secure Digital Memory Card）这样类似硬盘功能的存储卡插槽，并没有内存插槽、CPU 插槽这些东西。这是由于尺寸的原因，手机制造商们选择把 CPU、内存、网络通信，乃至摄像头芯片，都封装到一个芯片，然后再嵌入到手机主板上。这种方式叫 <strong>SoC</strong>，也就是 <strong>System on a Chip（系统芯片）</strong>。</p>
<p>无论是个人电脑、服务器、智能手机，还是 Raspberry Pi 这样的微型卡片机，都遵循着同一个“计算机”的抽象概念。这是怎么样一个“计算机”呢？这其实就是<strong>冯·诺依曼体系结构（Von Neumann architecture）</strong>，也叫存储程序计算机。包含了两个概念：一个是可编程，一个是存储。</p>
<p><strong>关于“存储程序计算机”的由来，可以参见冯在秘密开发的 EDVAC 写了一篇报告First Draft of a Report on the EDVAC，描述了他心目中的一台计算机应该长什么样。</strong></p>
<h2 id="计算机结构体系"><a href="#计算机结构体系" class="headerlink" title="计算机结构体系"></a>计算机结构体系</h2><ul>
<li>处理器：首先是一个包含算术逻辑单元（Arithmetic Logic Unit，ALU）和处理器寄存器（Processor Register）的处理器单元（Processing Unit），用来完成各种算术和逻辑运算。因为它能够完成各种数据的处理或者计算工作，因此也有人把这个叫作数据通路（Datapath）或者运算器。</li>
<li>控制器单元：一个包含指令寄存器（Instruction Register）和程序计数器（Program Counter）的控制器单元（Control Unit&#x2F;CU），用来控制程序的流程，通常就是不同条件下的分支和跳转。在现在的计算机里，<em><strong>上面的算术逻辑单元和这里的控制器单元，共同组成了我们说的 CPU。</strong></em></li>
<li>内存：接着是用来存储数据（Data）和指令（Instruction）的内存。以及更大容量的外部存储，在过去，可能是磁带、磁鼓这样的设备，现在通常就是硬盘。</li>
<li>I&#x2F;O 设备：最后就是各种输入和输出设备，以及对应的输入和输出机制。我们现在无论是使用什么样的计算机，其实都是和输入输出设备在打交道。</li>
</ul>
<p><img src="/img/computer_img/%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84%E7%A4%BA%E6%84%8F%E5%9B%BE.webp" alt="体系结构示意图"></p>
<h2 id="延伸：图灵机"><a href="#延伸：图灵机" class="headerlink" title="延伸：图灵机"></a>延伸：图灵机</h2><p>冯·诺依曼机侧重于硬件抽象，而图灵机侧重于计算抽象</p>
<ul>
<li>图灵机是一种思想模型，是一种有穷的、构造性的问题的问题求解思路，图s灵认为凡事能用算法解决的问题也一定能用图灵机解决；</li>
<li>阿兰图灵确定了利用当代计算机的计算极限，即冯诺依曼机只能解决 1.有解的可计算的数学问题  2.能够在有限步骤得到解，为软件便成确立的问题领域边界。</li>
<li>相比而言，冯诺依曼机侧重于”程序存储”与”二进制执行”，并提出实现这两点必须的五个部分:控制器，处理器，存储单元，输入和输出设备。这个体系基本奠定了近现代计算机的硬件基础。</li>
</ul>
]]></content>
      <categories>
        <category>计算机组成原理</category>
      </categories>
      <tags>
        <tag>计算机体系</tag>
      </tags>
  </entry>
  <entry>
    <title>关于性能提升</title>
    <url>/posts/bc93e288/</url>
    <content><![CDATA[<h1 id="穿越功耗墙，我们该从哪些方面提升“性能”？"><a href="#穿越功耗墙，我们该从哪些方面提升“性能”？" class="headerlink" title="穿越功耗墙，我们该从哪些方面提升“性能”？"></a>穿越功耗墙，我们该从哪些方面提升“性能”？</h1><center> ` 程序的 CPU 执行时间 = 指令数 × CPI × Clock Cycle Time `</center>

<h2 id="功耗：CPU-的“人体极限”"><a href="#功耗：CPU-的“人体极限”" class="headerlink" title="功耗：CPU 的“人体极限”"></a>功耗：CPU 的“人体极限”</h2><p>案例：奔腾 4 的 CPU 主频从来没有达到过 10GHz，最终它的主频上限定格在 3.8GHz。这还不是最糟的，更糟糕的事情是，大家发现，奔腾 4 的主频虽然高，但是它的实际性能却配不上同样的主频。想要用在笔记本上的奔腾 4 2.4GHz 处理器，其性能只和基于奔腾 3 架构的奔腾 M 1.6GHz 处理器差不多。</p>
<span id="more"></span>
<img src="https://static001.geekbang.org/resource/image/18/80/1826102a89e4cdd31f7573db53dd9280.png?wh=756*468" alt="CPU的主频变化" style="zoom: 75%;"/>

<p>2019 年的最高配置 Intel i9 CPU，主频也只不过是 5GHz 而已。相较于 1978 年到 2000 年，这 20 年里 300 倍的主频提升，从 2000 年到现在的这 19 年，CPU 的主频大概提高了 3 倍。 </p>
<p>奔腾 4 的主频为什么没能超过 3.8GHz 的障碍呢？答案就是功耗问题。什么是功耗问题呢？</p>
<p>(一个 3.8GHz 的奔腾 4 处理器，满载功率是 130 瓦。这个 130 瓦是什么概念呢？机场允许带上飞机的充电宝的容量上限是 100 瓦时。如果我们把这个 CPU 安在手机里面，不考虑屏幕内存之类的耗电，这个 CPU 满载运行 45 分钟，充电宝里面就没电了。而 iPhone X 使用 ARM 架构的 CPU，功率则只有 4.5 瓦左右。)</p>
<p>我们的 CPU，一般都被叫作超大规模集成电路（Very-Large-Scale Integration，VLSI）。这些电路，实际上都是一个个晶体管组合而成的。CPU 在计算，其实就是让晶体管里面的“开关”不断地去“打开”和“关闭”，来组合完成各种运算和功能。</p>
<p>想要计算的快，一方面，我们要在 CPU 里，同样的面积里面，多放一些晶体管，也就是增加密度；另一方面，我们要让晶体管“打开”和“关闭”得更快一点，也就是提升主频。而这两者，都会增加功耗，带来耗电和散热的问题。</p>
<p>在 CPU 里面，能够放下的晶体管数量和晶体管的“开关”频率也都是有限的。一个CPU 的公路，可以用一个公式表示：</p>
<center> 功耗 ~= 1/2 ×负载电容×电压的平方×开关频率×晶体管数量</center>

<p>那么，为了要提升性能，我们需要不断地增加晶体管数量。同样的面积下，我们想要多放一点晶体管，就要把晶体管造得小一点。这个就是平时我们所说的提升“制程”。从 28nm 到 7nm，相当于晶体管本身变成了原来的 1&#x2F;4 大小。我们还要提升主频，让开关的频率变快。 但是，功耗增加太多，就会导致 CPU 散热跟不上，这时，我们就需要降低电压。这里有一点非常关键，在整个功耗的公式里面，功耗和电压的平方是成正比的。这意味着电压下降到原来的 1&#x2F;5，整个的功耗会变成原来的 1&#x2F;25 。</p>
<h2 id="并行优化，理解阿姆达尔定律"><a href="#并行优化，理解阿姆达尔定律" class="headerlink" title="并行优化，理解阿姆达尔定律"></a>并行优化，理解阿姆达尔定律</h2><p>就是所谓的“吞吐率”变大。所以，不管你有没有需要，现在 CPU 的性能就是提升了 2 倍乃至 8 倍、16 倍。这也是一个最常见的提升性能的方式，<strong>通过并行提高性能。</strong></p>
<p>通过并行提高性能来解决。如果想要使用这种思想，需要满足这样几个条件。</p>
<ol>
<li>需要进行的计算，本身可以分解成几个可以并行的任务。好比上面的乘法和加法计算，几个人可以同时进行，不会影响最后的结果。</li>
<li>需要能够分解好问题，并确保几个人的结果能够汇总到一起。</li>
<li>在“汇总”这个阶段，是没有办法并行进行的，还是得顺序执行，一步一步来。</li>
</ol>
<p>这就引出了阿姆达尔定律（Amdahl’s Law）：</p>
<p>这个定律说的就是，对于一个程序进行优化之后，处理器并行运算之后效率提升的情况。具体可以用这样一个公式来表示：</p>
<center> **优化后的执行时间 = 受优化影响的执行时间 / 加速倍数 + 不受影响的执行时间** </center>

<p>比如上面的各个向量的一小段的点积，需要 100ns，加法需要 20ns，总共需要 120ns。这里通过并行 4 个 CPU 有了 4 倍的加速度。那么最终优化后，就有了 100&#x2F;4+20&#x3D;45ns。即使我们增加更多的并行度来提供加速倍数，比如有 100 个 CPU，整个时间也需要 100&#x2F;100+20&#x3D;21ns。</p>
<p><img src="https://static001.geekbang.org/resource/image/f1/e5/f1d05ec439e6377803df741bc07b09e5.jpeg?wh=3140*2039"></p>
<p>T1就是不受影响的执行时间 ，T2就是可以优化影响的执行时间</p>
<h2 id="延伸："><a href="#延伸：" class="headerlink" title="延伸："></a>延伸：</h2><p>在“摩尔定律”和“并行计算”之外，在整个计算机组成层面，还有这样几个原则性的性能提升方法。</p>
<ol>
<li><p>加速大概率事件。最典型的就是，过去几年流行的深度学习，整个计算过程中，99% 都是向量和矩阵计算，于是，工程师们通过用 GPU 替代 CPU，大幅度提升了深度学习的模型训练过程。本来一个 CPU 需要跑几小时甚至几天的程序，GPU 只需要几分钟就好了。Google 更是不满足于 GPU 的性能，进一步地推出了 TPU。</p>
</li>
<li><p>通过流水线提高性能。现代的工厂里的生产线叫“流水线”。我们可以把装配 iPhone 这样的任务拆分成一个个细分的任务，让每个人都只需要处理一道工序，最大化整个工厂的生产效率。类似的，我们的 CPU 其实就是一个“运算工厂”。我们把 CPU 指令执行的过程进行拆分，细化运行，也是现代 CPU 在主频没有办法提升那么多的情况下，性能仍然可以得到提升的重要原因之一。</p>
</li>
<li><p>通过预测提高性能。通过预先猜测下一步该干什么，而不是等上一步运行的结果，提前进行运算，也是让程序跑得更快一点的办法。典型的例子就是在一个循环访问数组的时候，凭经验，你也会猜到下一步我们会访问数组的下一项。所谓的“分支和冒险”、“局部性原理”这些 CPU 和存储系统设计方法，其实都是在利用我们对于未来的“预测”，提前进行相应的操作，来提升我们的程序性能。</p>
</li>
</ol>
<p>比如：</p>
<p>1.加速大概率事件<br>各种缓存(内存缓存、CDN缓存)<br>2.流水线<br>并发编程、异步编程<br>音视频播放器边播放边缓冲<br>3.预测<br>小说的下一页预加载<br>电商大促的CDN预热</p>
]]></content>
      <categories>
        <category>计算机组成原理</category>
      </categories>
      <tags>
        <tag>性能与主频</tag>
      </tags>
  </entry>
  <entry>
    <title>CPU的主频</title>
    <url>/posts/5dfefcf1/</url>
    <content><![CDATA[<h1 id="透过CPU的主频，谈谈性能究竟是什么"><a href="#透过CPU的主频，谈谈性能究竟是什么" class="headerlink" title="透过CPU的主频，谈谈性能究竟是什么"></a>透过CPU的主频，谈谈性能究竟是什么</h1><h2 id="计算机的性能衡量"><a href="#计算机的性能衡量" class="headerlink" title="计算机的性能衡量"></a>计算机的性能衡量</h2><p>于计算机的性能，我们需要有个标准来衡量。这个标准中主要有两个指标。</p>
<p>一、响应时间（Response time）或者叫执行时间（Execution time）。想要提升响应时间这个性能指标，可以理解为让计算机“跑得更快”。</p>
<p>二、吞吐率（Throughout）或者带宽（Bandwidth），想要理解这个指标，可以理解为让计算机“搬得更多”。</p>
<span id="more"></span>
<p><img src="https://static001.geekbang.org/resource/image/27/27/27cab77c0eec95ec29792e6c3d093d27.png?wh=1142*300" alt="网络带宽通常就是一个吞吐率的性能指标"><br><br><br>响应时间指的就是，我们执行一个程序，到底需要花多少时间。花的时间越少，自然性能就越好。而吞吐率是指我们在一定的时间范围内，到底能处理多少事情。这里的“事情”，在计算机里就是处理的数据或者执行的程序指令。和搬东西来做对比，如果我们的响应时间短，跑得快，我们可以来回多跑几趟多搬几趟。所以说，缩短程序的响应时间，一般来说都会提升吞吐率。</p>
<p>除了缩短响应时间，我们还有别的方法吗？当然有，比如说，我们还可以多找几个人一起来搬，这就类似现代的服务器都是 8 核、16 核的。人多力量大，同时处理数据，在单位时间内就可以处理更多数据，吞吐率自然也就上去了。</p>
<p>提升吞吐率的办法有很多。大部分时候，我们只要多加一些机器，多堆一些硬件就好了。但是响应时间的提升却没有那么容易。</p>
<p><strong>一般把性能，定义成响应时间的倒数，也就是：</strong></p>
<center> 性能 = 1/ 响应时间    </center>


<h2 id="响应时间"><a href="#响应时间" class="headerlink" title="响应时间"></a>响应时间</h2><p>在业界，各大 CPU 和服务器厂商组织了一个叫作 SPEC（Standard Performance Evaluation Corporation）的第三方机构，专门用来指定各种“跑分”的规则。</p>
<img src="https://static001.geekbang.org/resource/image/a5/22/a50a6cb9d3df027aeda5ee8e53b75422.png?wh=1142*929" alt="SPEC基准测试" style="zoom:50%;" />
SPEC 提供的 CPU 基准测试程序，就好像 CPU 届的“高考”，通过数十个不同的计算程序，对于 CPU 的性能给出一个最终评分。[链接🔗](https://www.spec.org/cpu2017/results/cpu2017.html)

<h2 id="计算机的计时单位：CPU-时钟"><a href="#计算机的计时单位：CPU-时钟" class="headerlink" title="计算机的计时单位：CPU 时钟"></a>计算机的计时单位：CPU 时钟</h2><p>虽然时间是一个很自然的用来衡量性能的指标，但是用时间来衡量时，有两个问题。</p>
<p>第一个就是时间不“准”。如果用你自己随便写的一个程序，来统计程序运行的时间，每一次统计结果不会完全一样。有可能这一次花了 45ms，下一次变成了 53ms。为什么会不准呢？这里面有好几个原因。首先，我们统计时间是用类似于“掐秒表”一样，记录程序运行结束的时间减去程序开始运行的时间。这个时间也叫 Wall Clock Time 或者 Elapsed Time，就是在运行程序期间，挂在墙上的钟走掉的时间。</p>
<p>但是，计算机可能同时运行着好多个程序，CPU 实际上不停地在各个程序之间进行切换。在这些走掉的时间里面，很可能 CPU 切换去运行别的程序了。而且，有些程序在运行的时候，可能要从网络、硬盘去读取数据，要等网络和硬盘把数据读出来，给到内存和 CPU。所以说，要想准确统计某个程序运行时间，进而去比较两个程序的实际性能，我们得把这些时间给刨除掉。</p>
<p>Linux 下有一个叫 time 的命令，可以帮我们统计出来，同样的 Wall Clock Time 下，程序实际在 CPU 上到底花了多少时间。</p>
<p>运行一下 time 命令。它会返回三个值，第一个是 real time，也就是我们说的 Wall Clock Time，也就是运行程序整个过程中流逝掉的时间；第二个是 user time，也就是 CPU 在运行你的程序，在用户态运行指令的时间；第三个是 sys time，是 CPU 在运行你的程序，在操作系统内核里运行指令的时间。而程序实际花费的 CPU 执行时间（CPU Time），就是 user time 加上 sys time。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">$ time seq 1000000 | wc -l</span><br><span class="line">1000000</span><br><span class="line"></span><br><span class="line">real  0m0.101s</span><br><span class="line">user  0m0.031s</span><br><span class="line">sys   0m0.016s</span><br></pre></td></tr></table></figure>

<p><strong>即使我们已经拿到了 CPU 时间，我们也不一定可以直接“比较”出两个程序的性能差异。</strong>因为同一台计算机上，CPU 可能满载运行也可能降频运行，降频运行的时候自然花的时间会多一些。       除了 CPU 之外，时间这个性能指标还会受到主板、内存这些其他相关硬件的影响。</p>
<p>我们把程序的 CPU 执行时间变成 CPU 时钟周期数（CPU Cycles）和 时钟周期时间（Clock Cycle）的乘积。</p>
<p>​                        程序的 CPU 执行时间 &#x3D;CPU 时钟周期数×时钟周期时间</p>
<p>什么是时钟周期时间。你在买电脑的时候，一定关注过 CPU 的主频。比如这台电脑是 Intel Core-i7-7700HQ 2.8GHz，这里的 2.8GHz 就是电脑的主频（Frequency&#x2F;Clock Rate）。这个 2.8GHz，我们可以先粗浅地认为，CPU 在 1 秒时间内，可以执行的简单指令的数量是 2.8G 条。<em><strong>准确一点描述就是，这个主频就是我们 CPU 的一个“钟表”能够识别出来的最小的时间间隔</strong></em> 。</p>
<p>在 CPU 内部，和我们平时戴的电子石英表类似，有一个叫晶体振荡器（Oscillator Crystal）的东西，简称为晶振。我们把晶振当成 CPU 内部的电子表来使用。晶振带来的每一次“滴答”，就是时钟周期时间。 <em>（如果组装过台式机的话，可能听说过“超频”这个概念，这说的其实就相当于把买回来的 CPU 内部的钟给调快了，于是 CPU 的计算跟着这个时钟的节奏，也就自然变快了。当然这个快不是没有代价的，CPU 跑得越快，散热的压力也就越大。就和人一样，超过生理极限，CPU 就会崩溃了。)</em><br><br/></p>
<h2 id="CPU-时钟周期数"><a href="#CPU-时钟周期数" class="headerlink" title="CPU 时钟周期数"></a>CPU 时钟周期数</h2><p>对于 CPU 时钟周期数，我们可以再做一个分解，把它变成“指令数×每条指令的平均时钟周期数（Cycles Per Instruction，简称 CPI）”</p>
<p>不同的指令需要的 Cycles 是不同的，加法和乘法都对应着一条 CPU 指令，但是乘法需要的 Cycles 就比加法要多，自然也就慢。</p>
<p>拆分了之后，我们的程序的 CPU 执行时间就可以变成这样三个部分的乘积。</p>
<pre><code>程序的 CPU 执行时间 = 指令数 × CPI × Clock Cycle Time 
</code></pre>
<p><strong>因此，想要解决性能问题，其实就是要优化这三者。</strong></p>
<blockquote>
<ol>
<li><p>时钟周期时间，就是计算机主频，这个取决于计算机硬件。我们所熟知的 摩尔定律 就一直在不停地提高我们计算机的主频。比如说，我最早使用的 80386 主频只有 33MHz，现在手头的笔记本电脑就有 2.8GHz，在主频层面，就提升了将近 100 倍。</p>
</li>
<li><p>每条指令的平均时钟周期数 CPI，就是一条指令到底需要多少 CPU Cycle。在后面讲解 CPU 结构的时候，我们会看到，现代的 CPU 通过流水线技术（Pipeline），让一条指令需要的 CPU Cycle 尽可能地少。因此，对于 CPI 的优化，也是计算机组成和体系结构中的重要一环。</p>
</li>
<li><p>指令数，代表执行我们的程序到底需要多少条指令、用哪些指令。这个很多时候就把挑战交给了编译器。同样的代码，编译成计算机指令时候，就有各种不同的表示方式。</p>
</li>
</ol>
</blockquote>
<p>形象比喻： </p>
<ul>
<li>我们可以把自己想象成一个 CPU，坐在那里写程序。计算机主频就好像是你的打字速度，打字越快，你自然可以多写一点程序。CPI 相当于你在写程序的时候，熟悉各种快捷键，越是打同样的内容，需要敲击键盘的次数就越少。指令数相当于你的程序设计得够合理，同样的程序要写的代码行数就少。如果三者皆能实现，你自然可以很快地写出一个优秀的程序，你的“性能”从外面来看就是好的。</li>
</ul>
<h2 id="延伸"><a href="#延伸" class="headerlink" title="延伸"></a>延伸</h2><p>每次有新手机发布的时候，总会有一些对于手机的跑分结果的议论。乃至于有“作弊”跑分或者“针对跑分优化”的说法。我们能针对“跑分”作弊么？怎么做到呢？</p>
<ul>
<li>当检测到跑分程序运行的时候，降低系统调用，提高跑分程序优先级。关闭热管理系统(防止过热关核降频)，手机CPU核心全开，超频到最高等。不顾一切，全心全意为跑分程序服务</li>
<li>提高跑分，无非是优化CPU的执行时间，可以从两个方面入手，一是提高CPI，可以采取超频运行的模式；二是优化指令数，单独针对特定的CPU进行代码优化。</li>
</ul>
]]></content>
      <categories>
        <category>计算机组成原理</category>
      </categories>
      <tags>
        <tag>性能与主频</tag>
      </tags>
  </entry>
  <entry>
    <title>初窥计算机指令</title>
    <url>/posts/d754f544/</url>
    <content><![CDATA[<h1 id="初窥计算机指令"><a href="#初窥计算机指令" class="headerlink" title="初窥计算机指令"></a>初窥计算机指令</h1><h2 id="在软硬件接口中，CPU-帮我们做了什么事？"><a href="#在软硬件接口中，CPU-帮我们做了什么事？" class="headerlink" title="在软硬件接口中，CPU 帮我们做了什么事？"></a>在软硬件接口中，CPU 帮我们做了什么事？</h2><ul>
<li><p>从硬件角度来看，CPU就是一个超大规模集成电路<span id="more"></span>，通过电路实现了加法、乘法乃至各种各样的处理逻辑。</p>
</li>
<li><p>从软件上来看，CPU 就是一个执行各种计算机指令的逻辑机器。这里的计算机指令，就好比一门 CPU 能够听得懂的语言，我们也可以把它叫作机器语言（Machine Language）。</p>
</li>
</ul>
<p>不同的 CPU 能够听懂的语言不太一样。比如，我们的个人电脑用的是 Intel 的 CPU，苹果手机用的是 ARM 的 CPU。这两者能听懂的语言就不太一样。类似这样两种 CPU 各自支持的语言，就是两组不同的<strong>计算机指令集</strong>，英文叫 Instruction Set。<br>一个计算机程序实际上是由成千上万条指令组成的，但是CPU里不能一直放着所有指令，所以计算机程序平时是存储在存储器中的，这种程序指令存储在存储器里面的计算机，我们就叫作存储程序型计算机（Stored-program Computer）。</p>
<h2 id="从编译到汇编，代码怎么变成机器码？"><a href="#从编译到汇编，代码怎么变成机器码？" class="headerlink" title="从编译到汇编，代码怎么变成机器码？"></a>从编译到汇编，代码怎么变成机器码？</h2><p>平时编写的代码，到底是怎么变成一条条计算机指令，最后被 CPU 执行的呢？我们拿一小段真实的 C 语言程序来看看。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">// test.c</span><br><span class="line">int main()</span><br><span class="line">&#123;</span><br><span class="line">  int a = 1; </span><br><span class="line">  int b = 2;</span><br><span class="line">  a = a + b;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>要让这段程序在一个 Linux 操作系统上跑起来，我们需要把整个程序翻译成一个汇编语言（ASM，Assembly Language）的程序，这个过程我们一般叫编译（Compile）成汇编代码。</p>
<p>正对汇编代码，我们可以再用汇编器（Assember）翻译成机器码（Machine Code）。这些机器码由“0”和“1”组成的机器语言表示。这一条条机器码，就是一条条的计算机指令。这样一串串的 16 进制数字，就是我们 CPU 能够真正认识的计算机指令。</p>
<p>在一个 Linux 操作系统上，我们可以简单地使用 <strong>gcc</strong> 和 <strong>objdump</strong> 这样两条命令，把对应的汇编代码和机器码都打印出来。</p>
<p>可以看到，左边对应的是一条条机器码，右边有一系列的 push、mov、add、pop 等，这些就是对应的汇编代码。</p>
<img src="/img/computer_img/机器码和汇编.png" alt="机器码和汇编" style="zoom:50%;" />

<center style:"color:C0C0C0">机器码和汇编代码</center>

<h2 id=""><a href="#" class="headerlink" title=""></a><img src="/img/computer_img/汇编过程.webp" alt="汇编过程" style="zoom:50%;" /></h2><p>从高级语言到汇编代码，再到机器码，就是一个日常开发程序，最终变成了CPU 可以执行的计算机指令的过程。</p>
<h2 id="常见指令"><a href="#常见指令" class="headerlink" title="常见指令"></a>常见指令</h2><ol>
<li>算数类指令。我们的加减乘除，在 CPU 层面，都会变成一条条算术类指令。</li>
<li>数据传输类指令。给变量赋值、在内存里读写数据，用的都是数据传输类指令。</li>
<li>逻辑类指令。逻辑上的与或非，都是这一类指令。</li>
<li>条件分支类指令。比如“if&#x2F;else”</li>
<li>无条件条转指令。写一些大一点的程序，我们常常需要写一些函数或者方法。在调用函数的时候，其实就是发起了一个无条件跳转指令。</li>
</ol>
<p>&lt;img src&#x3D;”&#x2F;img&#x2F;computer_img&#x2F;各种指令.webp style&#x3D;”zoom:67%;)</p>
<center style="color:#C0C0C0;">常见指令</center>

<h2 id="MIPS-指令集"><a href="#MIPS-指令集" class="headerlink" title="MIPS 指令集"></a>MIPS 指令集</h2><img src="/img/computer_img/MIPS位数.webp" alt="MIPS位数" style="zoom:67%;" />

<p>MIPS 的指令是一个 32 位的整数，高 6 位叫操作码（Opcode），也就是代表这条指令具体是一条什么样的指令，剩下的 26 位有三种格式，分别是 R、I 和 J。</p>
<p><strong>R指令</strong>一般用来算术和逻辑操作，里面有读取和写入数据的寄存器的地址。如果是逻辑位移操作，后面还有位移操作的位移量，而最后的功能码，则是在前面的操作码不够的时候，扩展操作码表示对应的具体指令的。</p>
<p><strong>I 指令</strong>，则通常是用数据传输、条件分支，以及在运算的时候使用的并非变量还是常数的时候。这个时候，没有了位移量和操作码，也没有了第三个寄存器，而是把这三部分直接合并成了一个地址值或者一个常数。</p>
<p><strong>J 指令</strong>就是一个跳转指令，高 6 位之外的 26 位都是一个跳转后的地址。</p>
<p>我以一个简单的加法算术指令 add t0,s1, $s2, 为例，</p>
<p>​                                <code>add  $t0,$s2,$s1</code> </p>
<p>对应的 MIPS 指令里 opcode 是 0，rs 代表第一个寄存器 s1 的地址是 17，rt 代表第二个寄存器 s2 的地址是 18，rd 代表目标的临时寄存器 t0 的地址，是 8。因为不是位移操作，所以位移量是 0。把这些数字拼在一起，就变成了一个 MIPS 的加法指令。</p>
<p>为了读起来方便，我们一般把对应的二进制数，用 16 进制表示出来。在这里，也就是 0X02324020。这个数字也就是这条指令对应的机器码。</p>
 <img src="/img/computer_img/指令表示.webp" alt="指令表示" style="zoom:50%;" />

<p>为了读起来方便，我们一般把对应的二进制数，用 16 进制表示出来。在这里，也就是 0X02324020。这个数字也就是这条指令对应的机器码。</p>
]]></content>
      <categories>
        <category>计算机组成原理</category>
      </categories>
      <tags>
        <tag>CPU</tag>
        <tag>指令集</tag>
        <tag>机器码</tag>
      </tags>
  </entry>
  <entry>
    <title>函数调用</title>
    <url>/posts/f4b9b599/</url>
    <content><![CDATA[<h1 id="函数调用：为什么会发生stack-overflow？"><a href="#函数调用：为什么会发生stack-overflow？" class="headerlink" title="函数调用：为什么会发生stack overflow？"></a>函数调用：为什么会发生stack overflow？</h1><p>Stack Overflow 的名字来自于一个常见的报错，就是栈溢出（stack overflow）。</p>
<p>从程序的函数调用开始<span id="more"></span>，讲讲函数间的相互调用，在计算机指令层面是怎么实现的，以及什么情况下会发生栈溢出这个错误。</p>
<h2 id="为什么我们需要程序栈"><a href="#为什么我们需要程序栈" class="headerlink" title="为什么我们需要程序栈"></a>为什么我们需要程序栈</h2><p>我们还是从一个非常简单的 C 程序 function_example.c 看起。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">// function_example.c</span><br><span class="line"></span><br><span class="line">#include &lt;stdio.h&gt;</span><br><span class="line">int static add(int a, int b)</span><br><span class="line">&#123;</span><br><span class="line">    return a+b;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">int main()</span><br><span class="line">&#123;</span><br><span class="line">    int x = 5;</span><br><span class="line">    int y = 10;</span><br><span class="line">    int u = add(x, y);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>这个程序定义了一个简单的函数 add，接受两个参数 a 和 b，返回值就是 a+b。而 main 函数里则定义了两个变量 x 和 y，然后通过调用这个 add 函数，来计算 u&#x3D;x+y，最后把 u 的数值打印出来。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ gcc -g -c function_example.c</span><br><span class="line">$ objdump -d -M intel -S function_example.o</span><br></pre></td></tr></table></figure>

<p>我们把这个程序编译之后，objdump 出来。我们来看一看对应的汇编代码。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">int static add(int a, int b)</span><br><span class="line">&#123;</span><br><span class="line">   0:   55                      push   rbp</span><br><span class="line">   1:   48 89 e5                mov    rbp,rsp</span><br><span class="line">   4:   89 7d fc                mov    DWORD PTR [rbp-0x4],edi</span><br><span class="line">   7:   89 75 f8                mov    DWORD PTR [rbp-0x8],esi</span><br><span class="line">    return a+b;</span><br><span class="line">   a:   8b 55 fc                mov    edx,DWORD PTR [rbp-0x4]</span><br><span class="line">   d:   8b 45 f8                mov    eax,DWORD PTR [rbp-0x8]</span><br><span class="line">  10:   01 d0                   add    eax,edx</span><br><span class="line">&#125;</span><br><span class="line">  12:   5d                      pop    rbp</span><br><span class="line">  13:   c3                      ret    </span><br><span class="line">0000000000000014 &lt;main&gt;:</span><br><span class="line">int main()</span><br><span class="line">&#123;</span><br><span class="line">  14:   55                      push   rbp</span><br><span class="line">  15:   48 89 e5                mov    rbp,rsp</span><br><span class="line">  18:   48 83 ec 10             sub    rsp,0x10</span><br><span class="line">    int x = 5;</span><br><span class="line">  1c:   c7 45 fc 05 00 00 00    mov    DWORD PTR [rbp-0x4],0x5</span><br><span class="line">    int y = 10;</span><br><span class="line">  23:   c7 45 f8 0a 00 00 00    mov    DWORD PTR [rbp-0x8],0xa</span><br><span class="line">    int u = add(x, y);</span><br><span class="line">  2a:   8b 55 f8                mov    edx,DWORD PTR [rbp-0x8]</span><br><span class="line">  2d:   8b 45 fc                mov    eax,DWORD PTR [rbp-0x4]</span><br><span class="line">  30:   89 d6                   mov    esi,edx</span><br><span class="line">  32:   89 c7                   mov    edi,eax</span><br><span class="line">  34:   e8 c7 ff ff ff          call   0 &lt;add&gt;</span><br><span class="line">  39:   89 45 f4                mov    DWORD PTR [rbp-0xc],eax</span><br><span class="line">  3c:   b8 00 00 00 00          mov    eax,0x0</span><br><span class="line">&#125;</span><br><span class="line">  41:   c9                      leave  </span><br><span class="line">  42:   c3                      ret    </span><br><span class="line">  </span><br></pre></td></tr></table></figure>

<p>可以看出，这里的跳转主要是函数调用的call指令。call指令后面跟着的仍然是跳转后的程序地址。</p>
<p>来看 add 函数。可以看到，add 函数编译之后，代码先执行了一条 push 指令和一条 mov 指令；在函数执行结束的时候，又执行了一条 pop 和一条 ret 指令。这四条指令的执行，其实就是在进行我们接下来要讲<strong>压栈（Push）</strong>和<strong>出栈（Pop）</strong>操作。</p>
<p>这与if…else 和 for&#x2F;while 循环的跳转是有区别的，if…else 和 for&#x2F;while 的跳转，是跳转走了就不再回来了，就在跳转后的新地址开始顺序地执行指令，而函数调用的跳转，在对应函数的指令执行完了之后，还要再回到函数调用的地方，继续执行 call 之后的指令。</p>
<p>可以把调用的函数指令直接插入调用函数的地方，替换掉对应的 c a l l 自立，然后在编译器编译代码时，直接就把函数调用变成对应的指令替换掉。然而，仔细想来，果函数 A 调用了函数 B，然后函数 B 再调用函数 A，我们就得面临在 A 里面插入 B 的指令，然后在 B 里面插入 A 的指令，这样就会产生无穷无尽地替换。就好像两面镜子面对面放在一块儿，任何一面镜子里面都会看到无穷多面镜子。</p>
<p>所以，把被调用函数的指令直接插入在调用处的方法行不通。可不可以专门设立一个“程序调用寄存器”，来 存储接下来要跳转回来执行的指令地址。 等到函数调用结束，从这寄存器里取出地址，再跳转到这个记录的地址，继续执行就好。</p>
<p>但是在多层函数调用里，简单只记录一个地址也是不够的。我们在调用函数 A 之后，A 还可以调用函数 B，B 还能调用函数 C。这一层又一层的调用并没有数量上的限制。在所有函数调用返回之前，每一次调用的返回地址都要记录下来，<strong>但是CPU 里的寄存器的数量并不多，</strong>像我们一般使用的 Intel i7 CPU 只有 16 个 64 位寄存器，调用的层数一多就存不下了。</p>
<p>在内存里面开辟一段空间，用栈这个<strong>后进先出（LIFO，Last In First Out）</strong>的数据结构。栈就像一个乒乓球桶，每次程序调用函数之前，我们都把调用返回后的地址写在一个乒乓球上，然后塞进这个球桶。这个操作其实就是我们常说的<strong>压栈</strong>。如果函数执行完了，我们就从球桶里取出最上面的那个乒乓球，很显然，这就是<strong>出栈</strong>。</p>
<img src="/img/computer_img/栈结构.webp" alt="栈结构" style="zoom:33%;" />

<p>在真实的程序里，压栈的不只有函数调用完成后的返回地址。比如函数 A 在调用 B 的时候，需要传输一些参数数据，这些参数数据在寄存器不够用的时候也会被压入栈中。整个函数 A 所占用的所有内存空间，就是函数 A 的<strong>栈帧（Stack Frame）</strong>。Frame 在中文里也有“相框”的意思，所以，每次到这里，我都有种感觉，整个函数 A 所需要的内存空间就像是被这么一个“相框”给框了起来，放在了栈里面。</p>
<img src="/img/computer_img/栈指令.webp" alt="栈指令" style="zoom:50%;" />

<p>图中，rbp 是 register base pointer 栈基址寄存器（栈帧指针），指向当前栈帧的栈底地址。rsp 是 register stack pointer 栈顶寄存器（栈指针），指向栈顶元素。</p>
<p>对应上面函数 add 的汇编代码，我们来仔细看看，main 函数调用 add 函数时，add 函数入口在 0～1 行，add 函数结束之后在 12～13 行。</p>
<p>我们在调用第 34 行的 call 指令时，会把当前的 PC 寄存器里的下一条指令的地址压栈，保留函数调用结束后要执行的指令地址。而 add 函数的第 0 行，push rbp 这个指令，就是在进行压栈。这里的 rbp 又叫栈帧指针（Frame Pointer），是一个存放了当前栈帧位置的寄存器。push rbp 就把之前调用函数，也就是 main 函数的栈帧的栈底地址，压到栈顶。</p>
<p>接着，第 1 行的一条命令 mov rbp, rsp 里，则是把 rsp 这个栈指针（Stack Pointer）的值复制到 rbp 里，而 rsp 始终会指向栈顶。这个命令意味着，rbp 这个栈帧指针指向的地址，变成当前最新的栈顶，也就是 add 函数的栈帧的栈底地址了。</p>
<p>而在函数 add 执行完成之后，又会分别调用第 12 行的 pop rbp 来将当前的栈顶出栈，这部分操作维护好了我们整个栈帧。然后，我们可以调用第 13 行的 ret 指令，这时候同时要把 call 调用的时候压入的 PC 寄存器里的下一条指令出栈，更新到 PC 寄存器中，将程序的控制权返回到出栈后的栈顶。</p>
<h2 id="如何构造一个-stack-overflow？"><a href="#如何构造一个-stack-overflow？" class="headerlink" title="如何构造一个 stack overflow？"></a>如何构造一个 stack overflow？</h2><p>通过引入栈，我们可以看到，无论有多少层的函数调用，或者在函数 A 里调用函数 B，再在函数 B 里调用 A，这样的递归调用，我们都只需要通过维持 rbp 和 rsp，这两个维护栈顶所在地址的寄存器，就能管理好不同函数之间的跳转。不过，栈的大小也是有限的。如果函数调用层数太多，我们往栈里压入它存不下的内容，程序在执行的过程中就会遇到栈溢出的错误，这就是大名鼎鼎的<strong>“stack  overflow”</strong>。</p>
<p>要构造一个栈溢出的错误并不困难，最简单的办法，就是我们上面说的 Infiinite Mirror Effect 的方式，让函数 A 调用自己，并且不设任何终止条件。这样一个无限递归的程序，在不断地压栈过程中，将整个栈空间填满，并最终遇上 stack overflow。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">比如： </span><br><span class="line">int a()</span><br><span class="line">&#123;</span><br><span class="line">  return a();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">int main()</span><br><span class="line">&#123;</span><br><span class="line">  a();</span><br><span class="line">  return 0;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="如何利用函数内联进行性能优化？"><a href="#如何利用函数内联进行性能优化？" class="headerlink" title="如何利用函数内联进行性能优化？"></a>如何利用函数内联进行性能优化？</h2><p>上面我们提到一个方法，把一个实际调用的函数产生的指令，直接插入到的位置，来替换对应的函数调用指令。尽管这个通用的函数调用方案，被我们否决了，但是如果被调用的函数里，没有调用其他函数，这个方法还是可以行得通的。</p>
<p>事实上，这就是一个常见的编译器进行自动优化的场景，我们通常叫<strong>函数内联（Inline）</strong>。我们只要在 GCC 编译的时候，加上对应的一个让编译器自动优化的参数 -O，编译器就会在可行的情况下，进行这样的指令替换。</p>
<p>来看这样一段代码</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">#include &lt;stdio.h&gt;</span><br><span class="line">#include &lt;time.h&gt;</span><br><span class="line">#include &lt;stdlib.h&gt;</span><br><span class="line"></span><br><span class="line">int static add(int a, int b)</span><br><span class="line">&#123;</span><br><span class="line">    return a+b;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">int main()</span><br><span class="line">&#123;</span><br><span class="line">    srand(time(NULL));</span><br><span class="line">    int x = rand() % 5</span><br><span class="line">    int y = rand() % 10;</span><br><span class="line">    int u = add(x, y)</span><br><span class="line">    printf(&quot;u = %d\n&quot;, u)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>在代码的最后加上将 u 通过 printf 打印出来的语句。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ gcc -g -c -O function_example_inline.c</span><br><span class="line">$ objdump -d -M intel -S function_example_inline.o</span><br></pre></td></tr></table></figure>

<p>面的 function_example_inline.c 的编译出来的汇编代码，没有把 add 函数单独编译成一段指令顺序，而是在调用 u &#x3D; add(x, y) 的时候，直接替换成了一个 add 指令。</p>
<p>除了依靠编译器的自动优化，你还可以在定义函数的地方，加上 inline 的关键字，来提示编译器对函数进行内联。</p>
<p>内联带来的优化： CPU需要执行的指令数变少了，根据地址跳转的过程不需要了，压栈和出栈的过程也不用了。</p>
<p>不过付出的代价是把可以复用的程序指令在调用它的地方完全展开了。如果一个函数在很多地方都被调用了，意味着整个程序占用的空间就会变大。</p>
<img src="/img/computer_img/叶子函数.webp" alt="叶子函数" style="zoom:33%;" />

<p>这样没有调用其他函数，只会被调用的函数，我们一般称之为叶子函数（或叶子过程）。</p>
<h2 id="总结延伸"><a href="#总结延伸" class="headerlink" title="总结延伸"></a>总结延伸</h2><p>主要讲了一个程序的函数间调用，在CPU指令层面是怎么执行的，其中程序栈这个概念一定要记着。</p>
<p>可以方便地通过压栈和出栈操作，使得程序在不同的函数调用过程中进行转移。而函数内联和栈溢出，一个是我们常常可以选择的优化方案，另一个则是我们会常遇到的程序 bug。</p>
<p>通过加入了程序栈，我们相当于在指令跳转的过程中，加入了一个“记忆”的功能，能在跳转去运行新的指令之后，再回到跳出去的位置，能够实现更加丰富和灵活的指令执行流程。这个为我们在程序开发的过程中，提供了“函数”这样一个抽象，使我们在程序开发的过程中，可以复用代码和指令。而不是简单粗暴的复制、粘贴。</p>
]]></content>
      <categories>
        <category>计算机组成原理</category>
      </categories>
      <tags>
        <tag>机器码</tag>
        <tag>stack overflow</tag>
        <tag>函数内联</tag>
      </tags>
  </entry>
  <entry>
    <title>指令跳转</title>
    <url>/posts/84302a9d/</url>
    <content><![CDATA[<h1 id="指令跳转：原来if…else就是goto"><a href="#指令跳转：原来if…else就是goto" class="headerlink" title="指令跳转：原来if…else就是goto"></a>指令跳转：原来if…else就是goto</h1><p>用到 if…else 这样的条件判断语句、while 和 for 这样的循环语句，还有函数<span id="more"></span>或者过程调用。对应的，CPU 执行也不只是一条指令，一般一个程序包含很多条指令，因为有 if…else、for 这样的条件和循环存在，这些指令也不会一路平铺直叙地执行下去。</p>
<h2 id="CPU-是如何执行指令的？"><a href="#CPU-是如何执行指令的？" class="headerlink" title="CPU 是如何执行指令的？"></a>CPU 是如何执行指令的？</h2><p>对于做软件开发来说，写好的代码变成指令之后，是一条一条顺序执行的。<br>先不管几百亿的晶体管的背后是怎么通过电路运转起来的，逻辑上，我们可以认为，CPU 其实就是由一堆寄存器组成的。而寄存器就是CPU 内部，由多个触发器（Flip-Flop）或者锁存器（Latches）组成的简单电路。</p>
<p>N 个触发器或者锁存器，就可以组成一个 N 位（Bit）的寄存器，能够保存 N 位的数据。比方说，用的64位 Intel服务器，寄存器就是64位的。</p>
<img src="/img/computer_img/CPU寄存器.webp" alt="CPU寄存器" style="zoom:25%;" />

<p>一个CPU 里会有多种不同功能的寄存器，主要介绍以下三种较为特殊的：</p>
<ol>
<li><p>PC寄存器，也称为指令地址寄存器（Instruction Address Register）。是用来存放下一条需要执行的计算机指令的内存地址。</p>
</li>
<li><p>指令寄存器，用来存放当前正在执行的指令。</p>
</li>
<li><p>条件寄存器，用里面的一个一个标记位（Flag）,存放CPU进行算数或者逻辑计算的结果。</p>
</li>
</ol>
<ul>
<li>除了这些特殊的寄存器，CPU 里还有更多用来存储数据和内存地址的寄存器。这样的寄存器通常一类里面不止一个。通常根据存放的数据内存来给他们取名，比如整数寄存器、浮点寄存器、向量寄存器和地址寄存器等。有些既可以存数据，也能存地址的就称之为通用寄存器。</li>
</ul>
<img src="/img/computer_img/常见寄存器.webp" alt="常见寄存器" style="zoom:33%;" />

<p>实际上，一个程序执行的时候，CPU 会根据PC寄存器里的地址，从内存里把需要执行的指令读取到指令寄存器里执行，然后根据指令长度递增，开始顺序读取下一条指令。可以看到，一个程序的一条条指令，在内存里是连续保存的，也是一条条顺序加载的。</p>
<p>而一些特殊指令，比如 J指令，会修改PC寄存器里的地址值。这样，下一条要执行的指令就不是从内存里顺序加载的了。事实上，这些跳转指令的存在，也是我们在写程序的时候，使用 if…else 条件语句和 while&#x2F;for 循环语句的原因。</p>
<h2 id="从-if…else-来看程序的执行和跳转"><a href="#从-if…else-来看程序的执行和跳转" class="headerlink" title="从 if…else 来看程序的执行和跳转"></a>从 if…else 来看程序的执行和跳转</h2><p>现在就来看一个包含 if…else 的简单程序。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">// test.c</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">#include &lt;time.h&gt;</span><br><span class="line">#include &lt;stdlib.h&gt;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">int main()</span><br><span class="line">&#123;</span><br><span class="line">  srand(time(NULL));</span><br><span class="line">  int r = rand() % 2;</span><br><span class="line">  int a = 10;</span><br><span class="line">  if (r == 0)</span><br><span class="line">  &#123;</span><br><span class="line">    a = 1;</span><br><span class="line">  &#125; else &#123;</span><br><span class="line">    a = 2;</span><br><span class="line">  &#125; </span><br></pre></td></tr></table></figure>

<p>我们用 rand 生成了一个随机数 r，r 要么是 0，要么是 1。当 r 是 0 的时候，我们把之前定义的变量 a 设成 1，不然就设成 2。使用 gcc 和 objdump 把这个程序编译成汇编代码。只关注于这里的 if…else 条件判断语句。对应的汇编代码是这样的：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">  if (r == 0)</span><br><span class="line">3b:   83 7d fc 00             cmp    DWORD PTR [rbp-0x4],0x0</span><br><span class="line">3f:   75 09                   jne    4a &lt;main+0x4a&gt;</span><br><span class="line">  &#123;</span><br><span class="line">      a = 1;</span><br><span class="line">41:   c7 45 f8 01 00 00 00    mov    DWORD PTR [rbp-0x8],0x1</span><br><span class="line">48:   eb 07                   jmp    51 &lt;main+0x51&gt;</span><br><span class="line">  &#125;</span><br><span class="line">  else</span><br><span class="line">  &#123;</span><br><span class="line">      a = 2;</span><br><span class="line">4a:   c7 45 f8 02 00 00 00    mov    DWORD PTR [rbp-0x8],0x2</span><br><span class="line">51:   b8 00 00 00 00          mov    eax,0x0</span><br><span class="line">  &#125; </span><br></pre></td></tr></table></figure>

<p>可以看到，这里对于 r &#x3D;&#x3D; 0 的条件判断，被编译成了 cmp 和 jne 这两条指令。cmp 指令比较了前后两个操作数的值，这里的 DWORD PTR 代表操作的数据类型是 32 位的整数，而[rbp-0x4]则是变量 r 的内存地址。第一个操作数就是从内存里拿到的变量 r 的值。第二个操作数 0x0 就是我们设定的常量 0 的 16 进制表示。cmp 指令的比较结果，会存入到条件码寄存器当中去。</p>
<p>在这里，如果比较的结果是 True，也就是 r &#x3D;&#x3D; 0，就把<strong>零标志条件码</strong>（对应的条件码是 ZF，Zero Flag）设置为 <strong>1</strong>。除了零标志之外，Intel 的 CPU 下还有进位标志（CF，Carry Flag）、符号标志（SF，Sign Flag）以及溢出标志（OF，Overflow Flag），用在不同的判断条件下。</p>
<p>cop 指令执行完成之后，PC寄存器会自动增加，开始执行下一条 jne指令。</p>
<p>跟着的 jne 指令，是 <strong>jump if not equal</strong> 的意思，它会查看对应的零标志位。如果 ZF 为 1，说明上面的比较结果是 TRUE，如果是 ZF 是 0，也就是上面的比较结果是 False，会跳转到后面跟着的操作数 4a 的位置。<strong>这个 4a，对应这里汇编代码的行号，</strong>也就是上面设置的 else 条件里的第一条指令。<strong>当跳转发生的时候，PC 寄存器就不再是自增变成下一条指令的地址，而是被直接设置成这里的 4a 这个地址。</strong> CPU 再把 4a 地址里的指令加载到指令寄存器中来执行。</p>
<p>到执行地址为 4a 的指令，实际是一条 mov 指令，第一个操作数和前面的 cmp 指令一样，是另一个 32 位整型的内存地址，以及 2 的对应的 16 进制值 0x2。mov 指令把 2 设置到对应的内存里去，相当于一个<strong>赋值操作</strong>。然后，PC 寄存器里的值继续自增，执行下一条 mov 指令。</p>
<p>这条 mov 指令的第一个操作数 eax，代表累加寄存器，第二个操作数 0x0 则是 16 进制的 0 的表示。这条指令其实没有实际的作用，它的作用是一个<strong>占位符</strong>。前面的 if 条件，如果满足的话，在赋值的 mov 指令执行完成之后，有一个 jmp 的无条件跳转指令。跳转的地址就是这一行的地址 51。我们的main 函数没有设定返回值，而move  eax,0x0 其实就是给函数生成了一个默认的为0 的返回值的累加器里面。if 条件里面的内容执行完成之后也会跳转到这里，和 else 里的内容结束之后的位置是一样的。<br><img src="/img/computer_img/ifelse.webp" alt="ifelse" style="zoom:33%;" /></p>
<h2 id="如何通过-if…else-和-goto-来实现循环？"><a href="#如何通过-if…else-和-goto-来实现循环？" class="headerlink" title="如何通过 if…else 和 goto 来实现循环？"></a>如何通过 if…else 和 goto 来实现循环？</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">int main()</span><br><span class="line">&#123;</span><br><span class="line">    int a = 0;</span><br><span class="line">    for (int i = 0; i &lt; 3; i++)</span><br><span class="line">    &#123;</span><br><span class="line">        a += i;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>从上面这一段简单的利用 for 循环的程序。我们循环自增变量 i 三次，三次之后，i&gt;&#x3D;3，就会跳出循环。整个程序，对应的 Intel 汇编代码就是这样的：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">for (int i = 0; i &lt;= 2; i++)</span><br><span class="line"> b:   c7 45 f8 00 00 00 00    mov    DWORD PTR [rbp-0x4],0x0</span><br><span class="line">12:   eb 0a                   jmp    1e </span><br><span class="line">  &#123;</span><br><span class="line">      a += i;</span><br><span class="line">14:   8b 45 f8                mov    eax,DWORD PTR [rbp-0x4]</span><br><span class="line">17:   01 45 fc                add    DWORD PTR [rbp-0x8],eax</span><br><span class="line"></span><br><span class="line">1a:   83 45 f8 01             add    DWORD PTR [rbp-0x4],0x1</span><br><span class="line">1e:   83 7d f8 02             cmp    DWORD PTR [rbp-0x4],0x2</span><br><span class="line">22:   7e f0                   jle    14 </span><br><span class="line">24:   b8 00 00 00 00          mov    eax,0x0</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<p>可以看到，对应的循环也是用 1e 这个地址上的 cmp 比较指令，和紧接着的 jle条件跳转指令来实现的。主要的差别在于，这里的 jle 跳转的地址，在这条指令之前的地址 14。往前跳转使得条件满足的时候，PC寄存器会把指令地址设置到之前执行过的指令位置，重新执行之前执行过的指令，直到条件不满足，顺序往下执行jle之后的指令，整个循环才结束。</p>
<img src="/img/computer_img/循环.webp" alt="循环" style="zoom:33%;" />

<p>其实，jle 和 jmp 指令，有点像程序语言里面的 goto 命令，直接指定了一个特定条件下的跳转位置。虽然我们在用高级语言开发程序的时候反对使用 goto，但是实际在机器指令层面，无论是 if…else…也好，还是 for&#x2F;while 也好，都是用和 goto 相同的跳转到特定指令位置的方式来实现的。</p>
<h2 id="总结延伸："><a href="#总结延伸：" class="headerlink" title="总结延伸："></a>总结延伸：</h2><p>程序里的多条指令，除了通过PC寄存器自增的方式顺序执行外，条件寄存器会记录下当前执行指令的条件判断状态，然后通过跳转指令读取对应的条件码，修改PC寄存器内的下一条指令的地址，最终实现if…else 以及 for&#x2F;while 这样的程序控制流程。</p>
<p><em>想要在硬件层面实现这个 goto 语句，除了本身需要用来保存下一条指令地址，以及当前正要执行指令的 PC 寄存器、指令寄存器外，我们只需要再增加一个条件码寄存器，来保留条件判断的状态。这样简简单单的三个寄存器，就可以实现条件判断和循环重复执行代码的功能。</em></p>
]]></content>
      <categories>
        <category>计算机组成原理</category>
      </categories>
      <tags>
        <tag>指令跳转</tag>
        <tag>寄存器</tag>
      </tags>
  </entry>
  <entry>
    <title>ELF和静态链接</title>
    <url>/posts/b778160a/</url>
    <content><![CDATA[<h1 id="ELF和静态链接：为什么程序无法同时在Linux和Windows下运行？"><a href="#ELF和静态链接：为什么程序无法同时在Linux和Windows下运行？" class="headerlink" title="ELF和静态链接：为什么程序无法同时在Linux和Windows下运行？"></a>ELF和静态链接：为什么程序无法同时在Linux和Windows下运行？</h1><h2 id="编译、链接和装载：-拆解程序执行"><a href="#编译、链接和装载：-拆解程序执行" class="headerlink" title="编译、链接和装载： 拆解程序执行"></a>编译、链接和装载： 拆解程序执行</h2><p>之前，我们通过 gcc 生成的文件和 objdump 获取到的汇编指令都有些小小的问题，以add 函数示例，<span id="more"></span>拆分成两个文件 add_lib.c 和 link_example.c 。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">// add_lib.c</span><br><span class="line">int add(int a, int b)</span><br><span class="line">&#123;</span><br><span class="line">    return a+b;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">// link_example.c</span><br><span class="line"></span><br><span class="line">#include &lt;stdio.h&gt;</span><br><span class="line">int main()</span><br><span class="line">&#123;</span><br><span class="line">    int a = 10;</span><br><span class="line">    int b = 5;</span><br><span class="line">    int c = add(a, b);</span><br><span class="line">    printf(&quot;c = %d\n&quot;, c);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>我们通过 gcc 来编译这两个文件，然后通过 objdump 命令看看它们的汇编代码。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ gcc -g -c add_lib.c link_example.c</span><br><span class="line">$ objdump -d -M intel -S add_lib.o</span><br><span class="line">$ objdump -d -M intel -S link_example.o</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">add_lib.o:     file format elf64-x86-64</span><br><span class="line">Disassembly of section .text:</span><br><span class="line">0000000000000000 &lt;add&gt;:</span><br><span class="line">   0:   55                      push   rbp</span><br><span class="line">   1:   48 89 e5                mov    rbp,rsp</span><br><span class="line">   4:   89 7d fc                mov    DWORD PTR [rbp-0x4],edi</span><br><span class="line">   7:   89 75 f8                mov    DWORD PTR [rbp-0x8],esi</span><br><span class="line">   a:   8b 55 fc                mov    edx,DWORD PTR [rbp-0x4]</span><br><span class="line">   d:   8b 45 f8                mov    eax,DWORD PTR [rbp-0x8]</span><br><span class="line">  10:   01 d0                   add    eax,edx</span><br><span class="line">  12:   5d                      pop    rbp</span><br><span class="line">  13:   c3                      ret    </span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">link_example.o:     file format elf64-x86-64</span><br><span class="line">Disassembly of section .text:</span><br><span class="line">0000000000000000 &lt;main&gt;:</span><br><span class="line">   0:   55                      push   rbp</span><br><span class="line">   1:   48 89 e5                mov    rbp,rsp</span><br><span class="line">   4:   48 83 ec 10             sub    rsp,0x10</span><br><span class="line">   8:   c7 45 fc 0a 00 00 00    mov    DWORD PTR [rbp-0x4],0xa</span><br><span class="line">   f:   c7 45 f8 05 00 00 00    mov    DWORD PTR [rbp-0x8],0x5</span><br><span class="line">  16:   8b 55 f8                mov    edx,DWORD PTR [rbp-0x8]</span><br><span class="line">  19:   8b 45 fc                mov    eax,DWORD PTR [rbp-0x4]</span><br><span class="line">  1c:   89 d6                   mov    esi,edx</span><br><span class="line">  1e:   89 c7                   mov    edi,eax</span><br><span class="line">  20:   b8 00 00 00 00          mov    eax,0x0</span><br><span class="line">  25:   e8 00 00 00 00          call   2a &lt;main+0x2a&gt;</span><br><span class="line">  2a:   89 45 f4                mov    DWORD PTR [rbp-0xc],eax</span><br><span class="line">  2d:   8b 45 f4                mov    eax,DWORD PTR [rbp-0xc]</span><br><span class="line">  30:   89 c6                   mov    esi,eax</span><br><span class="line">  32:   48 8d 3d 00 00 00 00    lea    rdi,[rip+0x0]        # 39 &lt;main+0x39&gt;</span><br><span class="line">  39:   b8 00 00 00 00          mov    eax,0x0</span><br><span class="line">  3e:   e8 00 00 00 00          call   43 &lt;main+0x43&gt;</span><br><span class="line">  43:   b8 00 00 00 00          mov    eax,0x0</span><br><span class="line">  48:   c9                      leave  </span><br><span class="line">  49:   c3                      ret    </span><br></pre></td></tr></table></figure>

<p>既然代码已经被我们“编译”成了指令，我们不妨尝试运行一下 .&#x2F;link_example.o。不幸的是，文件没有执行权限，我们遇到一个 Permission denied 错误。即使通过 chmod 命令赋予 link_example.o 文件可执行的权限，运行.&#x2F;link_example.o 仍然只会得到一条 cannot execute binary file: Exec format error 的错误。</p>
<p>我们再仔细看一下 objdump 出来的两个文件的代码，会发现两个程序的地址都是从 0 开始的。如果地址是一样的，程序如果需要通过 call 指令调用函数的话，它怎么知道应该跳转到哪一个文件里呢？</p>
<p>这么说吧，无论是这里的运行报错，还是 objdump 出来的汇编代码里面的重复地址，都是因为 add_lib.o 以及 link_example.o 并不是一个<strong>可执行文件</strong>（Executable Program），而是<strong>目标文件（</strong>Object File）。只有通过链接器（Linker）把多个目标文件以及调用的各种函数库链接起来，才能得到一个可执行文件。</p>
<p>我们通过 gcc 的 -o 参数，可以生成对应的可执行文件，对应执行之后，就可以得到这个简单的加法调用函数的结果。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ gcc -o link-example add_lib.o link_example.o</span><br><span class="line">$ ./link_example</span><br><span class="line">c = 15</span><br></pre></td></tr></table></figure>

<h2 id="“C-语言代码-汇编代码-机器码”-的过程"><a href="#“C-语言代码-汇编代码-机器码”-的过程" class="headerlink" title="“C 语言代码 - 汇编代码 - 机器码” 的过程"></a>“C 语言代码 - 汇编代码 - 机器码” 的过程</h2><p>实际上，“C 语言代码 - 汇编代码 - 机器码” 这个过程，在我们的计算机上进行的时候是由两部分组成的。</p>
<ul>
<li><p>第一部分： 由编译（Compile）、汇编（Assemble）以及链接（Link）三个阶段组成。之后生成一个可执行文件。</p>
</li>
<li><p>第二部分，我们通过装载器（Loader）把可执行文件装载（Load）到内存中。CPU 从内存中读取指令和数据，开始真正执行程序。</p>
<img src="/img/computer_img/装载.webp" alt="装载" style="zoom:33%;" /></li>
</ul>
<h2 id="ELF-格式和链接：理解链接过程"><a href="#ELF-格式和链接：理解链接过程" class="headerlink" title="ELF 格式和链接：理解链接过程"></a>ELF 格式和链接：理解链接过程</h2><p>程序最终是通过装载器变成指令和数据的，所以其实我们生成的可执行代码也并不仅仅是一条条的指令。我们还是通过 objdump 指令，把可执行文件的内容拿出来看看。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">link_example:     file format elf64-x86-64</span><br><span class="line">Disassembly of section .init:</span><br><span class="line">...</span><br><span class="line">Disassembly of section .plt:</span><br><span class="line">...</span><br><span class="line">Disassembly of section .plt.got:</span><br><span class="line">...</span><br><span class="line">Disassembly of section .text:</span><br><span class="line">...</span><br><span class="line"></span><br><span class="line"> 6b0:   55                      push   rbp</span><br><span class="line"> 6b1:   48 89 e5                mov    rbp,rsp</span><br><span class="line"> 6b4:   89 7d fc                mov    DWORD PTR [rbp-0x4],edi</span><br><span class="line"> 6b7:   89 75 f8                mov    DWORD PTR [rbp-0x8],esi</span><br><span class="line"> 6ba:   8b 55 fc                mov    edx,DWORD PTR [rbp-0x4]</span><br><span class="line"> 6bd:   8b 45 f8                mov    eax,DWORD PTR [rbp-0x8]</span><br><span class="line"> 6c0:   01 d0                   add    eax,edx</span><br><span class="line"> 6c2:   5d                      pop    rbp</span><br><span class="line"> 6c3:   c3                      ret    </span><br><span class="line">00000000000006c4 &lt;main&gt;:</span><br><span class="line"> 6c4:   55                      push   rbp</span><br><span class="line"> 6c5:   48 89 e5                mov    rbp,rsp</span><br><span class="line"> 6c8:   48 83 ec 10             sub    rsp,0x10</span><br><span class="line"> 6cc:   c7 45 fc 0a 00 00 00    mov    DWORD PTR [rbp-0x4],0xa</span><br><span class="line"> 6d3:   c7 45 f8 05 00 00 00    mov    DWORD PTR [rbp-0x8],0x5</span><br><span class="line"> 6da:   8b 55 f8                mov    edx,DWORD PTR [rbp-0x8]</span><br><span class="line"> 6dd:   8b 45 fc                mov    eax,DWORD PTR [rbp-0x4]</span><br><span class="line"> 6e0:   89 d6                   mov    esi,edx</span><br><span class="line"> 6e2:   89 c7                   mov    edi,eax</span><br><span class="line"> 6e4:   b8 00 00 00 00          mov    eax,0x0</span><br><span class="line"> 6e9:   e8 c2 ff ff ff          call   6b0 &lt;add&gt;</span><br><span class="line"> ------------------------------------------------</span><br><span class="line"> 6ee:   89 45 f4                mov    DWORD PTR [rbp-0xc],eax</span><br><span class="line"> 6f1:   8b 45 f4                mov    eax,DWORD PTR [rbp-0xc]</span><br><span class="line"> 6f4:   89 c6                   mov    esi,eax</span><br><span class="line"> 6f6:   48 8d 3d 97 00 00 00    lea    rdi,[rip+0x97]        # 794 &lt;_IO_stdin_used+0x4&gt;</span><br><span class="line"> 6fd:   b8 00 00 00 00          mov    eax,0x0</span><br><span class="line"> 702:   e8 59 fe ff ff          call   560 &lt;printf@plt&gt;</span><br><span class="line"> 707:   b8 00 00 00 00          mov    eax,0x0</span><br><span class="line"> 70c:   c9                      leave  </span><br><span class="line"> 70d:   c3                      ret    </span><br><span class="line"> 70e:   66 90                   xchg   ax,ax</span><br><span class="line">...</span><br><span class="line">Disassembly of section .fini:</span><br><span class="line">...</span><br></pre></td></tr></table></figure>

<p>你会发现，可执行代码 dump 出来内容，和之前的目标代码长得差不多，但是长了很多。因为在 Linux 下，可执行文件和目标文件所使用的都是一种叫<strong>ELF（Execuatable and Linkable File Format）</strong>的文件格式，中文名叫可执行与可链接文件格式，这里面不仅存放了编译成的汇编指令，还保留了很多别的数据。</p>
<p>比如我们过去所有 objdump 出来的代码里，你都可以看到对应的函数名称，像 add、main 等等，乃至你自己定义的全局可以访问的变量名称，都存放在这个 ELF 格式文件里。这些名字和他们对应的地址，在ELF 文件里，存储在一个叫作符号表（Symbols Table）的位置里。符号表相当于一个地址簿，把它们关联了起来。</p>
<p>只关注和我们的 add 以及 main 函数相关的部分。你会发现，这里面，main 函数里调用 add 的跳转地址，不再是下一条指令的地址，而是 add 函数的入口地址了，这就是ELF 格式和链接器的功劳。</p>
<img src="/img/computer_img/ELF 格式.webp" alt="ELF 格式" style="zoom:33%;" />

<p>ELF 文件格式把各种信息，分成一个一个的 Section 保存起来。ELF 有一个基本的文件头（File Header）,用来表示这个文件的基本属性，比如是否是可执行文件，对应的 CPU、操作系统等等。除了这些基本属性之外，大部分程序还有这么一些 Section：</p>
<ol>
<li><p>text Section: 代码段或者指令段（Code Section）,用来保存程序的代码和指令；</p>
</li>
<li><p>data Section ,也叫数据段（Data Section）,用来保存程序里设置好的初始化数据信息。</p>
</li>
<li><p>.rel.text.Section ,也叫重定位表。表里保留的是当前的文件里面，哪些跳转地址是我们不知道的。比如上面的 link_example.o 里面，我们在 main 函数里面调用了 add 和 printf 这两个函数，但是在链接发生之前，我们并不知道该跳转到哪里，这些信息就存在重定位表里；</p>
</li>
<li><p>最后是 .symtab Section ,也叫符号表。符号表保留了我们所说的当前文件里面定义的函数名称和对应地址的地址簿。</p>
</li>
</ol>
<h2 id="链接器工作原理"><a href="#链接器工作原理" class="headerlink" title="链接器工作原理"></a>链接器工作原理</h2><p>链接器会扫描所有输入的目标文件，然后把所有<strong>符号表</strong>里的信息收集起来，构成一个全局的符号表。然后再根据<strong>重定位表</strong>，把所有不确定要跳转地址的代码，根据符号表里的存储地址，进行一次修正。最后，把所有的目标文件的对应段进行一次合并，变成了最终的可执行代码。<em>这也是为什么，可执行文件里面的函数调用的地址是正确的。</em></p>
<img src="/img/computer_img/ELF 链接过程.webp" alt="ELF 链接过程" style="zoom: 33%;" />

<p><strong>最后</strong>，在链接器把程序变成可执行文件之后，要装载器去执行程序就容易多了。装载器不再需要考虑地址跳转的问题，只需要解析 ELF 文件，把对应的指令和数据，加载到内存里面供 CPU 执行就可以了。</p>
<h2 id="总结延伸："><a href="#总结延伸：" class="headerlink" title="总结延伸："></a>总结延伸：</h2><p>什么同样一个程序，在 Linux 下可以执行而在 Windows 下不能执行了。其中一个非常重要的原因就是，两个操作系统下可执行文件的格式不一样。Linux 下的 ELF 文件格式，而 Windows 的可执行文件格式是一种叫作 PE（Portable Executable Format）的文件格式。Linux 下的装载器只能解析 ELF 格式而不能解析 PE 格式。</p>
<p>如果我们有一个可以能够解析PE 格式的装载器，我们就有可能在 Linux 下运行 Windows 程序。</p>
<p>对于 ELF 格式的文件，为了能够实现这样一个<em><strong>静态链接</strong></em>的机制，里面不只是简单罗列了程序所需要执行的指令，还会包括链接所需要的重定位表和符号表。</p>
]]></content>
      <categories>
        <category>计算机组成原理</category>
      </categories>
      <tags>
        <tag>程序</tag>
        <tag>链接</tag>
      </tags>
  </entry>
  <entry>
    <title>程序装载</title>
    <url>/posts/f17b55f/</url>
    <content><![CDATA[<h1 id="程序装载：“640K内存”真的不够用么？"><a href="#程序装载：“640K内存”真的不够用么？" class="headerlink" title="程序装载：“640K内存”真的不够用么？"></a>程序装载：“640K内存”真的不够用么？</h1><h2 id="程序装载面临的挑战"><a href="#程序装载面临的挑战" class="headerlink" title="程序装载面临的挑战"></a>程序装载面临的挑战</h2><p>通过链接器，把多个文件合并为一个最终可执行文件。在执行这些可执行文件的时候，就是通过一个装载器，解析ELF 或 PE 格式的可执行文件<span id="more"></span>。装载器把对应的指令和数据加到内存中，让CPU  去执行。</p>
<ul>
<li>所以装载器实际上要满足两个要求</li>
</ul>
<blockquote>
<p>一、可执行程序加载后占用的内存空间应该是连续的，在执行指令的时候，程序计数器是顺序的一条一条指令执行下去，这也意味着，这些指令需要连续的存储在一起。</p>
<p>二、我们需要同时加载多个程序，并且不能让程序自己规定在内存中加载的位置。虽然编译出来的指令有了对应的各种各样的内存地址，但是实际加载的时候，是没有办法确保，这个程序一定加载在那一段内存地址上。因为我们现在的计算机通常会同时运行很多个程序，可能你想要的内存地址已经被其他加载了的程序占用了。</p>
</blockquote>
<p>要满足这两个基本的要求，一个办法就是我们可以在内存里面，找到一段连续的内存空间，然后分配给装载的程序，然后把这段连续的内存空间地址，和整个程序指令里指定的内存地址做一个映射。</p>
<p>把指令里用到的内存地址叫作<strong>虚拟内存地址（Virtual Memory Address）</strong>，实际在内存硬件里面的空间地址，我们叫<strong>物理内存地址（Physical Memory Address）。</strong></p>
<p>程序里有指令和各种内存地址，我们只需要关心虚拟内存地址就行了。对于任何一个程序来说，看到的都是同样的内存地址。<strong>我们维护一个虚拟内存到物理内存的映射表，</strong>这样实际程序指令执行的时候，会通过虚拟内存地址，找到对应的物理内存地址，然后执行。</p>
<p>因为是连续的内存地址空间，所以我们只需要维护映射关系的起始地址和对应的空间大小。</p>
<h2 id="内存分段"><a href="#内存分段" class="headerlink" title="内存分段"></a>内存分段</h2><p>这种找出一段连续的物理内存和虚拟内存地址进行映射的方法，我们叫<strong>分段（Segmentation） 。</strong>这里的段，指的是系统分配出来的那个连续的内存空间。</p>
<img src="/img/computer_img/内存分段.webp" alt="内存分段" style="zoom: 50%;" />

<p>分段存在一个问题 就是内存碎片。</p>
<p>我们来看这样一个例子。我现在手头的这台电脑，有 1GB 的内存。我们先启动一个图形渲染程序，占用了 512MB 的内存，接着启动一个 Chrome 浏览器，占用了 128MB 内存，再启动一个 Python 程序，占用了 256MB 内存。这个时候，我们关掉 Chrome，于是空闲内存还有 1024 - 512 - 256 &#x3D; 256MB。按理来说，我们有足够的空间再去装载一个 200MB 的程序。但是，这 256MB 的内存空间不是连续的，而是被分成了两段 128MB 的内存。因此，实际情况是，我们的程序没办法加载进来。</p>
<img src="/img/computer_img/内部碎片.webp" alt="内部碎片" style="zoom:50%;" />

<p>这个也有办法解决。解决的办法叫<strong>内存交换（Memory Swapping）。</strong></p>
<p>(Memory swapping )可以将Python 程序占用的那256MB 的内存写到硬盘上，然后再从硬盘读回到内存里。读回来的时候，它被加载到那个占用了的512MB内存的后面。这样，我们就有了连续的 256MB 内存空间，就可以去加载一个新的 200MB 的程序。</p>
<ul>
<li>虚拟内存、分段再加上内存交换，看似解决了计算机同时装载多个程序的问题，但是会遇到性能瓶颈。硬盘访问速度比内存要慢的多，每一次内存交换都需要把一大段的连续内存数据写到磁盘上，所以交换非常占内存空间，会引起卡顿。</li>
</ul>
<h2 id="内存分页"><a href="#内存分页" class="headerlink" title="内存分页"></a>内存分页</h2><p>为了少出现一些内存碎片，另外，当需要进行内存交换的时候，让需要交换写入或者从磁盘装载的数据更少一点，在现在计算机的内存管理里面，就叫作内存分页（Paging）。</p>
<p>和分段这样分配一整段连续的空间给到程序相比，分页是把整个物理内存空间切成一段段固定尺寸的大小(4kB)。而对应程序所需要占用的虚拟内存空间，也同样切成一段段固定尺寸的大小。这样一个连续并且尺寸固定的内存空间，我们叫<strong>页（Page）。</strong>从虚拟内存到物理内存的映射，不再是拿整段连续的内存的物理地址，而是按照一个一个页来的。<em>页的尺寸一般远远小于整个程序的大小</em></p>
<p><strong>由于内存空间都是预先划分好的</strong>，也就没有了不能使用的碎片，而只有被释放出来的很多 4KB 的页。即使内存空间不够，需要让现有的、正在运行的其他程序，通过内存交换释放出一些内存的页出来，一次写入磁盘的也只有少数的一个页或者几个页，不会花太多时间，让整个机器被内存交换过程给卡住。</p>
<img src="/img/computer_img/内存分页.webp" alt="内存分页" style="zoom: 33%;" />

<p>更进一步地，分页的方式使得我们在加载程序的时候，不再需要一次性都把程序加载到物理内存中。而是只在程序运行中，需要用到对应虚拟内存页里的指令和数据是，再加载到物理内存里面去。</p>
<p>当要读取特定的页，却发现数据并没有加载到物理内存里的时候，就会触发一个来自于 CPU 的<strong>缺页错误（Page Fault）</strong>。当 OS 捕捉到这个错误时，将对应的页从存放在硬盘上的虚拟内存读取出来，加载到物理内存里。这种方式，使得我们可以运行那些远大于我们实际物理内存的程序（只需要加载到当前需要用到的就行）</p>
<ul>
<li>通过虚拟内存、内存交换和内存分页这三个技术的组合，我们得到了一个让程序不需要考虑实际的物理内存地址、大小和当前分配空间的解决方案。（这些技术和方法，对于我们程序的编写、编译和链接过程都是透明的。这也是我们在计算机的软硬件开发中常用的一种方法，就是加入一个间接层。）</li>
<li>任何一个程序，都只需要把内存当成是一块完整而连续的空间来直接使用。</li>
</ul>
<h2 id="总结延伸"><a href="#总结延伸" class="headerlink" title="总结延伸"></a>总结延伸</h2><p>实要运行一个程序，“必需”的内存是很少的。CPU 只需要执行当前的指令，极限情况下，内存也只需要加载一页就好了。再大的程序，也可以分成一页。每次，只在需要用到对应的数据和指令的时候，从硬盘上交换到内存里面来就好了。以我们现在 4K 内存一页的大小，640K 内存也能放下足足 160 页呢，也无怪乎在比尔·盖茨会说出“640K ought to be enough for anyone”这样的话。</p>
<p>不过呢，硬盘的访问速度比内存慢很多，所以我们现在的计算机，没有个几 G 的内存都不好意思和人打招呼。</p>
<p>除了程序分页装载这种方式之外，我们还有其他优化内存使用的方式么？其实“动态装载”也是可以优化内存使用。</p>
<h2 id="思考："><a href="#思考：" class="headerlink" title="思考："></a>思考：</h2><p>在 Java 这样使用虚拟机的编程语言里面，我们写的程序是怎么装载到内存里面来的呢？它也和我们讲的一样，是通过内存分页和内存交换的方式加载到内存里面来的么？</p>
<p>一、首先，我们编写的Java程序，即源代码<code>.java</code>文件经过编译生成字节码文件<code>.class</code>；</p>
<p>然后，创建JVM环境，即查找和装载<code>libjvm.so</code>文件；</p>
<p>最后，通过创建JVM实例，加载主类的字节码文件到系统给该JVM实例分配的内存中；</p>
<p>二、ava代码的执行需要JVM环境，JVM环境的创建就是查找和装载<code>libjvm.so</code>文件：装载<code>libjvm.so</code>是通过内存分页和内存交换的方式加载到内存的。<br>字节码文件是通过类加载器加载到主类文件对应的JVM实例的内存空间中的，这一部分不是使用内存分页和内存交换的方式来管理的，使用的是JVM的内存分配策略来管理的；</p>
]]></content>
      <categories>
        <category>计算机组成原理</category>
      </categories>
      <tags>
        <tag>虚拟内存</tag>
      </tags>
  </entry>
  <entry>
    <title>动态链接</title>
    <url>/posts/44c769c7/</url>
    <content><![CDATA[<h1 id="动态链接：程序内部的“共享单车”"><a href="#动态链接：程序内部的“共享单车”" class="headerlink" title="动态链接：程序内部的“共享单车”"></a>动态链接：程序内部的“共享单车”</h1><p>程序的链接，是把对应的不同文件内的代码段，合并到一起，成为最后的可执行文件。这个链接的方式，<span id="more"></span>让我们在写代码的时候做到了“复用”。同样的功能代码只要写一次，然后提供给不同的程序进行链接就行了。</p>
<p>这么说来，“链接”其实有点儿像我们日常生活中的标准化、模块化生产。但是，如果我们有很多个程序都要通过装载器装载到内存里面，那里面链接好的同样的功能代码，也都需要再装载一遍，再占一遍内存空间。岂不是显得很冗余？</p>
<h2 id="链接可以分动、静，共享运行省内存"><a href="#链接可以分动、静，共享运行省内存" class="headerlink" title="链接可以分动、静，共享运行省内存"></a>链接可以分动、静，共享运行省内存</h2><p>如果我们能够让同样功能的代码，在不同的程序里面，不需要各占一份内存空间，那该有多好啊！就好比，现在马路上的共享单车，我们并不需要给每个人都造一辆自行车，只要马路上有这些单车，谁需要的时候，直接通过手机扫码，都可以解锁骑行。</p>
<p>这个思路引入一个新的链接方法，叫作<strong>动态链接（</strong>Dynamic Link）。相应地，我们之前说的合并代码段的方法，就是<strong>静态链接（Static Link）。</strong></p>
<p>在DL中，我们想要的链接不是存储在硬盘上的目标文件代码，而是<strong>加载到内存中的共享库</strong>。这个共享库的代码会被很多个程序的指令调用到。win系统下，这些共享库文件就是 .dll文件，也就是Dynamic-Link Libary（DLL，动态链接库）。在 Linux 下，这些共享库文件就是.so 文件，也就是 Shared Object（一般我们也称之为动态链接库）。</p>
<img src="/img/computer_img/内存中的共享库.webp" alt="内存中的共享库" style="zoom: 33%;" />



<h2 id="地址无关很重要，相对地址解烦恼"><a href="#地址无关很重要，相对地址解烦恼" class="headerlink" title="地址无关很重要，相对地址解烦恼"></a>地址无关很重要，相对地址解烦恼</h2><p>想要在程序运行时共享代码，也有一定的要求，就是这些机器码必须是“地址无关”的。也就是，编译出来的共享库中的指令代码，是地址无关的（Position-Independent Code）。换句话说就是，这段代码，无论加载在哪个内存地址，都能够正常执行。</p>
<img src="/img/computer_img/内存中的共享库.webp" alt="内存中的共享库" style="zoom:33%;" />

<p>对于所有动态链接共享库的程序来说，虽然共享库用的都是同一段物理内存地址，但是在不同的应用程序里，它所在的虚拟内存地址是不同的的。</p>
<p>我们要怎么样才能做到，动态共享库编译出来的代码指令，都是地址无关码呢？</p>
<p>需需要使用相对地址（Relative Address）就好了。各种指令中使用到的内存地址，给出的不是一个绝对的地址空间，而是一个相对于当前指令偏移量的内存地址。因为共享库是放在一段连续的虚拟内存地址中的，无论装载到哪一段地址，不同指令之间的相对地址都是不变的。</p>
<h2 id="PLT-和-GOT，动态链接的解决方案"><a href="#PLT-和-GOT，动态链接的解决方案" class="headerlink" title="PLT 和 GOT，动态链接的解决方案"></a>PLT 和 GOT，动态链接的解决方案</h2><p>我们还是拿出一小段代码来看一看。</p>
<p>首先，lib.h 定义了动态链接库的一个函数 show_me_the_money。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">// lib.h</span><br><span class="line">#ifndef LIB_H</span><br><span class="line">#define LIB_H</span><br><span class="line"></span><br><span class="line">void show_me_the_money(int money);</span><br><span class="line"></span><br><span class="line">#endif</span><br></pre></td></tr></table></figure>

<p>lib.c 包含了 lib.h 的实际实现。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">// lib.c</span><br><span class="line">#include &lt;stdio.h&gt;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">void show_me_the_money(int money)</span><br><span class="line">&#123;</span><br><span class="line">    printf(&quot;Show me USD %d from lib.c \n&quot;, money);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>然后，show_me_poor.c 调用了 lib 里面的函数。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">// show_me_poor.c</span><br><span class="line">#include &quot;lib.h&quot;</span><br><span class="line">int main()</span><br><span class="line">&#123;</span><br><span class="line">    int money = 5;</span><br><span class="line">    show_me_the_money(money);</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>最后，我们把 lib.c 编译成了一个动态链接库，也就是 .so 文件。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">$ gcc lib.c -fPIC -shared -o lib.so</span><br><span class="line">$ gcc -o show_me_poor show_me_poor.c ./lib.so</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>在编译的过程中，我们指定了一个 <strong>-fPIC</strong> 的参数。这个参数其实就是 Position Independent Code 的意思，也就是我们要把这个编译成一个地址无关代码。</p>
<p>然后，我们再通过 gcc 编译 show_me_poor <strong>动态链接</strong>了 lib.so 的可执行文件。在这些操作都完成了之后，我们把 show_me_poor 这个文件通过 <strong>objdump</strong> 出来看一下。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">$ objdump -d -M intel -S show_me_poor</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">……</span><br><span class="line">0000000000400540 &lt;show_me_the_money@plt-0x10&gt;:</span><br><span class="line">  400540:       ff 35 12 05 20 00       push   QWORD PTR [rip+0x200512]        # 600a58 &lt;_GLOBAL_OFFSET_TABLE_+0x8&gt;</span><br><span class="line">  400546:       ff 25 14 05 20 00       jmp    QWORD PTR [rip+0x200514]        # 600a60 &lt;_GLOBAL_OFFSET_TABLE_+0x10&gt;</span><br><span class="line">  40054c:       0f 1f 40 00             nop    DWORD PTR [rax+0x0]</span><br><span class="line"></span><br><span class="line">0000000000400550 &lt;show_me_the_money@plt&gt;:</span><br><span class="line">  400550:       ff 25 12 05 20 00       jmp    QWORD PTR [rip+0x200512]        # 600a68 &lt;_GLOBAL_OFFSET_TABLE_+0x18&gt;</span><br><span class="line">  400556:       68 00 00 00 00          push   0x0</span><br><span class="line">  40055b:       e9 e0 ff ff ff          jmp    400540 &lt;_init+0x28&gt;</span><br><span class="line">……</span><br><span class="line">0000000000400676 &lt;main&gt;:</span><br><span class="line">  400676:       55                      push   rbp</span><br><span class="line">  400677:       48 89 e5                mov    rbp,rsp</span><br><span class="line">  40067a:       48 83 ec 10             sub    rsp,0x10</span><br><span class="line">  40067e:       c7 45 fc 05 00 00 00    mov    DWORD PTR [rbp-0x4],0x5</span><br><span class="line">  400685:       8b 45 fc                mov    eax,DWORD PTR [rbp-0x4]</span><br><span class="line">  400688:       89 c7                   mov    edi,eax</span><br><span class="line">  40068a:       e8 c1 fe ff ff          call   400550 &lt;show_me_the_money@plt&gt;</span><br><span class="line">  40068f:       c9                      leave  </span><br><span class="line">  400690:       c3                      ret    </span><br><span class="line">  400691:       66 2e 0f 1f 84 00 00    nop    WORD PTR cs:[rax+rax*1+0x0]</span><br><span class="line">  400698:       00 00 00 </span><br><span class="line">  40069b:       0f 1f 44 00 00          nop    DWORD PTR [rax+rax*1+0x0]</span><br><span class="line">……</span><br></pre></td></tr></table></figure>

<p>在 main 函数调用 show_me_the_money 的函数的时候，对应的代码是这样的：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">call   400550 &lt;show_me_the_money@plt&gt;</span><br></pre></td></tr></table></figure>

<p>后面有一个 @plt 的关键字，代表了我们需要从 <strong>PLT</strong>，也就是<strong>程序链接表</strong>（Procedure Link Table）里面找要调用的函数。对应的地址呢，则是 400550 这个地址。</p>
<p>当我们把目光挪到上面的 400550 这个地址，你又会看到里面进行了一次跳转，这个跳转指定的跳转地址，你可以在后面的注释里面可以看到，GLOBAL_OFFSET_TABLE+0x18。这里的 GLOBAL_OFFSET_TABLE，就是我接下来要说的全局偏移表。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">400550:       ff 25 12 05 20 00       jmp    QWORD PTR [rip+0x200512]        # 600a68 &lt;_GLOBAL_OFFSET_TABLE_+0x18&gt;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>在动态链接对应的共享库，我们在共享库的 data section 里面，保存了一张<strong>全局偏移表（GOT，Global Offset Table）。虽然共享库的代码部分的物理内存是共享的，但是数据部分是各个动态链接它的应用程序里面各加载一份的</strong>。需要引用共享库外部的地址的指令，都会查询GOT ，来 <em><strong>找到当前运行程序的虚拟内存里的对应位置。而 GOT 表里的数据，则是我们加载一个个共享库的时候写进去的。</strong></em></p>
<p>不同的进程，调用同样的 lib.so，各自 GOT 里面指向最终加载的动态链接库里面的虚拟内存地址是不同的。</p>
<blockquote>
<p>这样，虽然不同的程序调用的同样的动态库，各自的内存地址是独立的，调用的又都是同一个动态库， 但是不需要去修改动态库里的代码所使用的地址，而是各自程序各自维护好自己的 GOT ，能够找到对应的动态库就行</p>
</blockquote>
<img src="/img/computer_img/动态库GOT.webp" alt="动态库GOT" style="zoom:33%;" />

<ul>
<li><p>GOT 表位于共享库自己的数据段里，GOT 表在内存里和对应的代码段位置之间的偏移量始终是确定的。</p>
</li>
<li><p>这样，我们的共享库就是地址无关的代码，对应的各个程序只需要在物理内存里面加载同一份代码。而我们又要通过各个可执行程序在加载时，生成各个不同的 GOT 表，来找到它需要调用到的外部变量和函数的地址。</p>
</li>
<li><p>这是一个典型的、不修改代码，而是通过修改“地址数据”来进行关联的办法。它有点像我们在 C 语言里面用函数指针来调用对应的函数，并不是通过预先已经确定好的函数名称来调用，而是利用当时它在内存里的动态地址来调用。</p>
</li>
</ul>
<h2 id="总结延伸："><a href="#总结延伸：" class="headerlink" title="总结延伸："></a>总结延伸：</h2><p>利用动态链接把我们的内存利用到了极致。同样功能的代码生成的共享库，我们只要在内存里面保留一份就好了。这样，我们不仅能够做到代码在开发阶段的复用，也能做到代码在运行阶段的复用。</p>
<p>实际上，在进行 Linux 下的程序开发的时候，我们一直会用到各种各样的动态链接库。C 语言的标准库就在 1MB 以上。我们撰写任何一个程序可能都需要用到这个库，常见的 Linux 服务器里，&#x2F;usr&#x2F;bin 下面就有上千个可执行文件。如果每一个都把标准库静态链接进来的，几 GB 乃至几十 GB 的磁盘空间一下子就用出去了。如果我们服务端的多进程应用要开上千个进程，几 GB 的内存空间也会一下子就用出去了。这个问题在过去计算机的内存较少的时候更加显著。</p>
<p>​                                                <strong>(内存中的共享库)</strong></p>
]]></content>
      <categories>
        <category>计算机组成原理</category>
      </categories>
      <tags>
        <tag>链接</tag>
        <tag>虚拟内存</tag>
      </tags>
  </entry>
  <entry>
    <title>存储器层次结构</title>
    <url>/posts/8e9c1609/</url>
    <content><![CDATA[<h1 id="存储器层次结构全景：数据存储的大金字塔长什么样？"><a href="#存储器层次结构全景：数据存储的大金字塔长什么样？" class="headerlink" title="存储器层次结构全景：数据存储的大金字塔长什么样？"></a>存储器层次结构全景：数据存储的大金字塔长什么样？</h1><h2 id="存储器的层次结构"><a href="#存储器的层次结构" class="headerlink" title="存储器的层次结构"></a>存储器的层次结构</h2><p>在有计算机之前，我们通常把信息和数据存储在书、文件这样的物理介质里面<span id="more"></span>。有了计算机之后，我们通常把数据存储在计算机的存储器里面。而存储器系统是一个通过各种不同的方法和设备，一层一层组合起来的系统。</p>
<p>我们常常把 CPU 比喻成计算机的“大脑”。我们思考的东西，就好比 CPU 中的寄存器（Register）。 寄存器更像是 CPU 本身的一部分，只能存放极其有限的信息，但是速度非常快，和 CPU 同步。</p>
<p>而我们大脑中的记忆，就好比 CPU Cache（CPU 高速缓存，我们常常简称为“缓存”）。CPU Cache 用的是一种叫作  <strong>SRAM</strong>（Static Random-Access Memory，<strong>静态随机存取存储器</strong>）的芯片。</p>
<h2 id="SRAM"><a href="#SRAM" class="headerlink" title="SRAM"></a>SRAM</h2><p>SRAM 之所以被称为“静态”存储器，是因为只要处在通电状态，里面的数据就可以保持存在。而一旦断电，里面的数据就会丢失了。在 SRAM 里面，一个比特的数据，需要 6～8 个晶体管。所以 SRAM 的存储密度不高。同样的物理空间下，能够存储的数据有限。不过，因为 SRAM 的电路简单，所以访问速度非常快。</p>
<img src="/img/computer_img/6个晶体管组成的SRAM 一个比特.webp" alt="6个晶体管组成的SRAM 一个比特" style="zoom:50%;" />

<center>6 个晶体管组成 SRAM 的一个比特</center>



<p>在 CPU 里，通常会有 L1、L2、L3 这样三层高速缓存。每个 CPU 核心都有一块属于自己的 L1 高速缓存，通常分成指令缓存和数据缓存，分开存放 CPU 使用的指令和数据。L1 的 Cache 往往就嵌在 CPU 核心的内部。</p>
<p><strong>L2</strong> 的 Cache 同样是每个 CPU 核心都有的，不过它往往不在 CPU 核心的内部。所以，L2 Cache 的访问速度会比 L1 稍微慢一些。而 <strong>L3 Cache，</strong>则通常是多个 CPU 核心共用的，尺寸会更大一些，访问速度自然也就更慢一些。</p>
<p>你可以把 CPU 中的 L1 Cache 理解为我们的短期记忆，把 L2&#x2F;L3 Cache 理解成长期记忆，把内存当成我们拥有的书架或者书桌。 当我们自己记忆中没有资料的时候，可以从书桌或者书架上拿书来翻阅。<em><strong>这个过程中就相当于，数据从内存中加载到 CPU 的寄存器和 Cache 中，然后通过“大脑”，也就是 CPU，进行处理和运算。</strong></em></p>
<h2 id="DRAM"><a href="#DRAM" class="headerlink" title="DRAM"></a>DRAM</h2><p>内存用的芯片和 Cache 有所不同，它用的是一种叫作 DRAM（Dynamic Random Access Memory，动态随机存取存储器）的芯片，比起 SRAM 来说，它的密度更高，有更大的容量，而且它也比 SRAM 芯片便宜不少。</p>
<p>DRAM 被称为“动态”存储器，是因为 DRAM 需要靠不断地“刷新”，才能保持数据被存储起来。DRAM 的一个比特，只需要<em><strong>一个晶体管和一个电容</strong></em>就能存储。所以，DRAM 在同样的物理空间下，能够存储的数据也就更多，也就是存储的“密度”更大。但是，因为数据是存储在电容里的，电容会不断漏电，所以需要定时刷新充电，才能保持数据不丢失。DRAM 的数据访问电路和刷新电路都比 SRAM 更复杂，所以访问延时也就更长。</p>
<img src="/img/computer_img/L1L2L3.webp" alt="L1L2L3" style="zoom:50%;" />



<h2 id="存储器的层级结构"><a href="#存储器的层级结构" class="headerlink" title="存储器的层级结构"></a>存储器的层级结构</h2><p>整个存储器的层次结构，其实都类似于 SRAM 和 DRAM 在性能和价格上的差异。SRAM 更贵，速度更快。DRAM 更便宜，容量更大。SRAM 好像我们的大脑中的记忆，而 DRAM 就好像属于我们自己的书桌。</p>
<p>大脑（CPU）中的记忆（L1 Cache），不仅受成本层面的限制，更受物理层面的限制。这就好比 L1 Cache 不仅昂贵，其访问速度和它到 CPU 的物理距离有关。芯片造得越大，总有部分离 CPU 的距离会变远。电信号的传输速度又受物理原理的限制，没法超过光速。所以想要快，并不是靠多花钱就能解决的。</p>
<p>我们自己的书房和书桌（也就是内存）空间一般是有限的，没有办法放下所有书（也就是数据）。如果想要扩大空间的话，就相当于要多买几平方米的房子，成本就会很高。于是，想要放下更多的书，我们就要寻找更加廉价的解决方案。</p>
<hr>
<p>没错，我们想到了公共图书馆。对于内存来说，SSD（Solid-state drive 或 Solid-state disk，固态硬盘）、HDD（Hard Disk Drive，硬盘）这些被称为硬盘的外部存储设备，就是公共图书馆。于是，我们就可以去家附近的图书馆借书了。图书馆有更多的空间（存储空间）和更多的书（数据）。</p>
<p>SSD 这种基于 NAND 芯片的高速硬盘，价格已经大幅度下降。而 HDD 硬盘则是一种完全符合“磁盘”这个名字的传统硬件。“磁盘”的硬件结构，决定了它的访问速度受限于它的物理结构，是最慢的。</p>
<img src="/img/computer_img/存储器层次结构.webp" alt="存储器层次结构" style="zoom:50%;" />

<center> 存储器层次结构 </center>

<p>从 Cache、内存，到 SSD 和 HDD 硬盘，一台现代计算机中，就用上了所有这些存储器设备。其中，容量越小的设备速度越快，而且，CPU 并不是直接和每一种存储器设备打交道，而是每一种存储器设备，只和他相邻的存储器设备打交道。</p>
<p>比如，CPU Cache 是从内存里加载而来的，或者需要写回内存，并不会直接写回数据到硬盘，也不会直接从硬盘加载数据到 CPU Cache 中，而是先加载到内存，再从内存加载到 Cache 中。</p>
<p><strong>这样，各个存储器只和相邻的一层存储器打交道，并且随着一层层向下，存储器的容量逐层增大，访问速度逐层变慢，而单位存储成本也逐层下降，也就构成了我们日常所说的存储器层次结构。</strong></p>
<h3 id="使用存储器的时候，该如何权衡价格和性能？"><a href="#使用存储器的时候，该如何权衡价格和性能？" class="headerlink" title="使用存储器的时候，该如何权衡价格和性能？"></a>使用存储器的时候，该如何权衡价格和性能？</h3><p>存储器在不同层级之间的性能差异和价格差异，都至少在一个数量级以上。L1 Cache 的访问延时是 1 纳秒（ns），而内存就已经是 100 纳秒了。在价格上，这两者也差出了 400 倍。</p>
<p>因为这个价格和性能的差异，你会看到，我们实际在进行电脑硬件配置的时候，会去组合配置各种存储设备。</p>
<p>我们可以找一台现在主流的笔记本电脑来看看，比如，一款入门级的惠普战 66 的笔记本电脑。今天在京东上的价格是 4999 人民币。它的配置是下面这样的。</p>
<ol>
<li>Intle i5-8265U 的 CPU（这是一块 4 核的 CPU）</li>
</ol>
<ul>
<li>这块 CPU 每个核有 32KB，一共 128KB 的 L1 指令 Cache。</li>
<li>同样，每个核还有 32KB，一共 128KB 的 L1 数据 Cache，指令 Cache 和数据 Cache 都是采用 8 路组相连的放置策略。</li>
<li>每个核有 256KB，一共 1MB 的 L2 Cache。L2 Cache 是用 4 路组相连的放置策略。</li>
<li>最后还有一块多个核心共用的 12MB 的 L3 Cache，采用的是 12 路组相连的放置策略。</li>
</ul>
<ol start="2">
<li><p>8GB 的内存</p>
</li>
<li><p>一块 128G 的 SSD 硬盘</p>
</li>
<li><p>一块 1T 的 HDD 硬盘</p>
</li>
</ol>
<p>你可以看到，在一台实际的计算机里面，越是速度快的设备，容量就越小。这里一共十多兆的 Cache，成本只是几十美元。而 8GB 的内存、128G 的 SSD 以及 1T 的 HDD，大概零售价格加在一起，也就和我们的高速缓存的价格差不多。</p>
<h3 id="总结延伸"><a href="#总结延伸" class="headerlink" title="总结延伸"></a>总结延伸</h3><p>我们常常把 CPU 比喻成高速运转的大脑，那么和大脑同步的寄存器（Register），就存放着我们当下正在思考和处理的数据。<strong>而 L1-L3 的 CPU Cache，好比存放在我们大脑中的短期到长期的记忆。</strong>我们需要小小花费一点时间，就能调取并进行处理。</p>
<p>我们自己的书桌书架就好比计算机的内存，能放下更多的书也就是数据，但是找起来和看起来就要慢上不少。而图书馆更像硬盘这个外存，能够放下更多的数据，找起来也更费时间。<strong>从寄存器、CPU Cache，到内存、硬盘，这样一层层下来的存储器，速度越来越慢，空间越来越大，价格也越来越便宜。</strong></p>
]]></content>
      <categories>
        <category>计算机组成原理</category>
      </categories>
      <tags>
        <tag>寄存器</tag>
        <tag>cache</tag>
      </tags>
  </entry>
  <entry>
    <title>机械硬盘</title>
    <url>/posts/96e2fa17/</url>
    <content><![CDATA[<h1 id="机械硬盘：Google早期用过的“黑科技”"><a href="#机械硬盘：Google早期用过的“黑科技”" class="headerlink" title="机械硬盘：Google早期用过的“黑科技”"></a>机械硬盘：Google早期用过的“黑科技”</h1><h2 id="拆解机械硬盘"><a href="#拆解机械硬盘" class="headerlink" title="拆解机械硬盘"></a>拆解机械硬盘</h2><p>一块机械硬盘是由盘面、磁头和悬臂三个部件组成的。下面我们一一来看每一个部件<span id="more"></span>。</p>
<img src="/img/computer_img/机械硬盘构造.webp" alt="机械硬盘构造" style="zoom:50%;" />

<ul>
<li>首先，自然是盘面（Disk Platter）。我们平时买硬盘的时候经常会听到一个指标，叫作这个硬盘的转速。我们的硬盘有 5400 转的、7200 转的，乃至 10000 转的。这个多少多少转，指的就是盘面中间电机控制的转轴的旋转速度，英文单位叫 RPM，也就是每分钟的旋转圈数（Rotations Per Minute）。所谓 7200 转，其实更准确地说是 7200RPM，指的就是一旦电脑开机供电之后，我们的硬盘就可以一直做到每分钟转上 7200 圈。如果折算到每一秒钟，就是 120 圈。</li>
<li>我们来看磁头（Drive Head）。我们的数据并不能直接从盘面传输到总线上，而是通过磁头，从盘面上读取到，然后再通过电路信号传输给控制电路、接口，再到总线上的。         通常，我们的一个盘面上会有两个磁头，分别在盘面的正反面。盘面在正反两面都有对应的磁性涂层来存储数据，而且一块硬盘也不是只有一个盘面，而是上下堆叠了很多个盘面，各个盘面之间是平行的。每个盘面的正反两面都有对应的磁头。</li>
<li>最后我们来看悬臂（Actutor Arm）。悬臂链接在磁头上，并且在一定范围内会去把磁头定位到盘面的某个特定的磁道（Track）上。</li>
</ul>
<p>读取数据，分为两个步骤：</p>
<ol>
<li>就是把盘面旋转到某一个位置。在这个位置上，我们的悬臂可以定位到整个盘面的某一个子区间。这个子区间的形状有点儿像一块披萨饼，我们一般把这个区间叫作几何扇区（Geometrical Sector），意思是，在“几何位置上”，所有这些扇区都可以被悬臂访问到。</li>
<li>就是把我们的悬臂移动到特定磁道的特定扇区，也就在这个“几何扇区”里面，找到我们实际的扇区。找到之后，我们的磁头会落下，就可以读取到正对着扇区的数据。</li>
</ol>
<img src="/img/computer_img/数据扇区.webp" alt="数据扇区" style="zoom:50%;" />



<p>所以，我们进行一次磁盘上的随机访问，需要的时间由两个部分组成。</p>
<ol>
<li><p>第一个部分，叫作平均延时（Average Latency）。这个时间，其实就是把我们的盘面旋转，把几何扇区对准悬臂位置的时间。这个时间很容易计算，它其实就和我们机械硬盘的转速相关。随机情况下，平均找到一个几何扇区，我们需要旋转半圈盘面。上面 7200 转的硬盘，那么一秒里面，就可以旋转 240 个半圈。那么，这个平均延时就是 </p>
<p>​                    1s &#x2F; 240 &#x3D; 4.17ms</p>
</li>
<li><p>第二个部分，叫作平均寻道时间（Average Seek Time），也就是在盘面选转之后，我们的悬臂定位到扇区的的时间。我们现在用的 HDD 硬盘的平均寻道时间一般在 4-10ms。</p>
</li>
</ol>
<p>这样，我们就能够算出来，如果随机在整个硬盘上找一个数据，需要 8-14 ms。我们的硬盘是机械结构的，只有一个电机转轴，也只有一个悬臂，所以我们没有办法并行地去定位或者读取数据。那一块 7200 转的硬盘，我们一秒钟随机的 IO 访问次数，也就是 ：</p>
<p>​                    1s &#x2F; 8 ms &#x3D; 125 IOPS 或者 1s &#x2F; 14ms &#x3D; 70 IOPS</p>
<blockquote>
<p>现在，你明白我们上一讲所说的，HDD 硬盘的 IOPS 每秒 100 次左右是怎么来的吧？好了，现在你再思考一个问题。如果我们不是去进行随机的数据访问，而是进行顺序的数据读写，我们应该怎么最大化读取效率呢？</p>
<p>我们可以选择把顺序存放的数据，尽可能地存放在同一个柱面上。这样，我们只需要旋转一次盘面，进行一次寻道，就可以去写入或者读取，同一个垂直空间上的多个盘面的数据。<strong>如果一个柱面上的数据不够，我们也不要去动悬臂，而是通过电机转动盘面</strong>，这样就可以顺序读完一个磁道上的所有数据。   所以，其实对于 HDD 硬盘的顺序数据读写，吞吐率还是很不错的，可以达到 200MB&#x2F;s 左右。</p>
</blockquote>
<h2 id="Partial-Stroking：根据场景提升性能"><a href="#Partial-Stroking：根据场景提升性能" class="headerlink" title="Partial Stroking：根据场景提升性能"></a>Partial Stroking：根据场景提升性能</h2><p>只有 100 的 IOPS，其实很难满足现在互联网海量高并发的请求。所以，今天的数据库，都会把数据存储在 SSD 硬盘上。不过，如果我们把时钟倒播 20 年，那个时候，我们可没有现在这么便宜的 SSD 硬盘。数据库里面的数据，只能存放在 HDD 硬盘上。</p>
<p>今天，即便是数据中心用的 HDD 硬盘，一般也是 7200 转的，因为如果要更快的随机访问速度，我们会选择用 SSD 硬盘。但是在当时，SSD 硬盘价格非常昂贵，还没有能够商业化。硬盘厂商们在不断地研发转得更快的硬盘。在数据中心里，往往我们会用上 10000 转，乃至 15000 转的硬盘。甚至直到 2010 年，SSD 硬盘已经开始逐步进入市场了，西数还在尝试研发 20000 转的硬盘。转速更高、寻道时间更短的机械硬盘，才能满足实际的数据库需求。</p>
<p>不过，10000 转，乃至 15000 转的硬盘也更昂贵。如果你想要节约成本，提高性价比，那就得想点别的办法。你应该听说过，<em>Google 早年用家用 PC 乃至二手的硬件，通过软件层面的设计来解决可靠性和性能的问题。</em>那么，我们是不是也有什么办法，能提高机械硬盘的 IOPS 呢？</p>
<p>还真的有。这个方法，就叫作 <strong>Partial Stroking 或者 Short Stroking。</strong>我没有看到过有中文资料给这个方法命名。在这里，我就暂时把它翻译成“缩短行程”技术。</p>
<p>其实这个方法的思路很容易理解，我一说你就明白了。既然我们访问一次数据的时间，是“平均延时 + 寻道时间”，那么只要能缩短这两个之一，不就可以提升 IOPS 了吗？</p>
<p>一般情况下，硬盘的寻道时间都比平均延时要长。那么我们自然就可以想一下，有什么办法可以缩短平均的寻道时间。最极端的办法就是我们不需要寻道，也就是说，我们把所有数据都放在一个磁道上。比如，我们始终把磁头放在最外道的磁道上。这样，我们的寻道时间就基本为 0，访问时间就只有平均延时了。那样，我们的 IOPS，就变成了</p>
<p>​                                        1s &#x2F; 4ms &#x3D; 250 IOPS</p>
<p>不过呢，只用一个磁道，我们能存的数据就比较有限了。这个时候，可能我们还不如把这些数据直接都放到内存里面呢。所以，实践当中，我们可以只用 1&#x2F;2 或者 1&#x2F;4 的磁道，也就是最外面 1&#x2F;4 或者 1&#x2F;2 的磁道。这样，我们硬盘可以使用的容量可能变成了 1&#x2F;2 或者 1&#x2F;4。但是呢，我们的寻道时间，也变成了 1&#x2F;4 或者 1&#x2F;2，因为悬臂需要移动的“行程”也变成了原来的 1&#x2F;2 或者 1&#x2F;4，我们的 IOPS 就能够大幅度提升了。</p>
<p>比如说，我们一块 7200 转的硬盘，正常情况下，平均延时是 4.17ms，而寻道时间是 9ms。那么，它原本的 IOPS 就是</p>
<p>​                        1s &#x2F; (4.17ms + 9ms) &#x3D; 75.9 IOPS</p>
<p>如果我们只用其中 1&#x2F;4 的磁道，那么，它的 IOPS 就变成了</p>
<p>​                        1s &#x2F; (4.17ms + 9ms&#x2F;4) &#x3D; 155.8 IOPS</p>
<p>你看这个结果，IOPS 提升了一倍，和一块 15000 转的硬盘的性能差不多了。不过，这个情况下，我们的硬盘能用的空间也只有原来的 1&#x2F;4 了。</p>
<p>不过，要知道在当时，同样容量的 15000 转的硬盘的价格可不止是 7200 转硬盘的 4 倍啊。所以，这样通过软件去格式化硬盘，只保留部分磁道让系统可用的情况，可以大大提升硬件的性价比 。</p>
<h2 id="总结延伸"><a href="#总结延伸" class="headerlink" title="总结延伸"></a>总结延伸</h2><p>机械硬盘的硬件，主要由盘面、磁头和悬臂三部分组成。我们的数据在盘面上的位置，可以通过磁道、扇区和柱面来定位。实际的一次对于硬盘的访问，需要把盘面旋转到某一个“几何扇区”，对准悬臂的位置。然后，悬臂通过寻道，把磁头放到我们实际要读取的扇区上。</p>
<p>受制于机械硬盘的结构，我们对于随机数据的访问速度，就要包含旋转盘面的平均延时和移动悬臂的寻道时间。通过这两个时间，我们能计算出机械硬盘的 IOPS。</p>
<p>7200 转机械硬盘的 IOPS，只能做到 100 左右。在互联网时代的早期，我们也没有 SSD 硬盘可以用，所以工程师们就想出了 Partial Stroking 这个浪费存储空间，但是可以缩短寻道时间来提升硬盘的 IOPS 的解决方案。这个解决方案，也是一个典型的、在深入理解了硬件原理之后的软件优化方案。</p>
]]></content>
      <categories>
        <category>计算机组成原理</category>
      </categories>
      <tags>
        <tag>存储器</tag>
      </tags>
  </entry>
  <entry>
    <title>SSD硬盘（上）</title>
    <url>/posts/ad3b44ae/</url>
    <content><![CDATA[<h1 id="SSD硬盘（上）：如何完成性能优化的KPI？"><a href="#SSD硬盘（上）：如何完成性能优化的KPI？" class="headerlink" title="SSD硬盘（上）：如何完成性能优化的KPI？"></a>SSD硬盘（上）：如何完成性能优化的KPI？</h1><blockquote>
<p>随着智能手机的出现，互联网用户在 2008 年之后开始爆发性增长，大家在网上花的时间也越来越多。这也就意味着，隐藏在精美 App 和网页之后的服务端数据请求量<span id="more"></span>，呈数量级的上升。无论是用 10000 转的企业级机械硬盘，还是用 t Stroking 这样的方式进一步提升 IOPS，HDD 硬盘已经满足不了我们的需求了。上面这些优化措施，无非就是，把 IOPS 从 100 提升到 300、500 也就到头了。</p>
<p>SSD 硬盘在 2010 年前后，进入了主流的商业应用。我们在第 44 讲看过，一块普通的 SSD 硬盘，可以轻松支撑 10000 乃至 20000 的 IOPS。那个时候，不少互联网公司想要完成性能优化的 KPI，最后的解决方案都变成了换 SSD 的硬盘。如果这还不够，那就换上使用 PCI Express 接口的 SSD。</p>
</blockquote>
<h2 id="SSD-的读写原理"><a href="#SSD-的读写原理" class="headerlink" title="SSD 的读写原理"></a>SSD 的读写原理</h2><p>SSD 没有像机械硬盘那样的寻道过程，所以它的随机读写都更快。我在下面列了一个表格，对比了一下 SSD 和机械硬盘的优缺点。<br><img src="/img/computer_img/SSD.webp" alt="SSD" style="zoom:50%;" /></p>
<p>你会发现，不管是机械硬盘不擅长的随机读写，还是它本身已经表现不错的顺序写入，SSD 在这些方面都要比 HDD 强。不过，有一点，机械硬盘要远强于 SSD，那就是耐用性。如果我们需要频繁地重复写入删除数据，那么机械硬盘要比 SSD 性价比高很多。</p>
<p>要想知道为什么 SSD 的耐用性不太好，我们先要理解 SSD 硬盘的存储和读写原理。我们之前说过，CPU Cache 用的 SRAM 是用一个电容来存放一个比特的数据。<strong>对于 SSD 硬盘，我们也可以先简单地认为，它是由一个电容加上一个电压计组合在一起，记录了一个或者多个比特。</strong></p>
<h2 id="SLC、MLC、TLC-和-QLC"><a href="#SLC、MLC、TLC-和-QLC" class="headerlink" title="SLC、MLC、TLC 和 QLC"></a>SLC、MLC、TLC 和 QLC</h2><p>能够记录一个比特很容易理解。给电容里面充上电有电压的时候就是 1，给电容放电里面没有电就是 0。采用这样方式存储数据的 SSD 硬盘，我们一般称之为使用了 SLC 的颗粒，全称是 Single-Level Cell，也就是一个存储单元中只有一位数据。</p>
<img src="/img/computer_img/SSD电容.webp" alt="SSD电容" style="zoom:50%;" />

<p>但是，这样的方式会遇到和 CPU Cache 类似的问题，那就是，同样的面积下，能够存放下的元器件是有限的。如果只用 SLC，我们就会遇到，存储容量上不去，并且价格下不来的问题。于是呢，硬件工程师们就陆续发明了 MLC（Multi-Level Cell）、TLC（Triple-Level Cell）以及 QLC（Quad-Level Cell），也就是能在一个电容里面存下 2 个、3 个乃至 4 个比特。</p>
<img src="/img/computer_img/电压比特.webp" alt="电压比特" style="zoom:50%;" />

<p>只有一个电容，我们怎么能够表示更多的比特呢？别忘了，这里我们还有一个电压计。4 个比特一共可以从 0000-1111 表示 16 个不同的数。那么，如果我们能往电容里面充电的时候，充上 15 个不同的电压，并且我们电压计能够区分出这 15 个不同的电压。加上电容被放空代表的 0，就能够代表从 0000-1111 这样 4 个比特了。</p>
<p>不过，要想表示 15 个不同的电压，充电和读取的时候，对于精度的要求就会更高。这会导致充电和读取的时候都更慢，所以 QLC 的 SSD 的读写速度，要比 SLC 的慢上好几倍。如果你想要知道是什么样的物理原理导致这个 QLC 更慢，可以去读一读这篇<a href="https://www.anandtech.com/show/5067/understanding-tlc-nand/2">文章</a>。</p>
<hr>
<h2 id="P-x2F-E-擦写问题"><a href="#P-x2F-E-擦写问题" class="headerlink" title="P&#x2F;E 擦写问题"></a>P&#x2F;E 擦写问题</h2><p>如果我们去看一看 SSD 硬盘的硬件构造，可以看到，它大概是自顶向下是这么构成的。</p>
<img src="/img/computer_img/SSD 物理构造.webp" alt="SSD 物理构造" style="zoom:50%;" />

<p>首先，自然和其他的 I&#x2F;O 设备一样，它有对应的接口和控制电路。现在的 SSD 硬盘用的是 SATA 或者 PCI Express 接口。在控制电路里，有一个很重要的模块，叫作 FTL（Flash-Translation Layer），也就是闪存转换层。这个可以说是 SSD 硬盘的一个核心模块，SSD 硬盘性能的好坏，很大程度上也取决于 FTL 的算法好不好。现在容我卖个关子，我们晚一会儿仔细讲 FTL 的功能。</p>
<p>接下来是实际 I&#x2F;O 设备，它其实和机械硬盘很像。现在新的大容量 SSD 硬盘都是 3D 封装的了，也就是说，是由很多个裸片（Die）叠在一起的，就好像我们的机械硬盘把很多个盘面（Platter）叠放再一起一样，这样可以在同样的空间下放下更多的容量。</p>
<img src="/img/computer_img/SSD盘面.webp" alt="SSD盘面" style="zoom:50%;" />

<p>接下来，一张裸片上可以放多个平面（Plane），一般一个平面上的存储容量大概在 GB 级别。一个平面上面，会划分成很多个块（Block），一般一个块（Block）的存储大小， 通常几百 KB 到几 MB 大小。一个块里面，还会区分很多个页（Page），就和我们内存里面的页一样，一个页的大小通常是 4KB。</p>
<p>在这一层一层的结构里面，处在最下面的两层块和页非常重要</p>
<p>对于 SSD 硬盘来说，数据的写入叫作 Program。写入不能像机械硬盘一样，通过<strong>覆写（Overwrite）</strong>来进行的，而是要先去<strong>擦除（Erase）</strong>，然后再写入。</p>
<p>SSD 的读取和写入的基本单位，不是一个比特（bit）或者一个字节（byte），而是一个<strong>页（Page）</strong>。SSD 的擦除单位就更夸张了，我们不仅不能按照比特或者字节来擦除，连按照页来擦除都不行，我们必须按照<strong>块</strong>来擦除。</p>
<p><em>而且，你必须记住的一点是，SSD 的使用寿命，其实是每一个块（Block）的擦除的次数。</em>（你可以把 SSD 硬盘的一个平面看成是一张白纸。我们在上面写入数据，就好像用铅笔在白纸上写字。如果想要把已经写过字的地方写入新的数据，我们先要用橡皮把已经写好的字擦掉。但是，如果频繁擦同一个地方，那这个地方就会破掉，之后就没有办法再写字了。）</p>
<blockquote>
<p>我们上面说的 SLC 的芯片，可以擦除的次数大概在 10 万次，MLC 就在 1 万次左右，而 TLC 和 QLC 就只在几千次了。这也是为什么，你去购买 SSD 硬盘，会看到同样的容量的价格差别很大，因为它们的芯片颗粒和寿命完全不一样。</p>
</blockquote>
<hr>
<h2 id="SSD-读写的生命周期"><a href="#SSD-读写的生命周期" class="headerlink" title="SSD 读写的生命周期"></a>SSD 读写的生命周期</h2><ul>
<li>我用三种颜色分别来表示 SSD 硬盘里面的页的不同状态，白色代表这个页从来没有写入过数据，绿色代表里面写入的是有效的数据，红色代表里面的数据，在我们的操作系统看来已经是删除的了。</li>
</ul>
<img src="/img/computer_img/SSD 擦写.webp" alt="SSD 擦写" style="zoom:67%;" />

<p>一开始，所有块的每一个页都是白色的。随着我们开始往里面写数据，里面的有些页就变成了绿色。</p>
<p>然后，因为我们删除了硬盘上的一些文件，所以有些页变成了红色。但是这些红色的页，并不能再次写入数据。因为 SSD 硬盘不能单独擦除一个页，必须一次性擦除整个块，所以新的数据，我们只能往后面的白色的页里面写。这些散落在各个绿色空间里面的红色空洞，就好像硬盘碎片。</p>
<p>如果有哪一个块的数据一次性全部被标红了，那我们就可以把整个块进行擦除。它就又会变成白色，可以重新一页一页往里面写数据。这种情况其实也会经常发生。毕竟一个块不大，也就在几百 KB 到几 MB。你删除一个几 MB 的文件，数据又是连续存储的，自然会导致整个块可以被擦除。</p>
<p>随着硬盘里面的数据越来越多，红色空洞占的地方也会越来越多。于是，你会发现，我们就要没有白色的空页去写入数据了。这个时候，我们要做一次类似于 Windows 里面“磁盘碎片整理”或者 Java 里面的“内存垃圾回收”工作。   <em><strong>找一个红色空洞最多的块，把里面的绿色数据，挪到另一个块里面去，然后把整个块擦除，变成白色，可以重新写入数据。</strong></em></p>
<p>不过，这个“磁盘碎片整理”或者“内存垃圾回收”的工作，我们不能太主动、太频繁地去做。因为 SSD 的擦除次数是有限的。如果动不动就搞个磁盘碎片整理，那么我们的 SSD 硬盘很快就会报废了。</p>
<p>说到这里，你可能要问了，这是不是说，我们的 SSD 硬盘的容量是用不满的？因为我们总会遇到一些红色空洞？</p>
<p>没错，一块 SSD 的硬盘容量，是没办法完全用满的。   不过，为了不得罪消费者，生产 SSD 硬盘的厂商，其实是预留了一部分空间，<em><strong>专门用来做这个“磁盘碎片整理”工作的。一块标成 240G 的 SSD 硬盘，往往实际有 256G 的硬盘空间。</strong></em>     SSD 硬盘通过我们的控制芯片电路，把多出来的硬盘空间，用来进行各种数据的闪转腾挪，让你能够写满那 240G 的空间。这个多出来的 16G 空间，叫作<strong>预留空间</strong>（Over Provisioning），一般 SSD 的硬盘的预留空间都在 7%-15% 左右。</p>
<h2 id="总结延伸"><a href="#总结延伸" class="headerlink" title="总结延伸"></a>总结延伸</h2><hr>
<p>到这里，相信你对 SSD 硬盘的写入和擦除的原理已经清楚了，也明白了 SSD 硬盘的使用寿命受限于可以擦除的次数。</p>
<p>仔细想一想，你会发现 SSD 硬盘，特别适合读多写少的应用。在日常应用里面，我们的系统盘适合用 SSD。但是，如果我们用 SSD 做专门的下载盘，一直下载各种影音数据，然后刻盘备份就不太好了，特别是现在 QLC 颗粒的 SSD，它只有几千次可擦写的寿命啊。</p>
<ul>
<li>在数据中心里面，SSD 的应用场景也是适合读多写少的场景。</li>
<li>我们拿 SSD 硬盘用来做数据库，存放电商网站的商品信息很合适。但是，用来作为 Hadoop 这样的 Map-Reduce 应用的数据盘就不行了。因为 Map-Reduce 任务会大量在任务中间向硬盘写入中间数据再删除掉，这样用不了多久，SSD 硬盘的寿命就会到了</li>
</ul>
]]></content>
      <categories>
        <category>计算机组成原理</category>
      </categories>
      <tags>
        <tag>存储器</tag>
        <tag>SSD</tag>
      </tags>
  </entry>
  <entry>
    <title>SSD硬盘（下）</title>
    <url>/posts/158723cb/</url>
    <content><![CDATA[<h1 id="SSD硬盘（下）：如何完成性能优化的KPI？"><a href="#SSD硬盘（下）：如何完成性能优化的KPI？" class="headerlink" title="SSD硬盘（下）：如何完成性能优化的KPI？"></a>SSD硬盘（下）：如何完成性能优化的KPI？</h1><p>如果你平时用的是 Windows 电脑，你会发现，用了 SSD 的系统盘，就不能用磁盘碎片整理功能。    这是因为，一旦主动去运行磁盘碎片整理功能，就会发生一次块的擦除<span id="more"></span>，对应块的寿命就少了一点点。</p>
<p>我们的操作系统上，并没有 SSD 硬盘上各个块目前已经擦写的情况和寿命，所以它对待 SSD 硬盘和普通的机械硬盘没有什么区别。</p>
<p>我们日常使用 PC 进行软件开发的时候，会先在硬盘上装上操作系统和常用软件，比如 Office，或者工程师们会装上 VS Code、WebStorm 这样的集成开发环境。这些软件所在的块，写入一次之后，就不太会擦除了，所以<strong>就只有读的需求</strong>。</p>
<p>一旦开始开发，我们就会不断添加新的代码文件，还会不断修改已经有的代码文件。因为 SSD 硬盘没有<strong>覆写（Override）</strong>的功能，所以，这个过程中，其实我们是在反复地写入新的文件，然后再把原来的文件标记成逻辑上删除的状态。  等 SSD 里面空的块少了，我们会用“垃圾回收”的方式，进行擦除。这样，我们的擦除会反复发生在这些用来存放数据的地方。</p>
<img src="/img/computer_img/SSD 读写区.webp" alt="SSD 读写区" style="zoom:50%;" />

<p>有一天，这些块的擦除次数到了，变成了坏块。但是，我们安装操作系统和软件的地方还没有坏，而这块硬盘的可以用的容量却变小了。</p>
<hr>
<h2 id="磨损均衡、TRIM-和写入放大效应"><a href="#磨损均衡、TRIM-和写入放大效应" class="headerlink" title="磨损均衡、TRIM 和写入放大效应"></a>磨损均衡、TRIM 和写入放大效应</h2><h3 id="FTL-和磨损均衡"><a href="#FTL-和磨损均衡" class="headerlink" title="FTL 和磨损均衡"></a>FTL 和磨损均衡</h3><p>那么，我们有没有什么办法，不让这些坏块那么早就出现呢？我们能不能，匀出一些存放操作系统的块的擦写次数，给到这些存放数据的地方呢？</p>
<p>相信你一定想到了，其实我们要的就是想一个办法，让 SSD 硬盘各个块的擦除次数，均匀分摊到各个块上。这个策略呢，就叫作<strong>磨损均衡</strong>（Wear-Leveling）。</p>
<p>实现这个技术的核心办法，和我们前面讲过的虚拟内存一样，就是添加一个间接层。这个间接层，就是我们上一讲给你卖的那个关子，就是 FTL 这个<strong>闪存转换层</strong>。</p>
<img src="/img/computer_img/FTL 闪寸转换层.webp" alt="FTL 闪寸转换层" style="zoom:50%;" />

<p>就像在管理杯纯的时候，我们通过一个页表映射虚拟内存页和物理页一样，在 FTL 里面，存放了 <strong>逻辑块地址</strong>（Logical Block Address ）到 <strong>物理块地址</strong> 的映射。</p>
<p>操作系统访问的硬盘地址，其实都是逻辑地址。只有通过 FTL 转换之后，才会变成实际的物理地址，找到对应的块进行访问。操作系统本身，不需要去考虑块的磨损程度，只要和操作机械硬盘一样来读写数据就好了。</p>
<hr>
<p>操作系统所有对于 SSD 硬盘的读写请求，都要经过 FTL。FTL 里面又有逻辑块对应的物理块，所以 FTL 能够记录下来，每个物理块被擦写的次数。<em>如果一个物理块被擦写的次数多了，FTL 就可以将这个物理块，挪到一个擦写次数少的物理块上。但是，逻辑块不用变，操作系统也不需要知道这个变化。</em></p>
<p>这也是我们在设计大型系统中的一个典型思路，<em><strong>也就是各层之间是隔离的，操作系统不需要考虑底层的硬件是什么，</strong></em>完全交由硬件的控制电路里面的 FTL，来管理对于实际物理硬件的写入。</p>
<h3 id="TRIM-指令的支持"><a href="#TRIM-指令的支持" class="headerlink" title="TRIM 指令的支持"></a>TRIM 指令的支持</h3><p>不过，操作系统不去关心实际底层的硬件是什么，在 SSD 硬盘的使用上，也会带来一个问题。这个问题就是，操作系统的逻辑层和 SSD 的逻辑层里的块状态，是不匹配的。</p>
<p>我们在操作系统里面去删除一个文件，其实并没有真的在物理层面去删除这个文件，只是在文件系统里面，把对应的 inode 里面的元信息清理掉，这代表这个 inode 还可以继续使用，可以写入新的数据。这个时候，实际物理层面的对应的存储空间，在操作系统里面被标记成可以写入了。</p>
<p>所以，其实我们日常的文件删除，都只是一个操作系统层面的逻辑删除。这也是为什么，很多时候我们不小心删除了对应的文件，我们可以通过各种恢复软件，把数据找回来。同样的，这也是为什么，如果我们想要删除干净数据，需要用各种“文件粉碎”的功能才行。</p>
<p>这个删除的逻辑在机械硬盘层面没有问题，因为文件被标记成可以写入，后续的写入可以直接覆写这个位置。但是，在 SSD 硬盘上就不一样了。我在这里放了一张详细的示意图。我们下面一起来看看具体是怎么回事儿。</p>
<img src="/img/computer_img/ssd 删除.webp" alt="ssd 删除" style="zoom:50%;" />

<p>一开始，操作系统里面有好几个文件，不同的文件我用不同的颜色标记出来了。下面的 SSD 的逻辑块里面占用的页，我们也用同样的颜色标记出来文件占用的对应页。</p>
<p>当我们在操作系统里面，删除掉一个刚刚下载的文件，比如标记成黄色 openjdk.exe 这样一个 jdk 的安装文件，在操作系统里面，对应的 inode 里面，就没有文件的元信息。</p>
<p>但是，这个时候，我们的 SSD 的逻辑块层面，其实并不知道这个事情。所以在，逻辑块层面，openjdk.exe 仍然是占用了对应的空间。对应的物理页，也仍然被认为是被占用了的。</p>
<p>为了解决这个问题，现在的操作系统和 SSD 的主控芯片，都支持 TRIM 命令。这个命令可以在文件被删除的时候，让操作系统去通知 SSD 硬盘，对应的逻辑块已经标记成已删除了。现在的 SSD 硬盘都已经支持了 TRIM 命令。无论是 Linux、Windows 还是 MacOS，这些操作系统也都已经支持了 TRIM 命令了。</p>
<hr>
<h3 id="写入放大"><a href="#写入放大" class="headerlink" title="写入放大"></a>写入放大</h3><p>其实，TRIM 命令的发明，也反应了一个使用 SSD 硬盘的问题，那就是，SSD 硬盘容易越用越慢。</p>
<p>当 SSD 硬盘的存储空间被占用得越来越多，每一次写入新数据，我们都可能没有足够的空白。我们可能不得不去进行垃圾回收，合并一些块里面的页，然后再擦除掉一些页，才能匀出一些空间来。</p>
<p>这个时候，从应用层或者操作系统层面来看，我们可能只是写入了一个 4KB 或者 4MB 的数据。但是，实际通过 FTL 之后，我们可能要去搬运 8MB、16MB 甚至更多的数据。</p>
<p>我们通过 <strong>“实际的闪存写入的数据量 &#x2F; 系统通过 FTL 写入的数据量 &#x3D; 写入放大”</strong> ，可以得到，写入放大的倍数越多，意味着实际的 SSD 性能也就越差，会远远比不上实际 SSD 硬盘标称的指标。</p>
<p>解决写入放大，需要我们在后台定时进行垃圾回收，在硬盘比较空闲的时候，就把搬运数据、擦除数据、刘处空白的块的工作做完，而不是等实际数据写入的时候 ，在进行这样的操作。</p>
<hr>
<h2 id="AeroSpike：如何最大化-SSD-的使用效率？"><a href="#AeroSpike：如何最大化-SSD-的使用效率？" class="headerlink" title="AeroSpike：如何最大化 SSD 的使用效率？"></a>AeroSpike：如何最大化 SSD 的使用效率？</h2><p>讲到这里，相信你也发现了，想要把 SSD 硬盘用好，其实没有那么简单。如果我们只是简单地拿一块 SSD 硬盘替换掉原来的 HDD 硬盘，而不是从应用层面考虑任何 SSD 硬盘特性的话，我们多半还是没法获得想要的性能提升。</p>
<p>不过，既然清楚了 SSD 硬盘的各种特性，我们就可以依据这些特性，来设计我们的应用。接下来，我就带你一起看一看，AeroSpike 这个专门针对 SSD 硬盘特性设计的  <strong>Key-Value 数据库（键值对数据库）</strong>，是怎么利用这些物理特性的。</p>
<p>首先，AeroSpike 操作 SSD 硬盘，并没有通过操作系统的文件系统。而是直接操作 SSD 里面的块和页。因为操作系统里面的文件系统，对于 KV 数据库来说，只是让我们多了一层间接层，只会降低性能，对我们没有什么实际的作用。</p>
<p>其次，AeroSpike 在读写数据的时候，做了两个优化。在写入数据的时候，AeroSpike 尽可能去写一个较大的数据块，而不是频繁的去写很多小的数据块，这样，磁盘就不太容易频繁出现磁盘碎片，而且一次性写入一个大的数据块， 也更容易利用好顺序写入的性能优势。AeroSpike 写入的一个数据块，是 128 KB ,远比一个页的 4KB 要大得多。</p>
<p>另外，在读取数据的时候，AeroSpike 倒是可以读取 512 字节（Bytes）这样的小数据。  因为 SSD 的随机读取性能很好，也不像写入数据那样有擦除寿命问题。<strong>而且，很多时候我们读取的数据是键值对里面的值的数据，这些数据要在网络上传输。</strong>如果一次性必须读出比较大的数据，就会导致我们的网络带宽不够用。</p>
<p>因为 AeroSpike 是一个对于响应时间要求很高的实时 KV 数据库，如果出现了<em>严重的写放大效应</em>，会导致写入数据的响应时间大幅度变长。所以 AeroSpike 做了这样几个动作：</p>
<ul>
<li>第一个是持续地进行磁盘碎片整理。AeroSpike 用了所谓的高水位（High Watermark）算法。其实这个算法很简单，就是一旦一个物理块里面的数据碎片超过 50%，就把这个物理块搬运压缩，然后进行数据擦除，确保磁盘始终有足够的空间可以写入。</li>
<li>第二个是在 AeroSpike 给出的最佳实践中，为了保障数据库的性能，建议你只用到 SSD 硬盘标定容量的一半。也就是说，我们人为地给 SSD 硬盘预留了 50% 的预留空间，以确保 SSD 硬盘的写放大效应尽可能小，不会影响数据库的访问性能。</li>
</ul>
<img src="/img/computer_img/SSD AeroSpike.webp" alt="SSD AeroSpike" style="zoom:50%;" />

<p>正是因为做了这种种的优化，在 NoSQL 数据库刚刚兴起的时候，AeroSpike 的性能把 Cassandra、MongoDB 这些数据库远远甩在身后，和这些数据库之间的性能差距，有时候会到达一个数量级。 <em>这也让 AeroSpike 成为了当时高性能 KV 数据库的标杆。</em>你可以看一看 InfoQ 出的这个 <a href="https://www.infoq.com/news/2013/04/NoSQL-Benchmark/">Benchmark</a>，里面有 2013 年的时候，这几个 NoSQL 数据库巨大的性能差异。</p>
<h2 id="总结延伸"><a href="#总结延伸" class="headerlink" title="总结延伸"></a>总结延伸</h2><p>因为 SSD 硬盘的使用寿命，受限于块的擦除次数，所以我们需要通过一个磨损均衡的策略，来管理 SSD 硬盘的各个块的擦除次数。我们通过在逻辑块地址和物理块地址之间，引入 FTL 这个映射层，使得操作系统无需关心物理块的擦写次数，而是由 FTL 里的软件算法，来协调到底每一次写入应该磨损哪一块。</p>
<p>除了磨损均衡之外，操作系统和 SSD 硬件的特性还有一个不匹配的地方。那就是，<em>操作系统在删除数据的时候，并没有真的删除物理层面的数据，而只是修改了 inode 里面的数据。这个“伪删除”，使得 SSD 硬盘在逻辑和物理层面，都没有意识到有些块其实已经被删除了。</em>这就导致在垃圾回收的时候，会浪费很多不必要的读写资源。</p>
<p>SSD 这个需要进行垃圾回收的特性，使得我们在写入数据的时候，会遇到写入放大。明明我们只是写入了 4MB 的数据，可能在 SSD 的硬件层面，实际写入了 8MB、16MB 乃至更多的数据。</p>
<p>针对这些特性，AeroSpike，这个专门针对 SSD 硬盘特性的 KV 数据库，设计了很多的优化点，包括跳过文件系统直写硬盘、写大块读小块、用高水位算法持续进行磁盘碎片整理，以及只使用 SSD 硬盘的一半空间。这些策略，使得 AeroSpike 的性能，在早年间远远超过了 Cassandra 等其他 NoSQL 数据库。</p>
<blockquote>
<p>可以看到，针对硬件特性设计的软件，才能最大化发挥我们的硬件性能。</p>
</blockquote>
]]></content>
      <categories>
        <category>计算机组成原理</category>
      </categories>
      <tags>
        <tag>存储器</tag>
        <tag>SSD</tag>
      </tags>
  </entry>
  <entry>
    <title>DMA(Direct Memory Access)</title>
    <url>/posts/ee2bfcc7/</url>
    <content><![CDATA[<h1 id="DMA：为什么Kafka这么快？"><a href="#DMA：为什么Kafka这么快？" class="headerlink" title="DMA：为什么Kafka这么快？"></a>DMA：为什么Kafka这么快？</h1><p>过去几年里，整个计算机产业界，都在尝试不停地提升 I&#x2F;O 设备的速度。把 HDD 硬盘换成 SSD 硬盘，我们仍然觉得不够快；用 PCI Express 接口的 SSD 硬盘替代 SATA 接口的 SSD 硬盘<span id="more"></span>，我们还是觉得不够快，所以，现在就有了傲腾（Optane）这样的技术。</p>
<p>但是，无论 I&#x2F;O 速度如何提升，比起 CPU，总还是太慢。SSD 硬盘的 IOPS 可以到 2 万、4 万，但是我们 CPU 的主频有 2GHz 以上，也就意味着每秒会有 20 亿次的操作。</p>
<p>如果我们对于 I&#x2F;O 的操作，都是由 CPU 发出对应的指令，然后等待 I&#x2F;O 设备完成操作之后返回，<em>那 CPU 有大量的时间其实都是在等待 I&#x2F;O 设备完成操作。</em></p>
<p>但是，这个 CPU 的等待，在很多时候，其实并没有太多的实际意义。我们对于 I&#x2F;O 设备的大量操作，其实都只是把内存里面的数据，传输到 I&#x2F;O 设备而已。在这种情况下，其实 CPU 只是在傻等而已。特别是当传输的数据量比较大的时候，比如进行大文件复制，如果所有数据都要经过 CPU，实在是有点儿太浪费时间了。</p>
<hr>
<p>因此，计算机工程师们，就发明了 DMA 技术，也就是<strong>直接内存访问（Direct Memory Access）</strong>技术，来减少 CPU 等待的时间。</p>
<h2 id="理解-DMA，一个协处理器"><a href="#理解-DMA，一个协处理器" class="headerlink" title="理解 DMA，一个协处理器"></a>理解 DMA，一个协处理器</h2><p>其实 DMA 技术很容易理解，本质上，DMA 技术就是我们在主板上放一块独立的芯片。在进行内存和 I&#x2F;O 设备的数据传输的时候，我们不在通过CPU 来控制数据传输，而直接通过<strong>DMA 控制器</strong>（DMA Controller，简称 DMAC）。这块芯片，可以认为其实就是一个<strong>协处理器（Co-Processor）</strong>。</p>
<p>DMAC 最有价值的地方体现在，当我们要传输的数据特别大、速度特别快，或者传输的数据特别小、速度特别慢的时候。</p>
<blockquote>
<p>比如说，我们用千兆网卡或者硬盘传输大量数据的时候，如果都用 CPU 来搬运的话，肯定忙不过来，所以可以选择 DMAC。而当数据传输很慢的时候，DMAC 可以等数据到齐了，再发送信号，给到 CPU 去处理，而不是让 CPU 在那里忙等待。</p>
</blockquote>
<p>DMAC 是一块“协处理器芯片”，这是为什么呢?</p>
<p>注意，这里面的“协”字。DMAC 是在“协助”CPU，完成对应的数据传输工作。在 DMAC 控制数据传输的过程中，我们还是需要 CPU 的。</p>
<p>除此之外，DMAC 其实也是一个特殊的 I&#x2F;O 设备，它和 CPU 以及其他 I&#x2F;O 设备一样，通过连接到总线来进行实际的数据传输。总线上的设备呢，其实有两种类型。一种我们称之为主设备（Master），另外一种，我们称之为从设备（Slave）。</p>
<p>想要主动发起数据传输，必须要是一个主设备才可以，CPU 就是主设备。而我们从设备（比如硬盘）只能接受数据传输。所以，如果通过 CPU 来传输数据，要么是 CPU 从 I&#x2F;O 设备读数据，要么是 CPU 向 I&#x2F;O 设备写数据。</p>
<p>这个时候你可能要问了，那我们的 I&#x2F;O 设备不能向主设备发起请求么？可以是可以，不过这个发送的不是数据内容，而是控制信号。I&#x2F;O 设备可以告诉 CPU，我这里有数据要传输给你，但是实际数据是 CPU 拉走的，而不是 I&#x2F;O 设备推给 CPU 的。</p>
<img src="/img/computer_img/DMA 控制.webp" alt="DMA 控制" style="zoom:50%;" />

<p>不过，<em>DMAC 就很有意思了，它既是一个主设备，又是一个从设备。对于 CPU 来说，它是一个从设备；对于硬盘这样的 IO 设备来说呢，它又变成了一个主设备</em>。那使用 DMAC 进行数据传输的过程究竟是什么样的呢？下面我们来具体看看。</p>
<ol>
<li>首先，CPU 还是作为一个主设备，向 DMAC 设备发起请求。这个请求，其实就是在 DMAC 里面修改配置寄存器。</li>
<li>.CPU 修改 DMAC 的配置的时候，会告诉 DMAC 这样几个信息：</li>
</ol>
<ul>
<li><p>首先是源地址的初始值以及传输时候的地址增减方式。</p>
<p>所谓源地址，就是数据要从哪里传输过来。如果我们要从内存里面写入数据到硬盘上，那么就是要读取的数据在内存里面的地址。如果是从硬盘读取数据到内存里，那就是硬盘的 I&#x2F;O 接口的地址。</p>
<p>我们讲过总线的时候说过，I&#x2F;O 的地址可以是一个内存地址，也可以是一个端口地址。而地址的增减方式就是说，数据是从大的地址向小的地址传输，还是从小的地址往大的地址传输。</p>
</li>
<li><p>其次是目标地址初始值和传输时候的地址增减方式。目标地址自然就是和源地址对应的设备，也就是我们数据传输的目的地。</p>
</li>
<li><p>第三个自然是要传输的数据长度，也就是我们一共要传输多少数据。</p>
</li>
</ul>
<ol start="3">
<li>设置完这些信息之后，DMAC 就会变成一个空闲的状态（Idle）。</li>
<li>如果我们要从硬盘上往内存里面加载数据，这个时候，硬盘就会向 DMAC 发起一个数据传输请求。这个请求并不是通过总线，而是通过一个额外的连线。</li>
<li>然后，我们的 DMAC 需要再通过一个额外的连线响应这个申请。</li>
<li>于是，DMAC 这个芯片，就向硬盘的接口发起要总线读的传输请求。数据就从硬盘里面，读到了 DMAC 的控制器里面。</li>
<li>然后，DMAC 再向我们的内存发起总线写的数据传输请求，把数据写入到内存里面。</li>
<li>DMAC 会反复进行上面第 6、7 步的操作，直到 DMAC 的寄存器里面设置的数据长度传输完成。</li>
<li>数据传输完成之后，DMAC 重新回到第 3 步的空闲状态。</li>
</ol>
<p>所以，整个数据传输的过程中，我们不是通过 CPU 来搬运数据，而是由 DMAC 这个芯片来搬运数据。但是 CPU 在这个过程中也是必不可少的。因为传输什么数据，从哪里传输到哪里，其实还是由 CPU 来设置的。这也是为什么，DMAC 被叫作“协处理器”。</p>
<img src="/img/computer_img/内置DMAC.webp" alt="内置DMAC" style="zoom:50%;" />

<center>现在的外设里面，很多都内置了 DMAC</center>

<p>最早，计算机里是没有 DMAC 的，所有数据都是由 CPU 来搬运的。随着人们对于数据传输的需求越来越多，先是出现了主板上独立的 DMAC 控制器。到了今天，各种 I&#x2F;O 设备越来越多，数据传输的需求越来越复杂，使用的场景各不相同。加之显示器、网卡、硬盘对于数据传输的需求都不一样，所以各个设备里面都有自己的 DMAC 芯片了。</p>
<hr>
<h2 id="为什么那么快？一起来看-Kafka-的实现原理"><a href="#为什么那么快？一起来看-Kafka-的实现原理" class="headerlink" title="为什么那么快？一起来看 Kafka 的实现原理"></a>为什么那么快？一起来看 Kafka 的实现原理</h2><p>了解了 DMAC 是怎么回事儿，那你可能要问了，这和我们实际进行程序开发有什么关系呢？有什么 API，我们直接调用一下，就能加速数据传输，减少 CPU 占用吗？</p>
<p>你还别说，过去几年的大数据浪潮里面，还真有一个开源项目很好地利用了 DMA 的数据传输方式，通过 DMA 的方式实现了非常大的性能提升。这个项目就是 <strong>Kafka</strong>。下面我们就一起来看看它究竟是怎么利用 DMA 的。</p>
<p>Kafka 是一个用来处理实时数据的管道，常常用作一个消息队列，或者用来采集和落地海量的日志。作为一个实时处理数据和日志管道，瓶颈自然在 I&#x2F;O 层面。</p>
<p>Kafka 里面会有两种常见的海量数据传输的情况。一种是从网络中接收上游的数据，然后需要落地到本地的磁盘上，确保数据不丢失。另一种情况呢，则是从本地磁盘上读取出来，通过网络发送出去。</p>
<p>我们来看一看后一种情况，从磁盘读数据发送到网络上去。如果我们自己写一个简单的程序，最直观的办法，自然是用一个文件读操作，从磁盘上把数据读到内存里面来，然后再用一个 Socket，把这些数据发送到网络上去。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">File.read(fileDesc, buf, len);</span><br><span class="line">Socket.send(socket, buf, len);</span><br></pre></td></tr></table></figure>

<p>在这个过程中，数据一共发生了四次传输的过程。其中两次是 DMA 的传输，另外两次，则是通过 CPU 控制的传输。下面我们来具体看看这个过程。</p>
<p>第一次传输，是从硬盘上，读到操作系统内核的缓冲区。这个传输过程是通过 DMA 搬运的。</p>
<p>第二次传输，需要从内核缓冲区里的数据，复制到我们应用分配的内存里面，这个传输是通过 CPU 搬运的。</p>
<p>第三次传输，要从我们应用的内存里面，再写到操作系统的 Socket 的缓冲区里面去。这个传输，还是由 CPU 搬运的。</p>
<p>最后一次传输，需要从Socket的缓冲区里面， 写到网卡的缓冲区里面去。这个传输又是通过 DMA 搬运的。</p>
<img src="/img/computer_img/四次搬运.webp" alt="四次搬运" style="zoom:50%;" />

<hr>
<p>这个时候，你可以回过头看看这个过程。我们只是要“搬运”一份数据，结果却整整搬运了四次。而且这里面，从内核的读缓冲区传输到应用的内存里，再从应用的内存里传输到 Socket 的缓冲区里，其实都是把同一份数据在内存里面搬运来搬运去，特别没有效率。</p>
<p>像 Kafka 这样的应用场景，其实大部分最终利用到的硬件资源，其实又都是在干这个搬运数据的事儿。所以，我们需要尽可能减少数据搬运的需求。</p>
<p>事实上，<strong>Kafka</strong> 做的事情就是，把这个数据搬运的次数<strong>，从上面的四次，变成了两次，并且只有 DMA 来进行数据搬运，而不需要 CPU。</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">@Override</span><br><span class="line">public long transferFrom(FileChannel fileChannel, long position, long count) throws IOException &#123;</span><br><span class="line">    return fileChannel.transferTo(position, count, socketChannel);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<center style="color:#C0C0C0;text-decoration:underline">如果你层层追踪 Kafka 的代码，你会发现，最终它调用了 Java NIO 库里的 transferTo 方法</center>

<p>Kafka 的代码调用了 Java NIO 库，具体是 FileChannel 里面的 transferTo 方法。<strong>我们的数据并没有读到中间的应用内存里面，而是直接通过 Channel，写入到对应的网络设备里。</strong>  对于 Socket 的操作，也不是写入到 Socket 的 Buffer 里面， 而是直接根据描述符号（Descriptor），于是，在这个过程之中，我们只进行了两次数据传输。</p>
<img src="/img/computer_img/Kafka两次传输.webp" alt="Kafka两次传输" style="zoom: 50%;" />

<p>第一次，是通过DMA，从硬盘直接读到操作系统内核的读缓冲区里面。 第二次，则是根据Socket的秒舒服信息，直接从读缓冲区里面，写入到网卡的缓冲区里面。</p>
<p>这样，我们同一份数据传输的次数从四次变成了两次，<strong>并且没有通过 CPU 来进行数据搬运，所有的数据都是通过 DMA 来进行传输的。</strong></p>
<p>在这个方法里面，我们没有在内存层面去“复制（Copy）”数据，所以这个方法，也被称之为 <em><strong>零拷贝（Zero-Copy）</strong></em>。</p>
<p>IBM Developer Works 里面有一篇文章，专门写过程序来测试过，在同样的硬件下，使用零拷贝能够带来的性能提升。我在这里放上这篇文章<a href="https://developer.ibm.com/articles/j-zerocopy/">链接</a>。在这篇文章最后，你可以看到，无论传输数据量的大小，传输同样的数据，使用了零拷贝能够缩短 65% 的时间，大幅度提升了机器传输数据的吞吐量。想要深入了解零拷贝，建议你可以仔细读一读这篇文章。</p>
<hr>
<h2 id="总结延伸"><a href="#总结延伸" class="headerlink" title="总结延伸"></a>总结延伸</h2><p>如果我们始终让 CPU 来进行各种数据传输工作，会特别浪费。一方面，我们的数据传输工作用不到多少 CPU 核心的“计算”功能。另一方面，CPU 的运转速度也比 I&#x2F;O 操作要快很多。所以，我们希望能够给 CPU“减负”。</p>
<p>于是，工程师们就在主板上放上了 DMAC 这样一个协处理器芯片。通过这个芯片，CPU 只需要告诉 DMAC，我们要传输什么数据，从哪里来，到哪里去，就可以放心离开了。后续的实际数据传输工作，都会由 DMAC 来完成。随着现代计算机各种外设硬件越来越多，光一个通用的 DMAC 芯片不够了，我们在各个外设上都加上了 DMAC 芯片，使得 CPU 很少再需要关心数据传输的工作了。</p>
<p>在我们实际的系统开发过程中，利用好 DMA 的数据传输机制，也可以大幅提升 I&#x2F;O 的吞吐率。最典型的例子就是 Kafka。</p>
<p>传统地从硬盘读取数据，然后再通过网卡向外发送，我们需要进行四次数据传输，<em>其中有两次是发生在内存里的缓冲区和对应的硬件设备之间，我们没法节省掉。但是还有两次，完全是通过 CPU 在内存里面进行数据复制。</em></p>
<p>在 Kafka 里，通过 Java 的 NIO 里面 FileChannel 的 transferTo 方法调用，我们可以不用把数据复制到我们应用程序的内存里面。通过 DMA 的方式，  我们可以把数据从内存缓冲区直接写到网卡的缓冲区里面。在使用了这样的零拷贝的方法之后呢，我们传输同样数据的时间，可以缩减为原来的 1&#x2F;3，相当于提升了 3 倍的吞吐率。</p>
<p>这也是为什么，Kafka 是目前实时数据传输管道的标准解决方案。</p>
<p>Kafka 的论文:<a href="http://notes.stephenholiday.com/Kafka.pdf">Kakfa:a Distrubted Messaging System for Log Processing</a></p>
]]></content>
      <categories>
        <category>计算机组成原理</category>
      </categories>
      <tags>
        <tag>存储器</tag>
        <tag>Kafka</tag>
      </tags>
  </entry>
  <entry>
    <title>数据完整性（上）</title>
    <url>/posts/9527a54e/</url>
    <content><![CDATA[<h1 id="数据完整性（上）：硬件坏了怎么办？"><a href="#数据完整性（上）：硬件坏了怎么办？" class="headerlink" title="数据完整性（上）：硬件坏了怎么办？"></a>数据完整性（上）：硬件坏了怎么办？</h1><h2 id="问题引人"><a href="#问题引人" class="headerlink" title="问题引人"></a>问题引人</h2><p>当时，我正在 MediaV 带领一个 20 多人的团队，负责公司的广告数据和机器学习算法。其中有一部分工作，就是用 Hadoop 集群处理所有的数据和报表业务。<span id="more"></span>当时我们的业务增长很快，所以会频繁地往 Hadoop 集群里面添置机器。2012 年的时候，国内的云计算平台还不太成熟，所以我们都是自己采购硬件，放在托管的数据中心里面。</p>
<p>那个时候，我们的 Hadoop 集群服务器，在从 100 台服务器往 1000 台服务器走。我们觉得，像 Dell 这样品牌厂商的服务器太贵了，而且能够提供的硬件配置和我们的期望也有差异。于是，运维的同学开始和 OEM 厂商合作，自己定制服务器，批量采购硬盘、内存。</p>
<p>那个时候，大家都听过 Google 早期发展时，为了降低成本买了很多二手的硬件来降低成本，通过分布式的方式来保障系统的可靠性的办法。虽然我们还没有抠门到去买二手硬件，不过当时，我们选择购买了普通的机械硬盘，而不是企业级的、用在数据中心的机械硬盘；采购了普通的内存条，而不是带 ECC 纠错的服务器内存条，想着能省一点儿是一点儿。</p>
<h2 id="单比特翻转：软件解决不了的硬件错误"><a href="#单比特翻转：软件解决不了的硬件错误" class="headerlink" title="单比特翻转：软件解决不了的硬件错误"></a>单比特翻转：软件解决不了的硬件错误</h2><p>忽然有一天，我们最大的、每小时执行一次的数据处理报表应用，完成时间变得比平时晚了不少。一开始，我们并没有太在意，毕竟当时数据量每天都在增长，慢一点就慢一点了。但是，接着糟糕的事情开始发生了。</p>
<p>一方面，我们发现，报表任务有时候在一个小时之内执行不完，接着，偶尔整个报表任务会执行失败。于是，我们不得不停下手头开发的工作，开始排查这个问题。</p>
<p>用过 Hadoop 的话，你可能知道，作为一个分布式的应用，考虑到硬件的故障，Hadoop 本身会在特定节点计算出错的情况下，重试整个计算过程。之前的报表跑得慢，就是因为有些节点的计算任务失败过，只是在重试之后又成功了。进一步分析，我们发现，程序的错误非常奇怪。有些数据计算的结果，比如“34+23”，结果应该是“57”，但是却变成了一个美元符号“$”。</p>
<p>前前后后折腾了一周，我们发现，从日志上看，大部分出错的任务都在几个固定的硬件节点上。</p>
<p>另一方面，我们发现，问题出现在我们新的一批自己定制的硬件上架之后。于是，和运维团队的同事沟通近期的硬件变更，并且翻阅大量 Hadoop 社区的邮件组列表之后，我们有了一个大胆的推测。</p>
<p>我们推测，这个错误，来自我们自己定制的硬件。定制的硬件没有使用 ECC 内存，在大量的数据中，内存中出现了<strong>单比特翻转（</strong>Single-Bit Flip）这个传说中的硬件错误。</p>
<p>那这个符号是怎么来的呢？是由于内存中的一个整数字符，遇到了一次单比特翻转转化而来的。 它的 ASCII 码二进制表示是 0010 0100，所以它完全可能来自 0011 0100 遇到一次在第 4 个比特的单比特翻转，也就是从整数“4”变过来的。但是我们也只能推测是这个错误，而不能确信是这个错误。因为单比特翻转是一个随机现象，我们没法稳定复现这个问题。</p>
<img src="/img/computer_img/单比特翻转.webp" alt="单比特翻转" style="zoom:50%;" />

<p><strong>ECC 内存</strong> 的全称是 Error-Correcting Code memory，中文名字叫作纠错内存。顾名思义，就是在内存里面出现错误的时候，能够自己纠正过来。</p>
<p>在和运维同学沟通之后，我们把所有自己定制的服务器的内存替换成了 ECC 内存，之后这个问题就消失了。这也使得我们基本确信，问题的来源就是因为没有使用 ECC 内存。</p>
<hr>
<h2 id="奇偶校验和校验位：捕捉错误的好办法"><a href="#奇偶校验和校验位：捕捉错误的好办法" class="headerlink" title="奇偶校验和校验位：捕捉错误的好办法"></a>奇偶校验和校验位：捕捉错误的好办法</h2><p>其实，内存里面的单比特翻转或者错误，并不是一个特别罕见的现象。无论是因为内存的制造质量造成的漏电，还是外部的射线，都有一定的概率，会造成单比特错误。而内存层面的数据出错，软件工程师并不知道，而且这个出错很有可能是随机的。遇上随机出现难以重现的错误，大家肯定受不了。我们必须要有一个办法，避免这个问题。</p>
<p>其实，在 ECC 内存发明之前，工程师们已经开始通过<strong>奇偶校验</strong>的方式，来发现这些错误。</p>
<p>奇偶校验的思路很简单。我们把内存里面的 N 位比特当成是一组。常见的，比如 8 位就是一个字节。然后，用额外的一位去记录，这 8 个比特里面有奇数个 1 还是偶数个 1。如果是奇数个 1，那额外的一位就记录为 1；如果是偶数个 1，那额外的一位就记录成 0。那额外的一位，我们就称之为<strong>校验码位</strong>。</p>
<img src="/img/computer_img/奇偶校验.webp" alt="奇偶校验" style="zoom:50%;" />

<p>如果在这个字节里面，我们不幸发生了单比特翻转，那么数据位计算得到的校验码，就和实际校验位里面的数据不一样。我们的内存就知道出错了。</p>
<p>除此之外，校验位有一个很大的优点，就是计算非常快，往往只需要遍历一遍需要校验的数据，通过一个 O(N) 的时间复杂度的算法，就能把校验结果计算出来。</p>
<p>校验码比方说，我们下载一些软件的时候，你会看到，除了下载的包文件，还会有对应的 MD5 这样的哈希值或者循环冗余编码（CRC）的校验文件。这样，当我们把对应的软件下载下来之后，我们可以计算一下对应软件的校验码，和官方提供的校验码去做个比对，看看是不是一样。思路，在很多地方都会用到。</p>
<p>如果不一样，你就不能轻易去安装这个软件了。因为有可能，这个软件包是坏的。但是，还有一种更危险的情况，就是你下载的这个软件包，可能是被人植入了后门的。安装上了之后，你的计算机的安全性就没有保障了。</p>
<p>不过，使用奇偶校验，还是有两个比较大的缺陷。</p>
<ul>
<li>一、就是奇偶校验只能解决遇到单个位的错误，或者说奇数个位的错误。如果出现 2 个位进行了翻转，那么这个字节的校验位计算结果其实没有变，我们的校验位自然也就不能发现这个错误。</li>
<li>二、它只能发现错误，但是不能纠正错误。所以，即使在内存里面发现数据错误了，我们也只能中止程序，而不能让程序继续正常地运行下去。如果这个只是我们的个人电脑，做一些无关紧要的应用，这倒是无所谓了。</li>
</ul>
<p>但是，你想一下，如果你在服务器上进行某个复杂的计算任务，这个计算已经跑了一周乃至一个月了，还有两三天就跑完了。这个时候，出现内存里面的错误，要再从头跑起，估计你内心是崩溃的。</p>
<p>所以，我们需要一个比简单的校验码更好的解决方案，一个能够发现更多位的错误，并且能够把这些错误纠正过来的解决方案，也就是工程师们发明的 ECC 内存所使用的解决方案。</p>
<p>我们不仅能捕捉到错误，还要能够纠正发生的错误。这个策略，我们通常叫作纠错码（Error Correcting Code）。它还有一个升级版本，叫作纠删码（Erasure Code），不仅能够纠正错误，还能够在错误不能纠正的时候，直接把数据删除。无论是我们的 ECC 内存，还是网络传输，乃至硬盘的 RAID，其实都利用了纠错码和纠删码的相关技术。</p>
<p>我们怎么通过算法，怎么配置硬件，使得我们不仅能够发现单个位的错误，而能发现更多位的错误。</p>
<hr>
<h2 id="总结延伸"><a href="#总结延伸" class="headerlink" title="总结延伸"></a>总结延伸</h2><p>我给你介绍了我自己亲身经历的一个硬件错误带来的 Bug。由于没有采用 ECC 内存，导致我们的数据处理中，出现了大量的单比特数据翻转的错误。这些硬件带来的错误，其实我们没有办法在软件层面解决。</p>
<p>如果对于硬件以及硬件本身的原理不够熟悉，恐怕这个问题的解决方案还是遥遥无期。如果你对计算机组成原理有所了解，并能够意识到，在硬件的存储层有着数据验证和纠错的需求，那你就能在有限的时间内定位到问题所在。</p>
<p>进一步地，简单介绍了奇偶校验，也就是如何通过冗余的一位数据，发现在硬件层面出现的位错误。但是，奇偶校验以及其他的校验码，只能发现错误，没有办法纠正错误。所以，下一讲，我们一起来看看，怎么利用纠错码这样的方式，来解决问题。</p>
]]></content>
      <categories>
        <category>计算机组成原理</category>
      </categories>
      <tags>
        <tag>存储器</tag>
        <tag>奇偶校验</tag>
        <tag>校验位</tag>
      </tags>
  </entry>
  <entry>
    <title>数据完整性（下）</title>
    <url>/posts/2d9bc22b/</url>
    <content><![CDATA[<h1 id="数据完整性（下）：如何还原犯罪现场？"><a href="#数据完整性（下）：如何还原犯罪现场？" class="headerlink" title="数据完整性（下）：如何还原犯罪现场？"></a>数据完整性（下）：如何还原犯罪现场？</h1><p>讲完校验码之后，你现在应该知道，无论是奇偶校验码，还是 CRC 这样的循环校验码，都只能告诉我们一个事情，就是你的数据出错了。所以，校验码也被称为检错码（Error Detecting Code）。<span id="more"></span></p>
<p>不管是校验码，还是检错码，在硬件出错的时候，只能告诉你“我错了”。但是，下一个问题，“错哪儿了”，它是回答不了的。这就导致，我们的处理方式只有一种，那就是当成“哪儿都错了”。如果是下载一个文件，发现校验码不匹配，我们只能重新去下载；如果是程序计算后放到内存里面的数据，我们只能再重新算一遍。</p>
<p>这样的效率实在是太低了，所以我们需要有一个办法，不仅告诉我们“我错了”，还能告诉我们“错哪儿了”。于是，计算机科学家们就发明了纠错码。纠错码需要更多的冗余信息，通过这些冗余信息，我们不仅可以知道哪里的数据错了，还能直接把数据给改对。    这个是不是听起来很神奇？接下来就让我们一起来看一看。</p>
<p>这样的效率实在是太低了，所以我们需要有一个办法，不仅告诉我们“我错了”，还能告诉我们“错哪儿了”。于是，计算机科学家们就发明了纠错码。纠错码需要更多的冗余信息，通过这些冗余信息，我们不仅可以知道哪里的数据错了，还能直接把数据给改对。这个是不是听起来很神奇？接下来就让我们一起来看一看。</p>
<hr>
<h2 id="海明码：我们需要多少信息冗余？"><a href="#海明码：我们需要多少信息冗余？" class="headerlink" title="海明码：我们需要多少信息冗余？"></a>海明码：我们需要多少信息冗余？</h2><p>最知名的纠错码就是海明码。海明码（Hamming Code）是以他的发明人 Richard Hamming（理查德·海明）的名字命名的。这个编码方式早在上世纪四十年代就被发明出来了。而直到今天，我们上一讲所说到的 ECC 内存，也还在使用海明码来纠错。</p>
<p>最基础的海明码叫 <strong>7-4 海明码</strong> 。这里的<em>“7”指的是实际有效的数据</em>，一共是 7 位（Bit）。而这里的<em>“4”，指的是我们额外存储了 4 位数据，用来纠错。</em></p>
<p>首先，你要明白一点，纠错码的纠错能力是有限的。不是说不管错了多少位，我们都能给纠正过来。不然我们就不需要那 7 个数据位，只需要那 4 个校验位就好了，这意味着我们可以不用数据位就能传输信息了。这就不科学了。事实上，在 7-4 海明码里面， 我们只能纠正某 1 位的错误。这是怎么做到的呢？我们一起来看看。</p>
<p>4 位的校验码，一共可以表示 2^4 &#x3D; 16 个不同的数。根据数据位计算出来的校验值，一定是确定的。所以，如果数据位出错了，计算出来的校验码，一定和确定的那个校验码不同。那可能的值，就是在 2^4 - 1 &#x3D; 15 那剩下的 15 个可能的校验值当中。</p>
<p>15 个可能的校验值，其实可以对应 15 个可能出错的位。这个时候你可能就会问了，既然我们的数据位只有 7 位，那为什么我们要用 4 位的校验码呢？用 3 位不就够了吗？2^3 - 1 &#x3D; 7，正好能够对上 7 个不同的数据位啊！</p>
<p>你别忘了，单比特翻转的错误，不仅可能出现在数据位，也有可能出现在校验位。校验位本身也是可能出错的。所以，7 位数据位和 3 位校验位，如果只有单比特出错，可能出错的位数就是 10 位，2^3 - 1 &#x3D; 7 种情况是不能帮我们找到具体是哪一位出错的。</p>
<p>事实上，如果我们的数据位有 K 位，校验位有 N 位。那么我们需要满足下面这个不等式，才能确保我们能够对单比特翻转的数据纠错。这个不等式就是：</p>
<p>​                                            <strong>K + N + 1 &lt;&#x3D; 2^N</strong></p>
<p>在有 7 位数据位，也就是 K&#x3D;7 的情况下，N 的最小值就是 4。4 位校验位，其实最多可以支持到 11 位数据位。我在下面列了一个简单的数据位数和校验位数的对照表，你可以自己算一算，理解一下上面的公式。</p>
<img src="/img/computer_img/单比特校验.webp" alt="单比特校验" style="zoom:50%;" />



<hr>
<h2 id="海明码的纠错原理"><a href="#海明码的纠错原理" class="headerlink" title="海明码的纠错原理"></a>海明码的纠错原理</h2><p>在数据位数确定的情况下，怎么计算需要的校验位。那接下来，我们就一起看看海明码的编码方式是怎么样的。</p>
<p>为了算起来简单一点，我们少用一些位数，来算一个 4-3 海明码（也就是 4 位数据位，3 位校验位）。我们把 4 位数据位，分别记作 d1、d2、d3、d4。这里的 d，取的是数据位 data bits 的首字母。我们把 3 位校验位，分别记作 p1、p2、p3。这里的 p，取的是校验位 parity bits 的首字母。</p>
<p>从 4 位的数据位里面，我们拿走 1 位，然后计算出一个对应的校验位。这个校验位的计算用之前讲过的奇偶校验就可以了。比如，我们用 d1、d2、d4 来计算出一个校验位 p1；用 d1、d3、d4 计算出一个校验位 p2；用 d2、d3、d4 计算出一个校验位 p3。就像下面这个对应的表格一样：</p>
<img src="/img/computer_img/纠错校验码.webp" alt="纠错校验码" style="zoom:50%;" />

<p>这个时候，你去想一想，如果 d1 这一位的数据出错了，会发生什么情况？我们会发现，p1 和 p2 和校验的计算结果不一样。d2 出错了，是因为 p1 和 p3 的校验的计算结果不一样；d3 出错了，则是因为 p2 和 p3；如果 d4 出错了，则是 p1、p2、p3 都不一样。你会发现，<strong>当数据码出错的时候，至少会有 2 位校验码的计算是不一致的。</strong></p>
<p>所以校验码不一致，一共有 2^3-1&#x3D;7 种情况，正好对应了 7 个不同的位数的错误。我把这个对应表格也放在下面了，你可以理解一下。</p>
<img src="/img/computer_img/4-3海明码校验结果.webp" alt="4-3海明码校验结果" style="zoom:50%;" />

<p>可以看到，海明码这样的纠错过程，有点儿像电影里面看到的推理探案的过程。通过出错现场的额外信息，一步一步条分缕析地找出，到底是哪一位的数据出错，还原出错时候的“犯罪现场”。</p>
<p>看到这里，相信你一方面会觉得海明码特别神奇，但是同时也会冒出一个新的疑问，我们怎么才能用一套程序或者规则来生成海明码呢？其实这个步骤并不复杂，接下来我们就一起来看一下。</p>
<p>首先，我们先确定编码后，要传输的数据是多少位。比如说，我们这里的 7-4 海明码，就是一共 11 位。</p>
<p>然后，我们给这 11 位数据从左到右进行编号，并且也把它们的二进制表示写出来。</p>
<p>接着，我们先把这 11 个数据中的二进制的整数次幂找出来。在这个 7-4 海明码里面，就是 1、2、4、8。这些数，就是我们的校验码位，我们把他们记录做 p1～p4。如果从二进制的角度看，它们是这 11 个数当中，唯四的，在 4 个比特里面只有一个比特是 1 的数值。</p>
<p>那么剩下的 7 个数，就是我们 d1-d7 的数据码位了。</p>
<p>然后，对于我们的校验码位，我们还是用奇偶校验码。但是每一个校验码位，不是用所有的 7 位数据来计算校验码。而是 p1 用 3、5、7、9、11 来计算。也就是，在二进制表示下，从右往左数的第一位比特是 1 的情况下，用 p1 作为校验码。</p>
<p>剩下的 p2，我们用 3、6、10、11 来计算校验码，也就是在二进制表示下，从右往左数的第二位比特是 1 的情况下，用 p2。那么，p3 自然是从右往左数，第三位比特是 1 的情况下的数字校验码。而 p4 则是第四位比特是 1 的情况下的校验码。</p>
<img src="/img/computer_img/校验码位.webp" alt="校验码位" style="zoom:50%;" />

<p>这个时候，你会发现，任何一个数据码出错了，就至少会有对应的两个或者三个校验码对不上，这样我们就能反过来找到是哪一个数据码出错了。如果校验码出错了，那么只有校验码这一位对不上，我们就知道是这个校验码出错了。</p>
<p>上面这个方法，我们可以用一段确定的程序表示出来，意味着无论是几位的海明码，我们都不再需要人工去精巧地设计编码方案了。</p>
<hr>
<h2 id="海明距离：形象理解海明码的作用"><a href="#海明距离：形象理解海明码的作用" class="headerlink" title="海明距离：形象理解海明码的作用"></a>海明距离：形象理解海明码的作用</h2><p>其实，我们还可以换一个角度来理解海明码的作用。对于两个二进制表示的数据，他们之间有差异的位数，我们称之为海明距离。比如 1001 和 0001 的海明距离是 1，因为他们只有最左侧的第一位是不同的。而 1001 和 0000 的海明距离是 2，因为他们最左侧和最右侧有两位是不同的。</p>
<img src="/img/computer_img/海明距离.webp" alt="海明距离" style="zoom:50%;" />

<p>于是，你很容易可以想到，所谓的进行一位纠错，也就是所有和我们要传输的数据的海明距离为 1 的数，都能被纠正回来。</p>
<p>而任何两个实际我们想要传输的数据，海明距离都至少要是 3。你可能会问了，为什么不能是 2 呢？因为如果是 2 的话，那么就会有一个出错的数，到两个正确的数据的海明距离都是 1。当我们看到这个出错的数的时候，我们就不知道究竟应该纠正到那一个数了。</p>
<p>在引入了海明距离之后，我们就可以更形象地理解纠错码了。在没有纠错功能的情况下，我们看到的数据就好像是空间里面的一个一个点。这个时候，我们可以让数据之间的距离很紧凑，但是如果这些点的坐标稍稍有错，我们就可能搞错是哪一个点。</p>
<p>在有了 1 位纠错功能之后，就好像我们把一个点变成了以这个点为中心，半径为 1 的球。只要坐标在这个球的范围之内，我们都知道实际要的数据就是球心的坐标。而各个数据球不能距离太近，不同的数据球之间要有 3 个单位的距离。</p>
<img src="/img/computer_img/纠错形象.webp" alt="纠错形象" style="zoom:50%;" />



<h2 id="总结延伸"><a href="#总结延伸" class="headerlink" title="总结延伸"></a>总结延伸</h2><p>好了，纠错码的内容到这里就讲完了。你可不要小看这个看起来简单的海明码。虽然它在上世纪 40 年代早早地就诞生了，不过直到今天的 ECC 内存里面，我们还在使用这个技术方案。而海明也因为海明码获得了图灵奖。</p>
<p>通过在数据中添加多个冗余的校验码位，海明码不仅能够检测到数据中的错误，还能够在只有单个位的数据出错的时候，把错误的一位纠正过来。</p>
<hr>
<p>在理解和计算海明码的过程中，有一个很重要的点，就是不仅原来的数据位可能出错。我们新添加的校验位，一样可能会出现单比特翻转的错误。这也是为什么，7 位数据位用 3 位校验码位是不够的，而需要 4 位校验码位。</p>
<p>实际的海明码编码的过程也并不复杂，我们通过用不同过的校验位，去匹配多个不同的数据组，确保任何一个数据位出错，都会产生一个多个校验码位出错的唯一组合。这样，在出错的时候，我们就可以反过来找到出错的数据位，并纠正过来。当只有一个校验码位出错的时候，我们就知道实际出错的是校验码位了。</p>
]]></content>
      <categories>
        <category>计算机组成原理</category>
      </categories>
      <tags>
        <tag>机器码</tag>
        <tag>海明码</tag>
      </tags>
  </entry>
  <entry>
    <title>分布式计算</title>
    <url>/posts/dd37509c/</url>
    <content><![CDATA[<h1 id="分布式计算：如果所有人的大脑都联网会怎样？"><a href="#分布式计算：如果所有人的大脑都联网会怎样？" class="headerlink" title="分布式计算：如果所有人的大脑都联网会怎样？"></a>分布式计算：如果所有人的大脑都联网会怎样？</h1><p>实际上，一台计算机在数据中心里是不够的。因为如果只有一台计算机，我们会遇到三个核心问题。第一个核心问题，叫作<strong>垂直扩展和水平扩展的选择问题</strong>，第二问题叫<span id="more"></span>作如何<strong>保持高可用性</strong>（High Availability），第三个问题叫作<strong>一致性问题</strong>（Consistency）。</p>
<p>围绕这三个问题，其实就是我们今天要讲的主题，分布式计算。当然，短短的一讲肯定讲不完这么大一个主题。这一讲的目标，是让你能理解水平扩展、高可用性这两个核心问题。对于分布式系统带来的一致性问题，我们会留在我们的实战篇里面，再用案例来为大家分析。</p>
<h2 id="从硬件升级到水平扩展"><a href="#从硬件升级到水平扩展" class="headerlink" title="从硬件升级到水平扩展"></a>从硬件升级到水平扩展</h2><p>从技术开发的角度来讲，想要在 2019 年创业真的很幸福。只要在 AWS 或者阿里云这样的云服务上注册一个账号，一个月花上一两百块钱，你就可以有一台在数据中心里面的服务器了。而且这台服务器，可以直接提供给世界各国人民访问。如果你想要做海外市场，你可以把这个服务器放在美国、欧洲、东南亚，任何一个你想要去的市场的数据中心里，然后把自己的网站部署在这台服务器里面就可以了。<br><img src="/img/computer_img/云服务购买.webp" alt="云服务购买" style="zoom:50%;" /><br>当然，这台服务器就是我们在第 34 讲里说的虚拟机。不过因为只是个业余时间的小项目，一开始这台服务器的配置也不会太高。我以我现在公司所用的 Google Cloud 为例。最低的配置差不多是 1 个 CPU 核心、3.75G 内存以及一块 10G 的 SSD 系统盘。这样一台服务器每个月的价格差不多是 28 美元。</p>
<p>幸运的是，你的网站很受大家欢迎，访问量也上来了。这个时候，这台单核心的服务器的性能有点不够用了。这个时候，你需要升级你的服务器。于是，你就会面临两个选择。</p>
<ul>
<li>第一个选择是升级现在这台服务器的硬件，变成 2 个 CPU 核心、7.5G 内存。这样的选择我们称之为<strong>垂直扩展</strong>（Scale Up）。</li>
<li>第二个选择则是我们再租用一台和之前一样的服务器。于是，我们有了 2 台 1 个 CPU 核心、3.75G 内存的服务器。这样的选择我们称之为<strong>水平扩展</strong>（Scale Out）。</li>
</ul>
<p>在这个阶段，这两个选择，从成本上看起来没有什么差异。2 核心、7.5G 内存的服务器，成本是 56.61 美元，而 2 台 1 核心、3.75G 内存的服务器价格，成本是 57 美元，这之间的价格差异不到 1%。</p>
<p>不过，垂直扩展和水平扩展看似是两个不同的选择，但是随着流量不断增长。到最后，只会变成一个选择。那就是既会垂直扩展，又会水平扩展，并且最终依靠水平扩展，来支撑 Google、Facebook、阿里、腾讯这样体量的互联网服务。</p>
<p>垂直扩展背后的逻辑和优势都很简单。一般来说，垂直扩展通常不需要我们去改造程序，也就是说，我们没有研发成本。那为什么我们最终还是要用水平扩展呢？你可以先自己想一想。</p>
<p>原因其实很简单，因为我们没有办法不停地去做垂直扩展。我们在 Google Cloud 上现在能够买到的性能最好的服务器，是 96 个 CPU 核心、1.4TB 的内存。如果我们的访问量逐渐增大，一台 96 核心的服务器也支撑不了了，那么我们就没有办法再去做垂直扩展了。这个时候，我们就不得不采用水平扩展的方案了。</p>
<p>96 个 CPU 核心看起来是个很强大的服务器，但是你算一算就知道，其实它的计算资源并没有多大。你现在多半在用一台 4 核心，或者至少也是 2 核心的 CPU。96 个 CPU 也就是 30～50 台日常使用的开发机的计算性能。而我们今天在互联网上遇到的问题，是每天数亿的访问量，靠 30～50 台个人电脑的计算能力想要支撑这样的计算需求，可谓是天方夜谭了。</p>
<p>然而，一旦开始采用水平扩展，我们就会面临在软件层面改造的问题，也就是需要我们进行分布式计算，需要引入<strong>负载均衡</strong>（Load Balancer）这样的组件，来进行流量分配。我们需要拆分应用服务器和数据库服务器，来进行垂直功能的切分。在不同的应用之间通过消息队列，来进行异步任务的执行。</p>
<img src="/img/computer_img/分布式负载均衡.webp" alt="分布式负载均衡" style="zoom:50%;" />

<p>所有这些软件层面的改造，其实都是在做分布式计算的一个核心工作，<em>就是通过消息传递（Message Passing）而不是共享内存（Shared Memory）的方式，让多台不同的计算机协作起来共同完成任务。</em></p>
<p>而因为我们最终必然要进行水平扩展，我们需要在系统设计的早期就基于消息传递而非共享内存来设计系统。即使这些消息只是在同一台服务器上进行传递。</p>
<p>事实上，有不少增长迅猛的公司，早期没有准备好通过水平扩展来支撑访问量的情况，而一味通过提升硬件配置 Scale Up，来支撑更大的访问量，最终影响了公司的存亡。最典型的例子，就是败在 Facebook 手下的<a href="https://en.wikipedia.org/wiki/Myspace">MySpace</a>。</p>
<hr>
<h2 id="理解高可用性和单点故障"><a href="#理解高可用性和单点故障" class="headerlink" title="理解高可用性和单点故障"></a>理解高可用性和单点故障</h2><p>尽管在 1 个 CPU 核心的服务器支撑不了我们的访问量的时候，选择垂直扩展是一个最简单的办法。不过如果是我的话，第一次扩展我会选择水平扩展。</p>
<p>选择水平扩展的一个很好的理由，自然是可以“强迫”从开发的角度，尽早地让系统能够支持水平扩展，避免在真的流量快速增长的时候，垂直扩展的解决方案跟不上趟。不过，其实还有一个更重要的理由，那就是系统的可用性问题。</p>
<p>上面的 1 核变 2 核的垂直扩展的方式，扩展完之后，我们还是只有 1 台服务器。如果这台服务器出现了一点硬件故障，比如，CPU 坏了，那我们的整个系统就坏了，就不可用了。</p>
<p>如果采用了水平扩展，即便有一台服务器的 CPU 坏了，我们还有另外一台服务器仍然能够提供服务。<strong>负载均衡</strong>能够通过健康检测（Health Check）发现坏掉的服务器没有响应了，就可以自动把所有的流量切换到第 2 台服务器上，这个操作就叫作<em><strong>故障转移（</strong></em>Failover），我们的系统仍然是<strong>可用</strong>的。</p>
<p>系统的<strong>可用性</strong>（Avaiability）指的就是，我们的系统可以正常服务的时间占比。无论是因为软硬件故障，还是需要对系统进行停机升级，都会让我们损失系统的可用性。可用性通常是用一个百分比的数字来表示，比如 99.99%。我们说，系统每个月的可用性要保障在 99.99%，也就是意味着一个月里，你的服务宕机的时间不能超过 4.32 分钟。</p>
<p>有些系统可用性的损失，是在我们计划内的。比如上面说的停机升级，这个就是所谓的计划内停机时间（Scheduled Downtime）。有些系统可用性的损失，是在我们计划外的，比如一台服务器的硬盘忽然坏了，这个就是所谓的计划外停机时间（Unscheduled Downtime）。</p>
<p>我们的系统是一定不可能做到 100% 可用的，特别是计划外的停机时间。从简单的硬件损坏，到机房停电、光缆被挖断，乃至于各种自然灾害，比如地震、洪水、海啸，都有可能使得我们的系统不可用。作为一个工程师和架构师，我们要做的就是尽可能低成本地提高系统的可用性。</p>
<hr>
<p>现在的服务器的可用性都已经很不错了，通常都能保障 99.99% 的可用性了。如果我们有一个小小的三台服务器组成的小系统，一台部署了 Nginx 来作为负载均衡和反向代理，一台跑了 PHP-FPM 作为 Web 应用服务器，一台用来作为 MySQL 数据库服务器。每台服务器的可用性都是 99.99%。那么我们整个系统的可用性是多少呢？你可以先想一想。</p>
<p>答案是 99.99% × 99.99% × 99.99% &#x3D; 99.97%。在这个系统当中，这个数字看起来似乎没有那么大区别。<strong>不过反过来看</strong>，我们是从损失了 0.01% 的可用性，变成了损失 0.03% 的可用性，不可用的时间变成了原来的 3 倍。</p>
<p>如果我们有 1000 台服务器，那么整个的可用性，就会变成 99.99% ^ 1000 &#x3D; 90.5%。也就是说，我们的服务一年里有超过一个月是不可用的。这可怎么办呀？<br><img src="/img/computer_img/服务器可用性.webp" alt="服务器可用性" style="zoom:67%;" /></p>
<p>先来分析一下原因。之所以会出现这个问题，是因为在这个场景下，任何一台服务器出错了，整个系统就没法用了。这个问题就叫作<strong>单点故障问题</strong>（Single Point of Failure，SPOF）。我们这里的这个假设特别糟糕。我们假设这 1000 台服务器，每一个都存在单点故障问题。所以，我们的服务也就特别脆弱，随便哪台出现点风吹草动，整个服务就挂了。</p>
<p>要解决单点故障问题，第一点就是要移除单点。其实移除单点最典型的场景，在我们水平扩展应用服务器的时候就已经看到了，那就是让两台服务器提供相同的功能，然后通过负载均衡把流量分发到两台不同的服务器去。即使一台服务器挂了，还有一台服务器可以正常提供服务。</p>
<p>不过光用两台服务器是不够的，单点故障其实在数据中心里面无处不在。我们现在用的是云上的两台虚拟机。如果这两台虚拟机是托管在同一台物理机上的，那这台物理机本身又成为了一个单点。那我们就需要把这两台虚拟机分到两台不同的物理机上。</p>
<p>不过这个还是不够。如果这两台物理机在同一个机架（Rack）上，那机架上的<strong>交换机</strong>（Switch）就成了一个单点。即使放到不同的机架上，还是有可能出现整个数据中心遭遇意外故障的情况。</p>
<img src="/img/computer_img/单点故障.webp" alt="单点故障" style="zoom:50%;" />

<p>去年我自己就遇到过，部署在 Azure 上的服务所在的数据中心，因为散热问题触发了整个数据中心所有服务器被关闭的问题。面对这种情况，我们就需要设计进行<strong>异地多活</strong>的系统设计和部署。所以，在现代的云服务，你在买服务器的时候可以选择服务器的 area（地区）和 zone（区域），而要不要把服务器放在不同的地区或者区域里，也是避免单点故障的一个重要因素。</p>
<hr>
<p>只是能够去除单点，其实我们的可用性问题还没有解决。比如，上面我们用负载均衡把流量均匀地分发到 2 台服务器上，当一台应用服务器挂掉的时候，我们的确还有一台服务器在提供服务。但是负载均衡会把一半的流量发到已经挂掉的服务器上，所以这个时候只能算作一半可用。</p>
<p>想要让整个服务完全可用，我们就需要有一套故障转移（Failover）机制。想要进行故障转移，就首先要能发现故障。</p>
<p>以我们这里的 PHP-FPM 的 Web 应用为例，负载均衡通常会定时去请求一个 Web 应用提供的健康检测（Health Check）的地址。这个时间间隔可能是 5 秒钟，如果连续 2～3 次发现健康检测失败，负载均衡就会自动将这台服务器的流量切换到其他服务器上。于是，我们就自动地产生了一次故障转移。    <strong>故障转移的自动化在大型系统里是很重要的</strong>，  因为服务器越多，出现故障基本就是个必然发生的事情。而自动化的故障转移既能够减少运维的人手需求，也能够缩短从故障发现到问题解决的时间周期，提高可用性。</p>
<img src="/img/computer_img/故障检测.webp" alt="故障检测" style="zoom:50%;" />

<center style="color:#C0C0C0;text-decoration:underline"> 我们在 Web 应用上设置了一个 Heartbeat 接口，每 20 秒检查一次，出现问题的时候可以进行故障转移切换</center>

<p>那么，让我们算一算，通过水平扩展相同功能的服务器来去掉单点故障，并且通过健康检查机制来触发自动的故障转移，这样的可用性会变成多少呢？你可以拿出纸和笔来试一下。</p>
<p>不知道你想明白应该怎么算了没有，在这种情况下，我们其实只要有任何一台服务器能够正常运转，就能正常提供服务。那么，我们的可用性就是：</p>
<p>​                100% - (100% - 99.99%) × (100% - 99.99%) &#x3D; 99.999999%</p>
<p>可以看出，不能提供服务的时间就减少到了原来的万分之一。</p>
<p>当然，在实际情况中，可用性没法做到那么理想的地步。光从硬件的角度，从服务器到交换机，从网线连接到机房电力，从机房的整体散热到外部的光纤线路等等，可能出现问题的地方太多了。这也是为什么，我们需要从整个系统层面，去设计<em><strong>系统的高可用性</strong></em>。</p>
<hr>
<h2 id="总结延伸"><a href="#总结延伸" class="headerlink" title="总结延伸"></a>总结延伸</h2><p>讲到这里，相信你已经很清楚，为什么我们需要水平扩展了。对于怎么去设计整个硬件的部署，来保障高可用性，你应该也有了一个清晰的认识。这两点也是分布式计算在实践中非常重要的应用场景。</p>
<p>光有这两点还是不够的。一旦系统里面有了很多台服务器。特别是，为了保障可用性，对于同样功能的、有状态的数据库进行了水平的扩展，我们就会面临一个新的挑战，那就是分区一致性问题。不过，这个问题更多的是一个软件设计问题，我把它留在后面的实战篇再进行讲解。这里讲了通过升级硬件规格来提升服务能力的垂直扩展。除此之外，也可以通过增加服务器数量来提升服务能力。不过归根到底，我们一定要走上水平扩展的路径。</p>
<p>一方面是因为垂直扩展不可持续；另一方面，则是只有水平扩展才能保障高可用性。</p>
<p>而通过水平扩展保障高可用性，则需要我们做三件事情。</p>
<ul>
<li>第一个是理解可用性是怎么计算的。服务器硬件的损坏只是可能导致可用性损失的因素之一，机房内的电力、散热、交换机、网络线路，都有可能导致可用性损失。而外部的光缆、自然灾害，也都有可能造成我们整个系统的不可用。</li>
<li>第二个，所以在分析设计系统的时候，我们需要尽可能地排除单点故障。</li>
<li>第三个，对于硬件的故障，我们还要有自动化的故障转移策略。</li>
</ul>
]]></content>
      <categories>
        <category>计算机组成原理</category>
      </categories>
      <tags>
        <tag>分布式</tag>
        <tag>负载均衡</tag>
      </tags>
  </entry>
  <entry>
    <title>局部性原理</title>
    <url>/posts/90cdc880/</url>
    <content><![CDATA[<h1 id="局部性原理：数据库性能跟不上，加个缓存就好了？"><a href="#局部性原理：数据库性能跟不上，加个缓存就好了？" class="headerlink" title="局部性原理：数据库性能跟不上，加个缓存就好了？"></a>局部性原理：数据库性能跟不上，加个缓存就好了？</h1><p>平时进行服务端软件开发的时候，我们通常会把数据存储在数据库里。而服务端系统遇到的第一个性能瓶颈<span id="more"></span>，往往就发生在访问数据库的时候。 这个时候，大部分工程师和架构师会拿出一种叫作“缓存”的武器，通过使用 Redis 或者 Memcache 这样的开源软件，在数据库前面提供一层缓存的数据，来缓解数据库面临的压力，提升服务端的程序性能。</p>
<img src="/img/computer_img/加一个缓存.webp" alt="加一个缓存" style="zoom:50%;" />

<center>在数据库前添加数据缓存是常见的性能优化方式</center>

<p>那么，不知道你有没有想过，这种添加缓存的策略一定是有效的吗？或者说，这种策略在什么情况下是有效的呢？如果从理论角度去分析，添加缓存一定是我们的最佳策略么？进一步地，如果我们对于访问性能的要求非常高，希望数据在 1 毫秒，乃至 100 微秒内完成处理，我们还能用这个添加缓存的策略么？</p>
<h2 id="理解局部性原理"><a href="#理解局部性原理" class="headerlink" title="理解局部性原理"></a>理解局部性原理</h2><p>可以看到，不同的存储器设备之间，访问速度、价格和容量都有几十乃至上千倍的差异。</p>
<img src="/img/computer_img/计算机组成/存储价目表.webp" alt="存储价目表" style="zoom:50%;" />

<p>以上一讲的 Intel 8265U 的 CPU 为例，它的 L1 Cache 只有 256K，L2 Cache 有个 1MB，L3 Cache 有 12MB。一共 13MB 的存储空间，如果按照 7 美元 &#x2F;1MB 的价格计算，就要 91 美元。</p>
<p>以上一讲的 Intel 8265U 的 CPU 为例，它的 L1 Cache 只有 256K，L2 Cache 有个 1MB，L3 Cache 有 12MB。一共 13MB 的存储空间，如果按照 7 美元 &#x2F;1MB 的价格计算，就要 91 美元。</p>
<p>我们的内存有 8GB，容量是 CPU Cache 的 600 多倍，按照表上的价格差不多就是 120 美元。如果按照今天京东上的价格，恐怕不到 40 美元。128G 的 SSD 和 1T 的 HDD，现在的价格加起来也不会超过 100 美元。虽然容量是内存的 16 倍乃至 128 倍，但是它们的访问速度却不到内存的 1&#x2F;1000。</p>
<p>性能和价格的巨大差异，给我们工程师带来了一个挑战：<strong>我们能不能既享受 CPU Cache 的速度，又享受内存、硬盘巨大的容量和低廉的价格呢？</strong></p>
<p>想要同时享受到这三点，前辈们已经探索出了答案，那就是，存储器中数据的<strong>局部性原理（Principle of Locality）</strong>。我们可以利用这个局部性原理，来制定管理和访问数据的策略。这个局部性原理包括  <strong>时间局部性（temporal locality）</strong>和 <strong>空间局部性（spatial locality）</strong>这两种策略。</p>
<ul>
<li>时间局部性。这个策略是说，如果一个数据被访问了，那么它在短时间内还会被再次访问。用一个简单的例子给你解释下，你一下就能明白了。</li>
</ul>
<p>比如说，《哈利波特与魔法石》这本小说，我今天读了一会儿，没读完，明天还会继续读。同理，在一个电子商务型系统中，如果一个用户打开了 App，看到了首屏。我们推断他应该很快还会再次访问网站的其他内容或者页面，我们就将这个用户的个人信息，从存储在硬盘的数据库读取到内存的缓存中来。这利用的就是时间局部性。</p>
<hr>
<ul>
<li>空间局部性。这个策略是说，如果一个数据被访问了，那么和它相邻的数据也很快会被访问。</li>
</ul>
<p>我们还拿刚才读《哈利波特与魔法石》的例子来说。我读完了这本书之后，感觉这书不错，所以就会借阅整套“哈利波特”。这就好比我们的程序，在访问了数组的首项之后，多半会循环访问它的下一项。因为，在存储数据的时候，数组内的多项数据会存储在相邻的位置。这就好比图书馆会把“哈利波特”系列放在一个书架上，摆放在一起，加载的时候，也会一并加载。我们去图书馆借书，往往会一次性把 7 本都借回来。</p>
<hr>
<blockquote>
<p>有了这个局部性原理，我们不用再把所有数据都放在内存里，也不用都放在 HDD 硬盘上，<strong>而是把访问次数多的数据，放在贵但是快一点的存储器里，把访问次数少的数据，放在慢但是大一点的存储器里。</strong> 这样组合使用内存、SSD 硬盘以及 HDD 硬盘，使得我们可以用最低的成本提供实际所需要的数据存储、管理和访问的需求。</p>
</blockquote>
<h2 id="如何花最少的钱，装下亚马逊的所有商品？"><a href="#如何花最少的钱，装下亚马逊的所有商品？" class="headerlink" title="如何花最少的钱，装下亚马逊的所有商品？"></a>如何花最少的钱，装下亚马逊的所有商品？</h2><hr>
<p>了解了局部性原理，下面我用一些真实世界中的数据举个例子，带你做个小小的思维体操，来看一看通过局部性原理，利用不同层次存储器的组合，究竟会有什么样的好处。</p>
<p>我们现在要提供一个亚马逊这样的电商网站。我们假设里面有 6 亿件商品，如果每件商品需要 4MB 的存储空间（考虑到商品图片的话，4MB 已经是一个相对较小的估计了），那么一共需要 2400TB（  &#x3D;  6 亿  × 4MB）的数据存储。</p>
<p>如果我们把数据都放在内存里面，那就需要 3600 万美元（  &#x3D;  2400TB&#x2F;1MB  × 0.015 美元  &#x3D;  3600 万美元）。但是，这 6 亿件商品中，不是每一件商品都会被经常访问。比如说，有 Kindle 电子书这样的热销商品，也一定有基本无人问津的商品，比如偏门的缅甸语词典。</p>
<p>如果我们只在内存里放前 1% 的热门商品，也就是 600 万件热门商品，而把剩下的商品，放在机械式的 HDD 硬盘上，那么，我们需要的存储成本就下降到 45.6 万美元（ &#x3D; 3600 万美元 × 1% + 2400TB &#x2F; 1MB × 0.00004 美元），是原来成本的 1.3% 左右。</p>
<p>这里我们用的就是时间局部性。<em>我们把有用户访问过的数据，加载到内存中，一旦内存里面放不下了，我们就把最长时间没有在内存中被访问过的数据，从内存中移走，</em>这个其实就是我们常用的 <strong>LRU（Least Recently Used）缓存算法</strong>。</p>
<hr>
<p>热门商品被访问得多，就会始终被保留在内存里，而冷门商品被访问得少，就只存放在 HDD 硬盘上，数据的读取也都是直接访问硬盘。即使加载到内存中，也会很快被移除。<strong>越是热门的商品，越容易在内存中找到，也就更好地利用了内存的随机访问性能。</strong> （缓存命中率）</p>
<p>那么，只放 600 万件商品真的可以满足我们实际的线上服务请求吗？这个就要看 LRU 缓存策略的 <strong>缓存命中率</strong>（Hit Rate&#x2F;Hit Ratio）了，也就是访问的数据中，可以在我们设置的内存缓存中找到的，占有多大比例。</p>
<p>内存的随机访问请求需要 100ns。这也就意味着，在极限情况下，内存可以支持 1000 万次随机访问。我们用了 24TB 内存，如果 8G 一条的话，意味着有 3000 条内存，可以支持每秒 300 亿次（ &#x3D; 24TB&#x2F;8GB × 1s&#x2F;100ns）访问。以亚马逊 2017 年 3 亿的用户数来看，我们估算每天的活跃用户为 1 亿，这 1 亿用户每人平均会访问 100 个商品，那么平均每秒访问的商品数量，就是 12 万次。</p>
<p>但是如果数据没有命中内存，那么对应的数据请求就要访问到 HDD 磁盘了。刚才的图表中，我写了，一块 HDD 硬盘只能支撑每秒 100 次的随机访问，2400TB 的数据，以 4TB 一块磁盘来计算，有 600 块磁盘，也就是能支撑每秒 6 万次（ &#x3D; 2400TB&#x2F;4TB × 1s&#x2F;10ms ）的随机访问。</p>
<p>这就意味着，所有的商品访问请求，都直接到了 HDD 磁盘，HDD 磁盘支撑不了这样的压力。我们至少要 50% 的缓存命中率，HDD 磁盘才能支撑对应的访问次数。不然的话，我们要么选择添加更多数量的 HDD 硬盘，做到每秒 12 万次的随机访问，或者将 HDD 替换成 SSD 硬盘，让单个硬盘可以支持更多的随机访问请求。</p>
<hr>
<p>当然，这里我们只是一个简单的估算。在实际的应用程序中，查看一个商品的数据可能意味着不止一次的随机内存或者随机磁盘的访问。对应的数据存储空间也不止要考虑数据，还需要考虑维护数据结构的空间，而缓存命中率和访问请求也要考虑均值和峰值问题。</p>
<p>通过这个估算过程，你需要理解，如何进行存储器的硬件规划。你需要考虑硬件的成本、访问的数据量以及访问的数据分布，然后根据这些数据的估算，来组合不同的存储器，能用尽可能低的成本支撑所需要的服务器压力。而当你用上了数据访问的局部性原理，组合起了多种存储器，你也就理解了怎么基于存储器层次结构，来进行硬件规划了。</p>
<h2 id="总结延伸"><a href="#总结延伸" class="headerlink" title="总结延伸"></a>总结延伸</h2><p>在实际的计算机日常的开发和应用中，我们对于数据的访问总是会存在一定的局部性。 有时候，这个局部性是时间局部性，就是我们最近访问过的数据还会被反复访问。有时候，这个局部性是空间局部性，就是我们最近访问过数据附近的数据很快会被访问到。</p>
<p>而局部性的存在，使得我们可以在应用开发中使用缓存这个有利的武器。比如，<strong>通过将热点数据加载并保留在速度更快的存储设备里面，我们可以用更低的成本来支撑服务器。</strong></p>
<hr>
<p>通过亚马逊这个例子，我们可以看到，我们可以通过快速估算的方式，来判断这个添加缓存的策略是否能够满足我们的需求，以及在估算的服务器负载的情况下，需要规划多少硬件设备。这个“估算 + 规划”的能力，是每一个期望成长为架构师的工程师，必须掌握的能力。</p>
]]></content>
      <categories>
        <category>计算机组成原理</category>
      </categories>
      <tags>
        <tag>数据存储</tag>
      </tags>
  </entry>
  <entry>
    <title>高速缓存（上）</title>
    <url>/posts/dccb8a89/</url>
    <content><![CDATA[<h1 id="高速缓存（上）：“4毫秒”究竟值多少钱？"><a href="#高速缓存（上）：“4毫秒”究竟值多少钱？" class="headerlink" title="高速缓存（上）：“4毫秒”究竟值多少钱？"></a>高速缓存（上）：“4毫秒”究竟值多少钱？</h1><p>这一节内容开始之前，我们先来看一个 3 行的小程序。你可以猜一猜，这个程序里的循环 1 和循环 2，运行所花费的时间会差多少？<span id="more"></span>你可以先思考几分钟，然后再看我下面的解释。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">int[] arr = new int[64 * 1024 * 1024];</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">// 循环1</span><br><span class="line">for (int i = 0; i &lt; arr.length; i++) arr[i] *= 3;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">// 循环2</span><br><span class="line">for (int i = 0; i &lt; arr.length; i += 16) arr[i] *= 3</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>在这段 Java 程序中，我们首先构造了一个 64×1024×1024 大小的整型数组。在循环 1 里，我们遍历整个数组，将数组中每一项的值变成了原来的 3 倍；在循环 2 里，我们每隔 16 个索引访问一个数组元素，将这一项的值变成了原来的 3 倍。</p>
<p>按道理来说，循环 2 只访问循环 1 中 1&#x2F;16 的数组元素，只进行了循环 1 中 1&#x2F;16 的乘法计算，那循环 2 花费的时间应该是循环 1 的 1&#x2F;16 左右。但是实际上，循环 1 在我的电脑上运行需要 50 毫秒，循环 2 只需要 46 毫秒。这两个循环花费时间之差在 15% 之内。</p>
<hr>
<p>为什么会有这 15% 的差异呢？这和我们今天要讲的 CPU Cache 有关。之前我们看到了内存和硬盘之间存在的巨大性能差异。在 CPU 眼里，内存也慢得不行。于是，聪明的工程师们就在 CPU 里面嵌入了 CPU Cache（高速缓存），来解决这一问题。</p>
<h2 id="我们为什么需要高速缓存"><a href="#我们为什么需要高速缓存" class="headerlink" title="我们为什么需要高速缓存?"></a>我们为什么需要高速缓存?</h2><p>按照摩尔定律，CPU 的访问速度每 18 个月便会翻一番，相当于每年增长 60%。内存的访问速度虽然也在不断增长，却远没有这么快，每年只增长 7% 左右。而这两个增长速度的差异，使得 CPU 性能和内存访问性能的差距不断拉大。到今天来看，一次内存的访问，大约需要 120 个 CPU Cycle，这也意味着，在今天，CPU 和内存的访问速度已经有了 120 倍的差距。</p>
<p>如果拿我们现实生活来打个比方的话，CPU 的速度好比风驰电掣的高铁，每小时 350 公里，然而，它却只能等着旁边腿脚不太灵便的老太太，也就是内存，以每小时 3 公里的速度缓慢步行。因为 CPU 需要执行的指令、需要访问的数据，都在这个速度不到自己 1% 的内存里。</p>
<img src="/img/computer_img/CPU和内存性能差异.webp" alt="CPU和内存性能差异" style="zoom:50%;" />



<p>为了弥补两者之间的性能差异，我们能真实地把 CPU 的性能提升用起来，而不是让它在那儿空转，我们在现代 CPU 中引入了高速缓存。</p>
<p>从 CPU Cache 被加入到现有的 CPU 里开始，内存中的指令、数据，会被加载到 L1-L3 Cache 中，而不是直接由 CPU 访问内存去拿。在 95% 的情况下，CPU 都只需要访问 L1-L3 Cache，从里面读取指令和数据，而无需访问内存。要注意的是，要注意的是，这里我们说的 CPU  cache 或者 L1&#x2F;L3  Cache，不是一个单纯的、概念上的缓存（比如之前我们说的拿内存作为硬盘的缓存）），而是指特定的由 SRAM 组成的物理芯片。 </p>
<p>这里是一张 Intel CPU 的放大照片。这里面大片的长方形芯片，就是这个 CPU 使用的 20MB 的  L3 Cache。</p>
<img src="/img/computer_img/SRAM缓存.webp" alt="SRAM缓存" style="zoom:50%;" />

<p> 这一讲一开始的程序里，运行程序的时间主要花在将对应的数据从内存中读取出来，加载到CPU cache 里。 CPU 从内存中读取数据到 CPU Cache 的过程中，是<strong>一小块一小块来读取数据的</strong>（是一小块一小块来读取数据的），而不是按照单个数组元素来读取数据的。这样一小块一小块的数据，在 CPU Cache 里面，我们把它叫作  <strong>Cache Line（缓存块）</strong>。</p>
<p>在我们日常使用的 Intel 服务器或者 PC 里，Cache Line 的大小通常是 64 字节。而在上面的循环 2 里面，我们每隔 16 个整型数计算一次，16 个整型数正好是 64 个字节。于是，循环 1 和循环 2，需要把同样数量的 Cache Line 数据从内存中读取到 CPU Cache 中，最终两个程序花费的时间就差别不大了。</p>
<p>知道了为什么需要 CPU Cache，接下来，我们就来看一看，CPU 究竟是如何访问 CPU Cache 的，以及 CPU Cache 是如何组织数据，使得 CPU 可以找到自己想要访问的数据的。    因为 Cache 作为“缓存”的意思，在很多别的存储设备里面都会用到。为了避免你混淆，在表示抽象的“缓存“概念时，用中文的“缓存”；如果是 CPU Cache，我会用“高速缓存“或者英文的“Cache”，来表示。</p>
<h2 id="Cache-的数据结构和读取过程是什么样的？"><a href="#Cache-的数据结构和读取过程是什么样的？" class="headerlink" title="Cache 的数据结构和读取过程是什么样的？"></a>Cache 的数据结构和读取过程是什么样的？</h2><p>现代 CPU 进行数据读取的时候，无论数据是否已经存储在 Cache 中，CPU 始终会首先访问 Cache。只有当 CPU 在 Cache 中找不到数据的时候，才会去访问内存，并将读取到的数据写入 Cache 之中。  当时间局部性原理起作用后，这个最近刚刚被访问的数据，会很快再次被访问。而 Cache 的访问速度远远快于内存，这样，CPU 花在等待内存访问上的时间就大大变短了。</p>
<img src="/img/computer_img/访问cache.webp" alt="访问cache" style="zoom:50%;" />

<p>这样的访问机制，和我们自己在开发应用系统的时候，“使用内存作为硬盘的缓存”的逻辑是一样的。在各类基准测试（Benchmark）和实际应用场景中，CPU Cache 的命中率通常能达到 95% 以上。</p>
<p>问题来了，CPU 如何知道要访问的内存数据，存储在 Cache 的哪个位置呢？接下来，我就从最基本的直接映射 Cache（Direct Mapped Cache）说起，带你来看整个 Cache 的数据结构和访问逻辑。</p>
<h3 id="直接映射-Cache"><a href="#直接映射-Cache" class="headerlink" title="直接映射 Cache"></a>直接映射 Cache</h3><p>CPU 访问内存数据，是一小块一小块数据来读取的。对于读取内存中的数据，我们首先拿到的是数据所在的内存块（Block）的地址。 而直接映射 Cache 采用的策略，就是确保任何一个内存块的地址，始终映射到一个固定的 <strong>CPU Cache</strong> 地址（Cache Line）。而这个映射关系，通常用 mod 运算（求余运算）来实现。下面我举个例子帮你理解一下。</p>
<p>比如说，我们的主内存被分成 0～31 号这样 32 个块。我们一共有 8个缓存块。用户想要访问第 21 号内存块。如果 21 号内存块内容在缓存块中的话，它一定在 5 号缓存块（21 mod 8 &#x3D; 5）中。</p>
<img src="/img/computer_img/直接映射cache.webp" alt="直接映射cache" style="zoom:50%;" />



<p>实际计算中，有一个小小的技巧，通常我们会把缓存块的数量设置成 2 的 N 次方。这样在计算取模的时候，可以直接取地址的低 N 位，也就是二进制里面的后几位。比如这里的 8 个缓存块，就是 2 的 3 次方。那么，在对 21 取模的时候，可以对 21 的 2 进制表示 10101 取地址的低三位，也就是 101，对应的 5，就是对应的缓存块地址。</p>
<img src="/img/computer_img/低3位cache.webp" alt="低3位cache" style="zoom:33%;" />

<p>取 Block 地址的低位，就能得到对应的 Cache Line 地址，除了 21 号内存块外，13 号、5 号等很多内存块的数据，都对应着 5 号缓存块中。既然如此，假如现在 CPU 想要读取 21 号内存块，在读取到 5 号缓存块的时候，我们怎么知道里面的数据，究竟是不是 21 号对应的数据呢？</p>
<hr>
<p>这个时候，在对应的缓存块中，我们会存储一个<strong>组标记</strong>（Tag）。这个组标记会记录，当前缓存块内存存储的数据对应的内存块，而缓存块本身的地址表示访问地址的低 N 位。就像上面的例子，21 的低 3 位 101，缓存块本身的地址已经涵盖了对应的信息、对应的组标记，我们只需要记录 21 剩余的高 2 位的信息，也就是 10 就可以了。</p>
<p>除了组标记信息之外，缓存块中还有两个数据。一个自然是从主内存中加载来的实际存放的数据，另一个是<strong>有效位</strong>（valid bit），它其实就是用来标记，对应的缓存块中的数据是否是有效的，确保不是机器刚刚启动时候的空数据 。 <em>如果有效位是 0，无论其中的组标记和 Cache Line 里的数据内容是什么，CPU 都不会管这些数据，而要直接访问内存，重新加载数据。</em></p>
<hr>
<p>CPU 在读取数据的时候，并不是要读取一整个 Block，而是读取一个他需要的数据片段。这样的数据，我们叫作 CPU 里的一个字（Word）。具体是哪个字，就用这个字在整个 Block 里面的位置来决定。这个位置，我们叫作偏移量（Offset）</p>
<p>总结一下，<strong>一个内存的访问地址，最终包括高位代表的组标记、低位代表的索引，以及在对应的 Data Block 中定位对应字的位置偏移量。</strong></p>
<img src="/img/computer_img/内存地址到Cache Line.webp" alt="内存地址到Cache Line" style="zoom:50%;" />

<center>内存地址到 Cache Line 的关系</center>

<p>而内存地址对应到 Cache 里的数据结构，则多了一个有效位和对应的数据，由“索引 + 有效位 + 组标记 + 数据”组成。如果内存中的数据已经在 CPU Cache 里了，那一个内存地址的访问，就会经历这样 4 个步骤：</p>
<ol>
<li>根据内存地址的低位，计算在 Cache 中的索引；</li>
<li>判断有效位，确认 Cache 中的数据是有效的；</li>
<li>对比内存访问地址的高位，和 Cache 中的组标记，确认 Cache 中的数据就是我们要访问的内存数据，从 Cache Line 中读取到对应的数据块（Data Block）；</li>
<li>根据内存地址的 Offset 位，从 Data Block 中，读取希望读取到的字。</li>
</ol>
<p>如果在 2、3 这两个步骤中，CPU 发现，Cache 中的数据并不是要访问的内存地址的数据，那CPU 就会访问内存，并把对应的Block Data 更新到 Cache Line 中，同时更新对应的有效位和组标记的数据。</p>
<hr>
<p>讲到这里，相信你明白现代 CPU，是如何通过直接映射 Cache，来定位一个内存访问地址在 Cache 中的位置了。其实，除了直接映射 Cache 之外，我们常见的缓存放置策略还有全相连 Cache（Fully Associative Cache）、组相连 Cache（Set Associative Cache）。这几种策略的数据结构都是相似的，理解了最简单的直接映射 Cache，其他的策略你很容易就能理解了。</p>
<h2 id="减少-4-毫秒，公司挣了多少钱"><a href="#减少-4-毫秒，公司挣了多少钱" class="headerlink" title="减少 4 毫秒，公司挣了多少钱?"></a>减少 4 毫秒，公司挣了多少钱?</h2><p>讲了 CPU 和内存之间的性能差异，以及我们如何通过 CPU Cache 来尽可能解决这两者之间的性能鸿沟。你可能要问了，这样做的意义和价值究竟是什么？毕竟，一次内存的访问，只不过需要 100 纳秒而已。1 秒钟时间内，足有 1000 万个 100 纳秒。别着急，我们先来看一个故事。</p>
<p>2008 年，一家叫作 Spread Networks 的通信公司花费 3 亿美元，做了一个光缆建设项目。目标是建设一条从芝加哥到新泽西，总长 1331 公里的光缆线路。建设这条线路的目的，其实是为了将两地之间原有的网络访问延时，从 17 毫秒降低到 13 毫秒。</p>
<p>你可能会说，仅仅缩短了 4 毫秒时间啊，却花费 3 个亿，真的值吗？为这 4 毫秒时间买单的，其实是一批高频交易公司。它们以 5 年 1400 万美元的价格，使用这条线路。利用这短短的 4 毫秒的时间优势，这些公司通过高性能的计算机程序，在芝加哥和新泽西两地的交易所进行高频套利，以获得每年以 10 亿美元计的利润。现在你还觉得这个不值得吗？</p>
<p>其实，只要 350 微秒的差异，就足够高频交易公司用来进行无风险套利了。而 350 微秒，如果用来进行 100 纳秒一次的内存访问，大约只够进行 3500 次。而引入 CPU Cache 之后，我们可以进行的数据访问次数，提升了数十倍，使得各种交易策略成为可能。</p>
<h2 id="总结延伸"><a href="#总结延伸" class="headerlink" title="总结延伸"></a>总结延伸</h2><p>很多时候，程序的性能瓶颈，来自使用 DRAM 芯片的内存访问速度。</p>
<p>根据摩尔定律，自上世纪 80 年代以来，CPU 和内存的性能鸿沟越拉越大。于是，现代 CPU 的设计者们，直接在 CPU 中嵌入了使用更高性能的 SRAM 芯片的 Cache，来弥补这一性能差异。通过巧妙地将内存地址，拆分成“索引 + 组标记 + 偏移量”的方式，使得我们可以将很大的内存地址，映射到很小的 CPU Cache 地址里。而 CPU Cache 带来的毫秒乃至微秒级别的性能差异，又能带来巨大的商业利益。</p>
<p>在搞清楚从内存加载数据到 Cache，以及从 Cache 里读取到想要的数据之后，我们又要面临一个新的挑战了。CPU 不仅要读数据，还需要写数据，我们不能只把数据写入到 Cache 里面就结束了。下一讲会来说 CPU 要写入数据的时候，怎么既不牺牲性能，又能保持数据的一致性。</p>
]]></content>
      <categories>
        <category>计算机组成原理</category>
      </categories>
      <tags>
        <tag>cache</tag>
        <tag> 虚拟内存</tag>
      </tags>
  </entry>
  <entry>
    <title>高速缓存（下）</title>
    <url>/posts/6477edec/</url>
    <content><![CDATA[<h1 id="高速缓存（下）：你确定你的数据更新了么？"><a href="#高速缓存（下）：你确定你的数据更新了么？" class="headerlink" title="高速缓存（下）：你确定你的数据更新了么？"></a>高速缓存（下）：你确定你的数据更新了么？</h1><p>面试过大量的 Java 工程师。对于一些表示自己深入了解和擅长多线程的同学，我经常会问这样一个面试题：<strong>“volatile 这个关键字有什么作用？”</strong><span id="more"></span><br>就我面试过的工程师而言，即使是工作了多年的 Java 工程师，也很少有人能准确说出 volatile 这个关键字的含义。这里面最常见的理解错误有两个，一个是把 volatile 当成一种锁机制，认为给变量加上了 volatile，就好像是给函数加了 sychronized 关键字一样，不同的线程对于特定变量的访问会去加锁；    另一个是把 volatile 当成一种原子化的操作机制，认为加了 volatile 之后，对于一个变量的自增的操作就会变成原子性的了。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">// 一种错误的理解，是把volatile关键词，当成是一个锁，可以把long/double这样的数的操作自动加锁</span><br><span class="line">private volatile long synchronizedValue = 0;</span><br><span class="line"></span><br><span class="line">// 另一种错误的理解，是把volatile关键词，当成可以让整数自增的操作也变成原子性的</span><br><span class="line">private volatile int atomicInt = 0;</span><br><span class="line">amoticInt++;</span><br></pre></td></tr></table></figure>

<p>事实上，这两种理解都是完全错误的。很多工程师容易把 volatile 关键字，当成和锁或者数据数据原子性相关的知识点。而实际上，<strong>volatile 关键字的最核心知识点，要关系到 Java 内存模型（JMM，Java Memory Model）上。</strong></p>
<p>虽然 JMM 只是 Java 虚拟机这个进程级虚拟机里的一个内存模型，但是这个内存模型，和计算机组成里的 CPU、高速缓存和主内存组合在一起的硬件体系非常相似。理解了 JMM，可以让你很容易理解计算机组成里 CPU、高速缓存和主内存之间的关系。</p>
<hr>
<h2 id="“隐身”的变量"><a href="#“隐身”的变量" class="headerlink" title="“隐身”的变量"></a>“隐身”的变量</h2><p>我们先来一起看一段 Java 程序。这是一段经典的 volatile 代码，来自知名的 Java 开发者网站 [dzone.com][<a href="https://dzone.com/articles/java-volatile-keyword-0]%EF%BC%8C%E5%90%8E%E7%BB%AD%E6%88%91%E4%BB%AC%E4%BC%9A%E4%BF%AE%E6%94%B9%E8%BF%99%E6%AE%B5%E4%BB%A3%E7%A0%81%E6%9D%A5%E8%BF%9B%E8%A1%8C%E5%90%84%E7%A7%8D%E5%B0%8F%E5%AE%9E%E9%AA%8C%E3%80%82">https://dzone.com/articles/java-volatile-keyword-0]，后续我们会修改这段代码来进行各种小实验。</a></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">public class VolatileTest &#123;</span><br><span class="line">    private static volatile int COUNTER = 0;</span><br><span class="line"></span><br><span class="line">    public static void main(String[] args) &#123;</span><br><span class="line">        new ChangeListener().start();</span><br><span class="line">        new ChangeMaker().start();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    static class ChangeListener extends Thread &#123;</span><br><span class="line">        @Override</span><br><span class="line">        public void run() &#123;</span><br><span class="line">            int threadValue = COUNTER;</span><br><span class="line">            while ( threadValue &lt; 5)&#123;</span><br><span class="line">                if( threadValue!= COUNTER)&#123;</span><br><span class="line">                    System.out.println(&quot;Got Change for COUNTER : &quot; + COUNTER + &quot;&quot;);</span><br><span class="line">                    threadValue= COUNTER;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    static class ChangeMaker extends Thread&#123;</span><br><span class="line">        @Override</span><br><span class="line">        public void run() &#123;</span><br><span class="line">            int threadValue = COUNTER;</span><br><span class="line">            while (COUNTER &lt;5)&#123;</span><br><span class="line">                System.out.println(&quot;Incrementing COUNTER to : &quot; + (threadValue+1) + &quot;&quot;);</span><br><span class="line">                COUNTER = ++threadValue;</span><br><span class="line">                try &#123;</span><br><span class="line">                    Thread.sleep(500);</span><br><span class="line">                &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>在这个程序里，我们先定义了一个 volatile 的 int 类型的变量，COUNTER。   然后，我们分别启动了两个单独的线程，一个线程我们叫 ChangeListener。另一个线程，我们叫 ChangeMaker。</p>
<p>ChangeListener 这个线程运行的任务很简单。它先取到 COUNTER 当前的值，然后一直监听着这个 COUNTER 的值。一旦 COUNTER 的值发生了变化，就把新的值通过 println 打印出来。直到 COUNTER 的值达到 5 为止。这个监听的过程，通过一个永不停歇的 while 循环的忙等待来实现。</p>
<p>ChangeMaker 这个线程运行的任务同样很简单。它同样是取到 COUNTER 的值，在 COUNTER 小于 5 的时候，每隔 500 毫秒，就让 COUNTER 自增 1。在自增之前，通过 println 方法把自增后的值打印出来。</p>
<p>最后，在 main 函数里，我们分别启动这两个线程，来看一看这个程序的执行情况。程序的输出结果并不让人意外。ChangeMaker 函数会一次一次将 COUNTER 从 0 增加到 5。因为这个自增是每 500 毫秒一次，而 ChangeListener 去监听 COUNTER 是忙等待的，所以每一次自增都会被 ChangeListener 监听到，然后对应的结果就会被打印出来。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">Incrementing COUNTER to : 1</span><br><span class="line">Got Change for COUNTER : 1</span><br><span class="line">Incrementing COUNTER to : 2</span><br><span class="line">Got Change for COUNTER : 2</span><br><span class="line">Incrementing COUNTER to : 3</span><br><span class="line">Got Change for COUNTER : 3</span><br><span class="line">Incrementing COUNTER to : 4</span><br><span class="line">Got Change for COUNTER : 4</span><br><span class="line">Incrementing COUNTER to : 5</span><br><span class="line">Got Change for COUNTER : 5</span><br></pre></td></tr></table></figure>



<p>这个时候，我们就可以来做一个很有意思的实验。如果我们把上面的程序小小地修改一行代码，把我们定义 COUNTER 这个变量的时候，设置的 <strong>volatile 关键字</strong>给去掉，会发生什么事情呢？你可以自己先试一试，看结果是否会让你大吃一惊。</p>
<p><code>        private static int COUNTER = 0;</code></p>
<p>没错，你会发现，我们的 ChangeMaker 还是能正常工作的，每隔 500ms 仍然能够对 COUNTER 自增 1。但是，奇怪的事情在 ChangeListener 上发生了，我们的 ChangeListener 不再工作了。在 ChangeListener 眼里，它似乎一直觉得 COUNTER 的值还是一开始的 0。似乎 COUNTER 的变化，对于我们的 ChangeListener 彻底“隐身”了。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">Incrementing COUNTER to : 1</span><br><span class="line">Incrementing COUNTER to : 2</span><br><span class="line">Incrementing COUNTER to : 3</span><br><span class="line">Incrementing COUNTER to : 4</span><br><span class="line">Incrementing COUNTER to : 5</span><br></pre></td></tr></table></figure>

<p>这个有意思的小程序还没有结束，我们可以再对程序做一些小小的修改。<strong>我们不再让 ChangeListener 进行完全的忙等待，而是在 while 循环里面，小小地等待上 5 毫秒，看看会发生什么情况。</strong></p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">static class ChangeListener extends Thread &#123;</span><br><span class="line">    @Override</span><br><span class="line">    public void run() &#123;</span><br><span class="line">        int threadValue = COUNTER;</span><br><span class="line">        while ( threadValue &lt; 5)&#123;</span><br><span class="line">            if( threadValue!= COUNTER)&#123;</span><br><span class="line">                System.out.println(&quot;Sleep 5ms, Got Change for COUNTER : &quot; + COUNTER + &quot;&quot;);</span><br><span class="line">                threadValue= COUNTER;</span><br><span class="line">            &#125;</span><br><span class="line">            try &#123;</span><br><span class="line">                Thread.sleep(5);</span><br><span class="line">            &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>好了，不知道你有没有自己动手试一试呢？又一个令人惊奇的现象要发生了。虽然我们的 COUNTER 变量，仍然没有设置 volatile 这个关键字，但是我们的 ChangeListener 似乎“睡醒了”。在通过 Thread.sleep(5) 在每个循环里“睡上“5 毫秒之后，ChangeListener 又能够正常取到 COUNTER 的值了。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Incrementing COUNTER to : 1</span><br><span class="line">Sleep 5ms, Got Change for COUNTER : 1</span><br><span class="line">Incrementing COUNTER to : 2</span><br><span class="line">Sleep 5ms, Got Change for COUNTER : 2</span><br><span class="line">Incrementing COUNTER to : 3</span><br><span class="line">Sleep 5ms, Got Change for COUNTER : 3</span><br><span class="line">Incrementing COUNTER to : 4</span><br><span class="line">Sleep 5ms, Got Change for COUNTER : 4</span><br><span class="line">Incrementing COUNTER to : 5</span><br><span class="line">Sleep 5ms, Got Change for COUNTER : 5</span><br></pre></td></tr></table></figure>

<p>这些有意思的现象，其实来自于我们的 Java 内存模型以及关键字 volatile 的含义。<strong>那 volatile 关键字究竟代表什么含义呢？它会确保我们对于这个变量的读取和写入，都一定会同步到 主内存 里，而不是从  Cache  里面读取。</strong></p>
<p>刚刚第一个使用了 volatile 关键字的例子里，因为所有数据的读和写都来自主内存。那么自然地，我们的 ChangeMaker 和 ChangeListener 之间，看到的 COUNTER 值就是一样的。</p>
<p>到了第二段进行小小修改的时候，我们去掉了 volatile 关键字。这个时候，ChangeListener 又是一个忙等待的循环，它尝试不停地获取 COUNTER 的值，这样就会从当前线程的“Cache”里面获取。 于是，这个线程没有时间从主内存里面同步 更新后的counter 值，这样就一直卡死在 counter&#x3D;0 的死循环了。</p>
<p>而到了我们再次修改的第三段代码里面，虽然还是没有使用 volatile 关键字，<em>但是短短 5ms 的 Thead.Sleep 给了这个线程喘息之机。既然这个线程没有这么忙了，它也就有机会把最新的数据从主内存同步到自己的高速缓存里面了。</em> 于是，ChangeListener 在下一次查看 COUNTER 值的时候，就能看到 ChangeMaker 造成的变化了。</p>
<hr>
<p>虽然 Java 内存模型是一个隔离了硬件实现的虚拟机内的抽象模型，但是它给了我们一个很好的“缓存同步”问题的示例：</p>
<p>也就是，如果我们的数据，在不同的线程或者CPU 核心里面去更新，因为不同的线程或者 CPU 核都有各自的缓存，很可能在A 线程的更新，到 B 线程里面是看不见的。</p>
<h2 id="CPU-高速缓存的写入"><a href="#CPU-高速缓存的写入" class="headerlink" title="CPU 高速缓存的写入"></a>CPU 高速缓存的写入</h2><p>事实上，我们可以把 Java 内存模型和计算机组成里的 CPU 结构对照起来看。</p>
<p>我们现在用的 Intel CPU，通常都是多核的的。每一个 CPU 核里面，都有独立属于自己的 L1、L2 的 Cache，然后再有多个 CPU 核共用的 L3 的 Cache、主内存。</p>
<p>因为 CPU Cache 的访问速度要比主内存快很多，而在 CPU Cache 里面，L1&#x2F;L2 的 Cache 也要比 L3 的 Cache 快。</p>
<img src="/img/computer_img/CPU cache.webp" alt="CPU cache" style="zoom:50%;" />

<p>这个层级结构，就好像我们在 Java 内存模型里面，每一个线程都有属于自己的线程栈。线程在读取 COUNTER 的数据的时候，其实是从本地的线程栈的Cache 副本里面读取数据，而不是从主内存里面读取数据 。</p>
<p>但是，对于数据，我们不光要读，还要去写入修改。这个时候，有两个问题来了。</p>
<ol>
<li>写入 Cache 的性能也比写入主内存要快，那我们写入的数据，到底应该写到 Cache 里还是主内存呢？</li>
<li>如果我们直接写入到主内存里，Cache 里的数据是否会失效呢？</li>
</ol>
<h3 id="写直达"><a href="#写直达" class="headerlink" title="写直达"></a>写直达</h3><img src="/img/computer_img/计算机组成/写直达.webp" alt="写直达" style="zoom:33%;" />

<p>最简单的一种写入策略，叫作写直达（Write-Through）。在这个策略里，<strong>每一次数据都要写入到主内存里面</strong>。 在这个策略中，写入前，我们会先去判断数据是否已经在 Cache 里面了。如果数据已经在 Cache 里面了，我们先把数据写入更新到 Cache 里面，再写入到主内存里面； 如果数据不在 Cache 里，我们就只更新主内存。 </p>
<p>写直达的这个策略很直观，但是问题也很明显，那就是这个策略很慢。<strong>无论数据是不是在 Cache 里面，我们都需要把数据写到主内存里面。</strong>这个方式就有点儿像我们上面用 volatile 关键字，始终都要把数据同步到主内存里面。</p>
<h3 id="写回"><a href="#写回" class="headerlink" title="写回"></a>写回</h3><ul>
<li>这个时候，我们就想了，既然我们去读数据也是默认从 Cache 里面加载，能否不用把所有的写入都同步到主内存里呢？只写入 CPU Cache 里面是不是可以？</li>
</ul>
<img src="/img/computer_img/写回.webp" alt="写回" style="zoom:33%;" />

<p>在 CPU Cache 的写入策略里，还有一种策略就叫作写回（Write-Back）。这个策略里， <em>我们不再是每次都把数据写入到主内存，而是只写到 CPU Cache 里。只有当 CPU Cache 里面的数据要被“替换”的时候，我们才把数据写入到主内存里面去。</em></p>
<hr>
<p>写回策略的过程是这样的：</p>
<p>如果发现我们要写入的数据，就在 CPU Cache 里面，那么我们就只是更新 CPU Cache 里面的数据。同时，我们会标记 CPU Cache 里的这个 Block 是脏（Dirty）的。 所谓脏的，就是指这个时候，我们的 CPU Cache 里面的这个 Block 的数据，和主内存是不一致的。</p>
<p>如果我们发现，我们要写入的数据所对应的 Cache Block 里，放的是别的内存地址的数据。</p>
<ol>
<li>如果是脏的话，我们要先把这个 Cache Block 里面的数据，写入到主内存里面。然后，再把当前要写入的数据，写入到 Cache 里，同时把 Cache Block 标记成脏的。</li>
<li>如果 Block 里面的数据没有被标记成脏的，那么我们直接把数据写入到 Cache 里面，然后再把 Cache Block 标记成脏的就好了。</li>
</ol>
<p>在用了写回这个策略之后，我们在加载内存数据到 Cache 里面的时候，也要多出一步同步脏 Cache 的动作。 如果加载内存里的数据到Cache 的时候，<strong>发现 Cache Block 里面的数据没有标记成脏的，我们也要先把 Cache Block 里的数据写回到主内存，才能加载数据覆盖掉 Cache。</strong></p>
<ul>
<li>可以看到，在写回这个策略里，如果我们大量的操作，都能够命中缓存。那么大部分时间里，我们都不需要读写主内存，自然性能会比写直达的效果好很多。</li>
</ul>
<hr>
<p>然而，无论是写回还是写直达，其实都还没有解决我们在上面 volatile 程序示例中遇到的问题，也就是 <strong>多个线程，或者是多个 CPU 核的缓存一致性的问题。</strong>这也就是我们在写入修改缓存后，需要解决的第二个问题。</p>
<p>要解决这个问题，我们需要引入一个新的方法，叫作 MESI 协议。这是一个维护缓存一致性协议。这个协议不仅可以用在 CPU Cache 之间，也可以广泛用于各种需要使用缓存，同时缓存之间需要同步的场景下。</p>
<h2 id="总结延伸"><a href="#总结延伸" class="headerlink" title="总结延伸"></a>总结延伸</h2><p>过一个使用 Java 程序中使用 volatile 关键字程序，我们可以看到，在有缓存的情况下会遇到一致性问题。volatile 这个关键字可以保障我们对于数据的读写都会到达主内存。</p>
<p>我们可以看到，Java 内存模型和 CPU、CPU Cache 以及主内存的组织结构非常相似。在 CPU Cache 里，对于数据的写入，我们也有写直达和写回这两种解决方案。     写直达把所有的数据都直接写入到主内存里面，简单直观，但是性能就会受限于内存的访问速度。      而写回则通常只更新缓存，只有在需要把缓存里面的脏数据交换出去的时候，才把数据同步到主内存里。在缓存经常会命中的情况下，性能更好。</p>
<hr>
<p>但是，除了采用读写都直接访问主内存的办法之外，如何解决缓存一致性的问题，我们还是没有解答。</p>
]]></content>
      <categories>
        <category>计算机组成原理</category>
      </categories>
      <tags>
        <tag>高速缓存</tag>
        <tag>写回</tag>
      </tags>
  </entry>
  <entry>
    <title>MESI协议</title>
    <url>/posts/a2fa04e/</url>
    <content><![CDATA[<h1 id="MESI协议：如何让多核CPU的高速缓存保持一致？"><a href="#MESI协议：如何让多核CPU的高速缓存保持一致？" class="headerlink" title="MESI协议：如何让多核CPU的高速缓存保持一致？"></a>MESI协议：如何让多核CPU的高速缓存保持一致？</h1><p>多核 CPU 有很多好处，其中最重要的一个就是，它使得我们在不能提升 CPU 的主频之后，找到了另一种提升 CPU 吞吐率<span id="more"></span>的办法。</p>
<ul>
<li><p>多核 CPU 里的每一个 CPU 核，都有独立的属于自己的 L1 Cache 和 L2 Cache。 多个CPU 之间 ， 只是共用 L3 Cache 和主内存。</p>
<blockquote>
<p>CPU Cache 解决的是内存访问速度和 CPU 的速度差距太大的问题。而多核 CPU 提供的是，在主频难以提升的时候，通过增加 CPU 核心来提升 CPU 的吞吐率的办法。我们把多核和 CPU Cache 两者一结合，就给我们带来了一个新的挑战。因为 CPU 的每个核各有各的缓存，互相之间的操作又是各自独立的，就会带来<strong>缓存一致性（Cache Coherence）</strong>的问题。</p>
</blockquote>
</li>
</ul>
<h2 id="缓存一致性问题"><a href="#缓存一致性问题" class="headerlink" title="缓存一致性问题"></a>缓存一致性问题</h2><p>比方说，iPhone 降价了，我们要把 iPhone 最新的价格更新到内存里。为了性能问题，它采用了上一讲我们说的写回策略，先把数据写入到 L2 Cache 里面，然后把 Cache Block 标记成脏的。这个时候，数据其实并没有被同步到 L3 Cache 或者主内存里。1 号核心希望在这个 Cache Block 要被交换出去的时候，数据才写入到主内存里。</p>
<p>如果我们的 CPU 只有 1 号核心这一个 CPU 核，那这其实是没有问题的。不过，我们旁边还有一个 2 号核心呢！这个时候，2 号核心尝试从内存里面去读取 iPhone 的价格，结果读到的是一个错误的价格。这是因为，iPhone 的价格刚刚被 1 号核心更新过。但是这个更新的信息，只出现在 1 号核心的 L2 Cache 里，而没有出现在 2 号核心的 L2 Cache 或者主内存里面。 <strong>这个问题，就是所谓的缓存一致性问题，1 号核心和 2 号核心的缓存，在这个时候是不一致的。</strong></p>
<p>为了解决这个缓存不一致的问题，我们就需要有一种机制，来同步两个不同核心里面的缓存数据。那这样的机制需要满足什么条件呢？我觉得能够做到下面两点就是合理的。</p>
<p>一、 第一点叫写传播（Write Propagation）。写传播是说，在一个 CPU 核心里，我们的 Cache 数据更新，必须能够传播到其他的对应节点的 Cache Line 里。</p>
<p>二、 第二点叫事务的串行化（Transaction Serialization），事务串行化是说，我们在一个 CPU 核心里面的读取和写入，在其他的节点看起来，顺序是一样的。</p>
<p>从1 号到4 号核心，都能看到相同顺序的数据变化。  事务的串行化，不仅仅是缓存一致性中所必须的。比如，我们平时所用到的系统当中，最需要保障事务串行化的就是数据库。多个不同的连接去访问数据库的时候，我们必须保障事务的串行化，做不到事务的串行化的数据库，根本没法作为可靠的商业数据库来使用。</p>
<ul>
<li>而在 CPU Cache 里做到事务串行化，需要做到两点<ol>
<li>一个 CPU 核心对于数据的操作，需要同步通信给其他 CPU 核心</li>
<li>两个 CPU 核心里有同一个数据的 Cache，那么对于这个ca che 数据的更新，需要有一个“锁”的概念 ，只有 拿到了对应 Cache Block 的锁之后，才能进行对应的数据更新。</li>
</ol>
</li>
</ul>
<hr>
<h2 id="总线嗅探机制和-MESI-协议"><a href="#总线嗅探机制和-MESI-协议" class="headerlink" title="总线嗅探机制和 MESI 协议"></a>总线嗅探机制和 MESI 协议</h2><p>要解决缓存一致性问题，首先要解决的是多个 CPU 核心之间的数据传播问题。最常见的一种解决方案呢，叫作总线嗅探（Bus Snooping）。</p>
<p>本质上就是把所有的读写请求都通过总线（Bus）广播给所有的 CPU 核心，然后让各个核心去“嗅探”这些请求，再根据本地的情况进行响应。</p>
<ul>
<li>基于总线嗅探机制，其实还可以分成很多种不同的缓存一致性协议。不过其中最常用的，就是今天我们要讲的 MESI 协议。</li>
</ul>
<h3 id="MESI-协议-（写失效协议）"><a href="#MESI-协议-（写失效协议）" class="headerlink" title="MESI 协议 （写失效协议）"></a>MESI 协议 （写失效协议）</h3><p>在写失效协议里，只有一个 CPU 核心负责写入数据，其他的核心，只是同步读取到这个写入。在这个CPU 核心写入cache 之后，他会去广播一个“失效”请求告诉所有其他的 CPU 核心。其他的CPU 核心只是去判断自己是否也有一个“失效”版本的 Cache Block ，然后把这个也标记成失效的就好了。</p>
<h3 id="写广播协议"><a href="#写广播协议" class="headerlink" title="写广播协议"></a>写广播协议</h3><p>在那个协议里，一个写入请求广播到所有的 CPU 核心，同时更新各个核心里的 Cache。</p>
<p>写广播在实现上自然很简单，但是写广播需要<strong>占用更多的总线带宽</strong>。写失效只需要告诉其他的 CPU 核心，哪一个内存地址的缓存失效了（只需要去传输一个操作信号和一个地址信号），但是写广播还需要把对应的数据传输给其他 CPU 核心。</p>
<hr>
<p>MESI 协议的由来呢，来自于我们对 Cache Line 的四个不同的标记，分别是：</p>
<ul>
<li>M：代表已修改（Modified）</li>
<li>E：代表独占（Exclusive）</li>
<li>S：代表共享（Shared）</li>
<li>I：代表已失效（Invalidated）</li>
</ul>
<p>我们先来看看“已修改”和“已失效”，这两个状态比较容易理解。所谓的“已修改”，就是我们上一讲所说的“脏”的 Cache Block。<u>Cache Block 里面的内容我们已经更新过了，但是还没有写回到主内存里面。</u>  而所谓的“已失效“，自然是这个 Cache Block 里面的数据已经失效了，我们不可以相信这个 Cache Block 里面的数据。</p>
<p>然后，我们再来看<strong>“独占”</strong>和<strong>“共享”</strong>这两个状态。这就是 MESI 协议的精华所在了。无论是独占状态还是共享状态，缓存里面的数据都是“干净”的。 这个“干净”，自然对应的是前面所说的“脏”的，也就是说，<u><strong>这个时候，Cache Block 里面的数据和主内存里面的数据是一致的。</strong></u></p>
<hr>
<p>“独占”和“共享”这两个状态的差别:</p>
<p>在独占状态下，对应的 Cache Line 只加载到了当前 CPU 核所拥有的 Cache 里。其他的 CPU 核，并没有加载对应的数据到自己的 Cache 里。这个时候，如果要向独占的 Cache Block 写入数据，我们可以自由地写入数据，而不需要告知其他 CPU 核。</p>
<p>在独占状态下的数据，如果收到了一个来自于总线的读取对应缓存的请求，它就会变成共享状态。这个共享状态是因为，这个时候，另外一个 CPU 核心，也把对应的 Cache Block，从内存里面加载到了自己的 Cache 里来。</p>
<ul>
<li>而在共享状态下，因为同样的数据在多个 CPU 核心的 Cache 里都有。所以，当我们想要更新 Cache 里面的数据的时候，不能直接修改，而是要先向所有的其他 CPU 核心广播一个请求，要求先把其他 CPU 核心里面的 Cache，都变成无效的状态. 这个广播操作，一般叫作 RFO（Request For Ownership），也就是获取当前对应 Cache Block 数据的所有权。</li>
</ul>
<p>这个操作有点儿像我们在多线程里面用到的读写锁。在共享状态下，大家都可以并行去读对应的数据。但是如果要写，我们就需要通过一个锁，获取当前写入位置的所有权。</p>
<p>整个 MESI 的状态，可以用一个有限状态机来表示它的状态流转。需要注意的是，对于不同状态触发的事件操作，可能来自于当前 CPU 核心，也可能来自总线里其他 CPU 核心广播出来的信号。</p>
<hr>
<h2 id="总结延伸"><a href="#总结延伸" class="headerlink" title="总结延伸"></a>总结延伸</h2><p>这一节，我们其实就讲了两块儿内容，一个是缓存一致性，另一个是 MESI 协议。</p>
<ol>
<li><p>想要实现缓存一致性，关键是要满足两点。第一个是写传播，也就是在一个 CPU 核心写入的内容，需要传播到其他 CPU 核心里。更重要的是第二点，保障事务的串行化，才能保障我们的数据是真正一致的，我们的程序在各个不同的核心上运行的结果也是一致的。这个特性不仅在 CPU 的缓存层面很重要，在数据库层面更加重要。</p>
</li>
<li><p>我介绍了基于总线嗅探机制的 MESI 协议。MESI 协议是一种基于写失效的缓存一致性协议。写失效的协议的好处是，我们不需要在总线上传输数据内容，而只需要传输操作信号和地址信号就好了，不会那么占总线带宽。</p>
</li>
<li><p>MESI 协议，是已修改、独占、共享以及已失效这四个缩写的合称。独占和共享状态，就好像我们在多线程应用开发里面的读写锁机制，确保了我们的缓存一致性。而整个 MESI 的状态变更，则是根据来自自己 CPU 核心的请求，以及来自其他 CPU 核心通过总线传输过来的操作信号和地址信息，进行状态流转的一个有限状态机。</p>
</li>
</ol>
<p>sql里面不用sql拼接 而使用占位符可以防止sql注入攻击吧<br>那么为什么使用占位符可以防止攻击呢 ？</p>
<p>简单来说就是将占位符当成数据解析，而不是指令解析。不管占位符是什么，我都把它当成 数据，而不是指令。</p>
]]></content>
      <categories>
        <category>计算机组成原理</category>
      </categories>
      <tags>
        <tag>缓存一致性</tag>
      </tags>
  </entry>
  <entry>
    <title>理解内存</title>
    <url>/posts/ca0de21c/</url>
    <content><![CDATA[<h1 id="3-6-理解内存"><a href="#3-6-理解内存" class="headerlink" title="3.6 理解内存"></a>3.6 理解内存</h1><h2 id="上部分：-虚拟内存和内存保护是什么"><a href="#上部分：-虚拟内存和内存保护是什么" class="headerlink" title="上部分： 虚拟内存和内存保护是什么"></a>上部分： 虚拟内存和内存保护是什么</h2><p>计算机有五大组成部分，分别是：运算器、控制器、存储器、输入设备和输出设备。 如果说计算机最重要的组件，是承担了运算器和控制器作用的 CPU，那内存<span id="more"></span>就是我们第二重要的组件了。内存是五大组成部分里面的存储器，我们的指令和数据，都需要先加载到内存里面，才会被 CPU 拿去执行。</p>
<blockquote>
<p>在我们日常使用的 Linux 或者 Windows 操作系统下，程序并不能直接访问物理内存。</p>
<p>我们的内存需要被分成固定大小的页（Page），然后再通过虚拟内存地址（Virtual Address）到物理内存地址（Physical Address）的地址转换（Address Translation），才能到达实际存放数据的物理内存位置。而我们的程序看到的内存地址，都是虚拟内存地址。</p>
</blockquote>
<h3 id="简单页表"><a href="#简单页表" class="headerlink" title="简单页表"></a>简单页表</h3><p>一张映射表。这个映射表，能够实现虚拟内存里面的页，到物理内存里面的页的一一映射。这个映射表，在计算机里面，就叫作页表（Page Table）。</p>
<p>页表这个地址转换的办法，会把一个内存地址分成<strong>页号（Directory）</strong>和<strong>偏移量（Offset）</strong>两个部分。</p>
<p>其实，前面的高位，就是内存地址的页号。后面的低位，就是内存地址里面的偏移量。做地址转换的页表，只需要保留虚拟内存地址的页号和物理内存地址的页号之间的映射关系就可以了。同一个页里面的内存，在物理层面是连续的。以一个页的大小是 4K 字节（4KB）为例，我们需要 20 位的高位，12 位的低位。</p>
<img src="/img/computer_img/页表.webp" alt="页表" style="zoom:33%;" />

<center>页表</center>

<p>对于一个内存地址转换，其实就是这样三个步骤：</p>
<ol>
<li>把虚拟内存地址，切分成页号和偏移量的组合；</li>
<li>从页表里面，查询出虚拟页号，对应的物理页号；</li>
<li>直接拿物理页号，加上前面的偏移量，就得到了物理内存地址。</li>
</ol>
<hr>
<p>算一算，这样一个页表需要多大的空间吗？我们以 32 位的内存地址空间为例，</p>
<p>页表一共需要记录 2^20 个到物理页号的映射关系。这个存储关系，就好比一个 2^20 大小的数组。一个页号是完整的 32 位的 4 字节（Byte），这样一个页表就需要 4MB 的空间。听起来 4MB 的空间好像还不大啊，毕竟我们现在的内存至少也有 4GB，服务器上有个几十 GB 的内存和很正常。</p>
<p>不过，这个空间可不是只占用一份哦。我们每一个进程，都有属于自己独立的虚拟内存地址空间。这也就意味着，每一个进程都需要这样一个页表。不管我们这个进程，是个本身只有几 KB 大小的程序，还是需要几 GB 的内存空间，都需要这样一个页表。</p>
<p>这还只是 32 位的内存地址空间，现在大家用的内存，多半已经超过了 4GB，也已经用上了 64 位的计算机和操作系统。这样的话，用上面这个数组的数据结构来保存页面，内存占用就更大了。那么，我们有没有什么更好的解决办法呢？</p>
<hr>
<h3 id="多级页表"><a href="#多级页表" class="headerlink" title="多级页表"></a>多级页表</h3><p>仔细想一想，我们其实没有必要存下这 2^20 个物理页表啊。大部分进程所占的内存是有限的，所需要的页也自然是很有限的。我们只需要去存那些用到的页之间的映射关系就好了。</p>
<p>在实践中，我们其实采用的是一种叫作多级页表（Multi-Level Page Table）的解决方案。这是为什么呢？为什么我们不用哈希表而用多级页表呢？接下来我会慢慢解释</p>
<p>一个进程的内存地址空间是怎么分配的。在整个进程的内存地址空间，通常是“两头实、中间空”。 在程序运行的时候，内存地址从顶部往下，不断占用栈的空间，而堆的空间，内存地址这是从底部往上，是不断分配占用的。</p>
<p>所以，在一个实际的程序进程里，虚拟内存占用的地址空间，通常是两段连续的空间。 而不是完全散落的随机的内存地址。 多级列表，就比较适合这样的内存地址分布。</p>
<p>以一个 4 级的多级页表为例，来看一下。同样一个虚拟内存地址，偏移量的部分和上面简单页表一样不变，但是原先的页号部分，我们把它拆成四段，从高到低，分成 4 级到 1 级这样 4 个页表索引。</p>
<img src="/img/computer_img/内存地址分布.webp" alt="内存地址分布" style="zoom:33%;" />

<p>对应的，一个进程会有一个 4 级页表。我们先通过 4 级页表索引，找到 4 级页表里面对应的条目（Entry）。这个条目里存放的是一张 3 级页表所在的位置。4 级页面里面的每一个条目，都对应着一张 3 级页表，所以我们可能有多张 3 级页表。</p>
<p>找到对应这张 3 级页表之后，我们用 3 级索引去找到对应的 3 级索引的条目。3 级索引的条目再会指向一个 2 级页表。同样的，2 级页表里我们可以用 2 级索引指向一个 1 级页表。</p>
<p>而最后一层的 1 级页表里面的条目，对应的数据内容就是<strong>物理页号</strong>了。在拿到了物理页号之后，我们同样可以用<strong>“页号 + 偏移量”</strong>的方式，来获取最终的物理内存地址。</p>
<p>事实上，多级页表就像一个多叉树的数据结构，所以我们常常称它为页表树（Page Table Tree）。因为虚拟内存地址分布的连续性，树的第一层节点的指针，很多就是空的，也就不需要有对应的子树了。所谓不需要子树，其实就是不需要对应的 2 级、3 级的页表。找到最终的物理页号，就好像通过一个特定的访问路径，走到树最底层的叶子节点。</p>
<img src="/img/computer_img/页表树.webp" alt="页表树" style="zoom:50%;" />

<p>以这样的分成 4 级的多级页表来看，每一级如果都用 5 个比特表示。那么每一张某 1 级的页表，只需要 2^5&#x3D;32 个条目。如果每个条目还是 4 个字节，那么一共需要 128 个字节。而一个 1 级索引表，对应 32 个 4KB 的也就是 128KB 的大小。一个填满的 2 级索引表，对应的就是 32 个 1 级索引表，也就是 4MB 的大小。</p>
<p>可以一起来测算一下，一个进程如果占用了 8MB 的内存空间，分成了 2 个 4MB 的连续空间。</p>
<p>那么，它一共需要 2 个独立的、填满的 2 级索引表，也就意味着 64 个 1 级索引表，2 个独立的 3 级索引表，1 个 4 级索引表。一共需要 69 个索引表，每个 128 字节，大概就是 9KB 的空间。比起 4MB 来说，只有差不多 1&#x2F;500。</p>
<hr>
<p>虽然多级页表节约了我们的存储空间，却带来了时间上的开销，其实是一个“以时间换空间的”策略。原本我们进行一次地址转换，只需要访问一次内存就能找到物理页号，算出物理内存地址。但是，用了 4 级页表，我们就需要访问 4 次内存，才能找到物理页号了。</p>
<p>我们在前面两讲讲过，内存访问其实比 Cache 要慢很多。我们本来只是要做一个简单的地址转换，反而是一下子要多访问好多次内存。对于这个时间层面的性能损失，我们有没有什么更好的解决办法呢？</p>
<h3 id="总结延伸"><a href="#总结延伸" class="headerlink" title="总结延伸"></a>总结延伸</h3><p>我们从最简单的进行虚拟页号一一映射的简单页表说起，仔细讲解了现在实际应用的多级页表。多级页表就像是一颗树。因为一个进程的内存地址相对集中和连续，所以采用这种页表树的方式，可以大大节省页表所需要的空间。而因为每个进程都需要一个独立的页表，这个空间的节省是非常可观的。</p>
<p>在优化页表的过程中，我们可以观察到，数组这样的紧凑的数据结构，以及树这样稀疏的数据结构，在时间复杂度和空间复杂度的差异。另外，纯粹理论软件的数据结构和硬件的设计也是高度相关的。</p>
<p>注解：</p>
<p>哈希表有哈希冲突 并且顺序乱（无序） 不符合局部性原理 所以页表存储更复合计算机运行特点 64位系统的快表应该是对页表快速查询的一个优化</p>
<h2 id="下部分-解析TLB和内存保护"><a href="#下部分-解析TLB和内存保护" class="headerlink" title="下部分   解析TLB和内存保护"></a>下部分   解析TLB和内存保护</h2><hr>
<p>程序里面的每一个进程，都有一个属于自己的虚拟内存地址空间。我们可以通过地址转换来获得最终的实际物理地址。我们每一个指令都存放在内存里面，每一条数据都存放在内存里面。因此，<strong>“地址转换”</strong>是一个非常高频的动作，<strong>“地址转换”</strong>的性能就变的是非重要了。   今天要讲的第一个问题，也就是<strong>性能问题</strong>。</p>
<p>因为我们的指令、数据都存放在内存里面，这里就会遇到我们今天要谈的第二个问题，也就是<strong>内存安全问题</strong>。  如果被人修改了内存里面的内容，我们的 CPU 就可能会去执行我们计划之外的指令。这个指令可能是破坏我们服务器里面的数据，也可能是被人获取到服务器里面的敏感信息。</p>
<h3 id="加速地址转换-：-TLB"><a href="#加速地址转换-：-TLB" class="headerlink" title="加速地址转换 ： TLB"></a>加速地址转换 ： TLB</h3><p>上一节我们说了，从虚拟内存地址到物理内存地址的转换，我们通过页表这个数据结构来处理。为了节约页表的内存存储空间，我们会使用多级页表数据结构。</p>
<p>不过，多级页表虽然节约了我们的存储空间，但是却带来了时间上的开销，变成了一个“以时间换空间”的策略。原本我们进行一次地址转换，只需要访问一次内存就能找到物理页号，算出物理内存地址。但是用了 4 级页表，我们就需要访问 4 次内存，才能找到物理页号。</p>
<p>我们知道，内存访问其实比 Cache 要慢很多。我们本来只是要做一个简单的地址转换，现在反而要一下子多访问好多次内存。这种情况该怎么处理呢？你是否还记得之前讲过的“加个缓存”的办法呢？我们来试一试。</p>
<p>程序所需要使用的指令，都顺序存放在虚拟内存里面。我们执行的指令，也是一条条顺序执行下去的。也就是说，我们对于指令地址的访问，存在前面几讲所说的“空间局部性”和“时间局部性”，而需要访问的数据也是一样的。我们连续执行了5条指令。因为地址都是连续的， 所以这5个指令通常是在同一个 “虚拟页” 里。</p>
<p>因此，这连续 5 次的内存地址转换，其实都来自于同一个虚拟页号，转换的结果自然也就是同一个物理页号。可以用前面几讲说过的，用一个“加个缓存”的办法。把之前的内存转换地址缓存下来，使我们不需要反复去访问内存来进行内存地址转换。</p>
<img src="/img/computer_img/相邻的内存地址.webp" alt="相邻的内存地址" style="zoom:50%;" />

<p>于是，计算机工程师们专门在 CPU 里放了一块缓存芯片。这块缓存芯片我们称之为 TLB，全称是地址变换高速缓冲（Translation - Lookaside Buffer）。这块缓存存放了之前已经进行过地址转换的查询结果。这样，<strong>当同样的虚拟地址需要进行地址转换的时候， 我们可以直接在TLB 里面查询结果，而不需要多次访问内存来完成一次转换。</strong></p>
<p>TLB 和我们前面讲的 CPU 的高速缓存类似，可以分成指令的 TLB 和数据的 TLB，也就是 ITLB 和 DTLB。同样的，我们也可以根据大小对它进行分级，变成 L1、L2 这样多层的 TLB。</p>
<p>除此之外，还有一点和 CPU 里的高速缓存也是一样的，我们需要用脏标记这样的标记位，来实现“写回”这样缓存管理策略。</p>
<img src="/img/computer_img/TLB.webp" alt="TLB" style="zoom:50%;" />

<hr>
<p>为了性能，我们整个内存转换过程也要由硬件来执行。在 CPU 芯片里面，我们封装了内存管理单元（<strong>MMU</strong>，Memory Management Unit）芯片，用来完成地址转换。和 TLB 的访问和交互，都是由这个 MMU 控制的。</p>
<h3 id="内存保护和安全性"><a href="#内存保护和安全性" class="headerlink" title="内存保护和安全性"></a>内存保护和安全性</h3><p>进程的程序也好，数据也好，都要存放在内存里面。实际程序指令的执行，也是通过程序计数器里面的地址，去读取内存内的内容，然后运行对应的指令，使用相应的数据。</p>
<hr>
<p>虽然我们现代的操作系统和 CPU，已经做了各种权限的管控。正常情况下，我们已经通过虚拟内存地址和物理内存地址的区分，隔离了各个进程。但是，无论是 CPU 这样的硬件，还是操作系统这样的软件，都太复杂了，难免还是会被黑客们找到各种各样的漏洞。</p>
<p>在对于内存的管理里面，计算机也有一些最底层的安全保护机制。这些机制统称为  <strong>内存保护</strong> ，这里就为你简单介绍两个。</p>
<h4 id="可执行空间保护"><a href="#可执行空间保护" class="headerlink" title="可执行空间保护"></a>可执行空间保护</h4><p>这个机制是说，我们对于一个进程使用的内存，只把其中的指令部分设置成“可执行”的，对于其他部分，比如数据部分，不给予“可执行”的权限。因为无论是指令，还是数据，在我们的 CPU 看来，都是二进制的数据。 我们 直接把数据部分拿给 CPU ，如果这些数据解码后，也能变成一条合理的指令，其实就是可执行的。</p>
<p>这个时候，黑客们想到了一些搞破坏的办法。我们在程序的数据区里，放入一些要执行的指令编码后的数据，然后找到一个办法，让 CPU 去把它们当成指令去加载，那 CPU 就能执行我们想要执行的指令了。对于进程里内存空间的执行权限进行控制，可以使得 CPU 只能执行指令区域的代码。对于数据区域的内容，即使找到了其他漏洞想要加载成指令来执行，也会因为没有权限而被阻挡掉。  </p>
<p>其实，在实际的应用开发中，类似的策略也很常见。我下面给你举两个例子。</p>
<p>比如说，在用 PHP 进行 Web 开发的时候，我们通常会禁止 PHP 有 eval 函数的执行权限。这个其实就是害怕外部的用户，所以没有把数据提交到服务器，而是把一段想要执行的脚本提交到服务器。服务器里在拼装字符串执行命令的时候，可能就会执行到预计之外被“注入”的破坏性脚本。 </p>
<p>还有一个例子就是 SQL 注入攻击。如果服务端执行的 SQL 脚本是通过字符串拼装出来的，那么在 Web 请求里面传输的参数就可以藏下一些我们想要执行的 SQL，让服务器执行一些我们没有想到过的 SQL 语句。这样的结果就是，或者破坏了数据库里的数据，或者被人拖库泄露了数据。</p>
<hr>
<h4 id="地址空间布局随机化"><a href="#地址空间布局随机化" class="headerlink" title="地址空间布局随机化"></a>地址空间布局随机化</h4><p>第二个常见的安全机制，叫地址空间布局随机化（Address Space Layout Randomization）。</p>
<p>内存层面的安全保护核心策略，是在可能有漏洞的情况下进行安全预防。上面的可执行空间保护就是一个很好的例子。但是，内存层面的漏洞还有其他的可能性。</p>
<p>这里的核心问题是，其他的人、进程、程序，会去修改掉特定进程的指令、数据，然后，让当前进程去执行这些指令和数据，造成破坏。要想修改这些指令和数据，我们需要知道这些指令和数据所在的位置才行。</p>
<p>原先我们一个进程的内存布局空间是固定的，所以任何第三方很容易就能知道指令在哪里，程序栈在哪里，数据在哪里，堆又在哪里。这个其实为想要搞破坏的人创造了很大的便利。而地址空间布局随机化这个机制，就是让这些区域的位置不再固定，在内存空间随机去分配这些进程里不同部分所在的内存空间地址，让破坏者猜不出来。猜不出来呢，自然就没法找到想要修改的内容的位置。如果只是随便做点修改，程序只会 crash 掉，而不会去执行计划之外的代码。</p>
<img src="/img/computer_img/地址空间随机化.webp" alt="地址空间随机化" style="zoom:33%;" />



<p>这样的“随机化”策略，其实也是我们日常应用开发中一个常见的策略。一个大家都应该接触过的例子就是密码登陆功能。网站和 App 都会需要你设置用户名和密码，之后用来登陆自己的账号。然后，在服务器端，我们会把用户名和密码保存下来，在下一次用户登陆的时候，使用这个用户名和密码验证。</p>
<p>我们的密码当然不能明文存储在数据库里，不然就会有安全问题。如果明文存储在数据库里，意味着能拿到数据库访问权限的人，都能看到用户的明文密码。这个可能是因为安全漏洞导致被人拖库，而且网站的管理员也能直接看到所有的用户名和密码信息。</p>
<p>于是，大家会在数据库里存储密码的哈希值，比如用现在常用的 SHA256，生成一一个验证的密码哈希值。但是这个往往还是不够的。因为同样的密码，对应的哈希值都是相同的，大部分用户的密码又常常比较简单。于是，拖库成功的黑客可以通过彩虹表的方式，来推测出用户的密码。</p>
<p>这个时候，我们的“随机化策略”就可以用上了。我们可以在数据库里，给每一个用户名生成一个随机的、使用了各种特殊字符的盐值（salt）.  <em>这样，我们的哈希值就不再是仅仅使用密码来生成的了，而是密码和盐值放在一起生成的对应的哈希值。</em> 哈希值的生成中，包括了一些类似于“乱码”的随机字符串，所以通过彩虹表碰撞来猜出密码的办法就用不了了。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">例子：</span><br><span class="line"></span><br><span class="line">$password = &quot;goodmorning12345&quot;;</span><br><span class="line">// 我们的密码是明文存储的</span><br><span class="line"></span><br><span class="line">$hashed_password = hash(&#x27;sha256&#x27;, password);</span><br><span class="line">// 对应的hash值是 054df97ac847f831f81b439415b2bad05694d16822635999880d7561ee1b77ac</span><br><span class="line">// 但是这个hash值里可以用彩虹表直接“猜出来”原始的密码就是goodmorning12345</span><br><span class="line"></span><br><span class="line">$salt = &quot;#21Pb$Hs&amp;Xi923^)?&quot;;</span><br><span class="line">$salt_password = $salt.$password;</span><br><span class="line">$hashed_salt_password = hash(&#x27;sha256&#x27;, salt_password);</span><br><span class="line">// 这个hash后的slat因为有部分随机的字符串，不会在彩虹表里面出现。</span><br><span class="line">// 261e42d94063b884701149e46eeb42c489c6a6b3d95312e25eee0d008706035f</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>可以看到，通过加入“随机”因素，我们有了一道最后防线。即使在出现安全漏洞的时候，我们也有了更多的时间和机会去补救这些问题。</p>
<p>虽然安全机制似乎在平时用不太到，但是在开发程序的时候，还是要有安全意识。毕竟谁也不想看到，被拖库的新闻里出现的是自己公司的名字，也不希望用户因为我们的错误遭受到损失。</p>
<h3 id="总结延伸-1"><a href="#总结延伸-1" class="headerlink" title="总结延伸"></a>总结延伸</h3><p>为了节约页表所需要的内存空间，我们采用了多级页表这样一个数据结构。但是，多级页表虽然节省空间了，却要花费更多的时间去多次访问内存。于是，我们在实际进行地址转换的 MMU 旁边放上了 TLB 这个用于地址转换的缓存。TLB 也像 CPU Cache 一样，分成指令和数据部分，也可以进行 L1、L2 这样的分层。</p>
<p>然后，我为你介绍了内存保护。无论是数据还是代码，我们都要存放在内存里面。为了防止因为各种漏洞，导致一个进程可以访问别的进程的数据或者代码，甚至是执行对应的代码，造成严重的安全问题，我们介绍了最常用的两个内存保护措施，可执行空间保护和地址空间布局随机化。</p>
<p>通过让数据空间里面的内容不能执行，可以避免了类似于“注入攻击”的攻击方式。<strong>通过随机化内存空间的分配</strong>，可以避免让一个进程的内存里面的代码，被推测出来，从而不容易被攻击。</p>
]]></content>
      <categories>
        <category>计算机组成原理</category>
      </categories>
      <tags>
        <tag>虚拟内存</tag>
        <tag>页表</tag>
        <tag>TLB</tag>
        <tag>内存安全</tag>
      </tags>
  </entry>
  <entry>
    <title>总线-计算机内部的高速公路</title>
    <url>/posts/d6a63846/</url>
    <content><![CDATA[<h1 id="总线：计算机内部的高速公路"><a href="#总线：计算机内部的高速公路" class="headerlink" title="总线：计算机内部的高速公路"></a>总线：计算机内部的高速公路</h1><p>CPU所代表的控制器和运算器，要和存储器，也就是我们的主内存，以及输入和输出设备进行通信。 那问题来了，CP U从我们的键盘、鼠标接收输入信号，向显示器输出信号，这之间究竟是怎么通信的呢<span id="more"></span>？换句话说，计算机是用什么样的方式来完成，CPU 和内存、以及外部输入输出设备的通信呢？</p>
<hr>
<h2 id="降低复杂性：总线的设计思路来源"><a href="#降低复杂性：总线的设计思路来源" class="headerlink" title="降低复杂性：总线的设计思路来源"></a>降低复杂性：总线的设计思路来源</h2><p>计算机里其实有很多不同的硬件设备，除了 CPU 和内存之外，我们还有大量的输入输出设备。可以说，你计算机上的每一个接口，键盘、鼠标、显示器、硬盘，乃至通过 USB 接口连接的各种外部设备，都对应了一个设备或者模块。</p>
<p>如果各个设备间的通信，都是互相之间单独进行的。如果我们有 N 个不同的设备，他们之间需要各自单独连接，那么系统复杂度就会变成 N2。每一个设备或者功能电路模块，都要和其他 N−1 个设备去通信。<u>为了简化系统的复杂度，我们就引入了总线，把这个 N2 的复杂度，变成一个 N 的复杂度。</u></p>
<p>与其让各个设备之间互相单独通信，不如我们去设计一个公用的线路。</p>
<blockquote>
<p>CPU 想要和什么设备通信，通信的指令是什么，对应的数据是什么，都发送到这个线路上；设备要向 CPU 发送什么信息呢，也发送到这个线路上。这个线路就好像一个高速公路，各个设备和其他设备之间，不需要单独建公路，只建一条小路通向这条高速公路就好了。</p>
</blockquote>
<img src="/img/computer_img/总线降低复杂度.webp" alt="总线降低复杂度" style="zoom:50%;" />

<p>这个设计思路，就是我们今天要说的总线（Bus）。</p>
<p>总线，其实就是一组线路。我们的 CPU、内存以及输入和输出设备，都是通过这组线路，进行相互间通信的。总线的英文叫作 <strong>Bus</strong>，就是一辆公交车。这个名字很好地描述了总线的含义。我们的“公交车”的各个站点，就是各个接入设备。要想向一个设备传输数据，我们只要把数据放上公交车，在对应的车站下车就可以了。</p>
<p>其实，对应的设计思路，在软件开发中也是非常常见的。我们在做大型系统开发的过程中，经常会用到一种叫作<a href="https://dzone.com/articles/design-patterns-event-bus"><strong>事件总线</strong></a>（Event Bus）的设计模式。</p>
<p>进行大规模应用系统开发的时候，系统中的各个组件之间也需要相互通信。模块之间如果是两两之间单独去定义协议，这个软件系统一样会遇到一个复杂度变成了 N2 的问题。所以常见的一个解决方案，就是事件总线这个设计模式。</p>
<p>在事件总线这个设计模式里，各个模块触发对应的事件，并把事件对象发送到总线上。 也就是说，每个模块就是一个发布者。而每个模块也会把自己注册到总线上，去监听总线上的事件，并根据事件的对象类型或者是对象内容，来决定自己是否要进行特定的处理或者响应。<br><img src="/img/computer_img/事件总线.webp" alt="事件总线" style="zoom:50%;" /></p>
<p>这样的设计下，注册在总线上的各个模块就是松耦合的。模块互相之间并没有依赖关系。无论代码的维护，还是未来的扩展，都会很方便。</p>
<p><strong>发布者、订阅者</strong></p>
<h2 id="理解总线：-三种线路和多总线架构"><a href="#理解总线：-三种线路和多总线架构" class="headerlink" title="理解总线： 三种线路和多总线架构"></a>理解总线： 三种线路和多总线架构</h2><p>现代的 Intel CPU 的体系结构里面，通常有好几条总线。</p>
<p>首先，CPU 和内存以及高速缓存通信的总线，这里面通常有两种总线。</p>
<p>这种方式，我们称之为<strong>双独立总线</strong> （Dual Independent Bus，缩写为 DIB）。CPU 里，有一个快速的<strong>本地总线（</strong>Local Bus），以及一个速度相对较慢的<strong>前端总线</strong>（Front-side Bus）。</p>
<p>在前面几讲刚刚讲过，现代的 CPU 里，通常有专门的高速缓存芯片。这里的高速本地总线，就是用来和高速缓存通信的。而前端总线，则是用来和主内存以及输入输出设备通信的。有时候，我们会把本地总线也叫作后端总线（Back-side Bus），和前面的前端总线对应起来。而前端总线也有很多其他名字，比如处理器总线（Processor Bus）、内存总线（Memory Bus）。</p>
<p>除了前端总线呢，我们常常还会听到 PCI 总线、I&#x2F;O 总线或者系统总线（System Bus）。看到这么多总线的名字，你是不是已经有点晕了。这些名词确实容易混为一谈。其实各种总线的命名一直都很混乱，我们不如直接来看一看 <strong>CPU 的硬件架构图</strong>。对照图来看，一切问题就都清楚了。</p>
<p>CPU 里面的北桥芯片，把我们上面说的前端总线，一分为二，变成了三个总线。</p>
<img src="/img/computer_img/双独立总线.webp" alt="双独立总线" style="zoom:50%;" />

<p>我们的前端总线，其实就是 <strong>系统总线</strong> 。CPU 里面的内存接口，直接和系统总线通信，然后系统总线再接入一个 I&#x2F;O 桥接器（I&#x2F;O Bridge）。这个 I&#x2F;O 桥接器，一边接入了我们的内存总线，使得我们的 CPU 和内存通信；另一边呢，又接入了一个 I&#x2F;O 总线，用来连接 I&#x2F;O 设备。</p>
<hr>
<p>事实上，真实的计算机里，这个总线层面拆分得更细。根据不同的设备，还会分成独立的 PCI 总线、ISA 总线等等。</p>
<img src="/img/computer_img/总线细分.webp" alt="总线细分" style="zoom:50%;" />



<p>在物理层面，其实我们完全可以把总线看作一组“电线”。不过呢，这些电线之间也是有分工的，我们通常有三类线路。</p>
<ol>
<li><p>数据线（Data Bus），用来传输实际的数据信息，也就是实际上了公交车的“人”。</p>
</li>
<li><p>地址线（Address Bus），用来确定到底把数据传输到哪里去，是内存的某个位置，还是某一个 I&#x2F;O 设备。这个其实就相当于拿了个纸条，写下了上面的人要下车的站点。</p>
</li>
<li><p>控制线（Control Bus），用来控制对于总线的访问。虽然我们把总线比喻成了一辆公交车。那么有人想要做公交车的时候，需要告诉公交车司机，这个就是我们的控制信号。</p>
</li>
</ol>
<ul>
<li>尽管总线减少客设备之间的耦合，降低了系统设计的复杂度，但同时带来了一个问题，就是总线不能同时给多个设备提供通信功能。</li>
</ul>
<p>我们的总线是很多个设备公用的，那多个设备都想要用总线，我们就需要有一个机制，去决定这种情况下，到底把总线给哪一个设备用。这个机制，就叫作总线裁决（Bus Arbitraction）。总线裁决的机制有很多种不同的实现，如果你对这个实现的细节感兴趣，可以去看一看 <a href="https://en.wikipedia.org/wiki/Arbiter_(electronics)">Wiki</a> 里面关于裁决器的对应条目，这里我们就不多说了。</p>
<h2 id="总线延伸"><a href="#总线延伸" class="headerlink" title="总线延伸"></a>总线延伸</h2><p>讲解了计算机里各个不同的组件之间用来通信的渠道，也就是总线。总线的设计思路，核心是为了减少多个模块之间交互的复杂性和耦合度。实际上，总线这个设计思路在我们的软件开发过程中也经常会被用到。事件总线就是我们常见的一个设计模式，通常事件总线也会和订阅者发布者模式结合起来，成为大型系统的各个松耦合的模块之间交互的一种主要模式。</p>
<p>在实际的硬件层面，总线其实就是一组连接电路的线路。因为不同设备之间的速度有差异，所以一台计算机里往往会有多个总线。  常见的就有在 CPU 内部和高速缓存通信的本地总线，以及和外部 I&#x2F;O 设备以及内存通信的前端总线。</p>
<p>前端总线通常也被叫作<strong>系统总线</strong>。它可以通过一个 I&#x2F;O 桥接器，拆分成两个总线，分别来和 I&#x2F;O 设备以及内存通信。自然，这样拆开的两个总线，就叫作 I&#x2F;O 总线和内存总线<u>。总线本身的电路功能，又可以拆分成用来传输数据的数据线、用来传输地址的地址线，以及用来传输控制信号的控制线。</u></p>
<hr>
<p>总线是一个各个接入的设备公用的线路，所以自然会在各个设备之间争夺总线所有权的情况。于是，我们需要一个机制来决定让谁来使用总线，这个决策机制就是总线裁决。</p>
]]></content>
      <categories>
        <category>计算机组成原理</category>
      </categories>
  </entry>
  <entry>
    <title>输入输出设备</title>
    <url>/posts/44e65038/</url>
    <content><![CDATA[<h1 id="输入输出设备：我们并不是只能用灯泡显示“0”和“1”"><a href="#输入输出设备：我们并不是只能用灯泡显示“0”和“1”" class="headerlink" title="输入输出设备：我们并不是只能用灯泡显示“0”和“1”"></a>输入输出设备：我们并不是只能用灯泡显示“0”和“1”</h1><p>计算机的输入设备就是一个一个开关，输出设备呢，是一个一个灯泡。 早期发展的时候，计算机的核心是做“计算”<span id="more"></span>。我们从“计算机”这个名字上也能看出这一点。不管是中文名字“计算机”，还是英文名字“Computer”，核心都是在”计算“这两个字上。不过，到了今天，这些“计算”的工作，更多的是一个幕后工作。</p>
<hr>
<p>我们无论是使用自己的 PC，还是智能手机，大部分时间都是在和计算机进行各种“交互操作”。换句话说，就是在和输入输出设备打交道。这些输入输出设备也不再是一个一个开关，或者一个一个灯泡。</p>
<p>你在键盘上直接敲击的都是字符，而不是“0”和“1”，你在显示器上看到的，也是直接的图形或者文字的画面，而不是一个一个闪亮或者关闭的灯泡。想要了解这其中的关窍，那就请你和我一起来看一看，计算机里面的输入输出设备。</p>
<h2 id="接口和设备：经典的适配器模式"><a href="#接口和设备：经典的适配器模式" class="headerlink" title="接口和设备：经典的适配器模式"></a>接口和设备：经典的适配器模式</h2><ul>
<li>像蓝牙、WiFi 无线网卡这样的设备也是输入输出设备吗？我们的输入输出设备的寄存器在哪里？到底是在主板上，还是在硬件设备上？</li>
</ul>
<p>实际上，输入输出设备，并不只是一个设备。大部分的输入输出设备，都有两个组成部分。      第一个是它的<strong>接口</strong>（Interface），第二个才是<strong>实际的 I&#x2F;O 设备（</strong>Actual I&#x2F;O Device）。  <em>我们的硬件设备并不是直接接入到总线上和 CPU 通信的，而是通过接口，用接口连接到总线上，再通过总线和 CPU 通信。</em></p>
<p>你平时听说的并行接口（Parallel Interface）、串行接口（Serial Interface）、USB 接口，都是计算机主板上内置的各个接口。我们的实际硬件设备，比如，使用并口的打印机、使用串口的老式鼠标或者使用 USB 接口的 U 盘，都要插入到这些接口上，才能和 CPU 工作以及通信的。</p>
<hr>
<p>接口本身就是一块电路板。CPU 其实不是和实际的硬件设备打交道，而是和这个接口电路板打交道。我们平时说的，设备里有三类寄存器，其实都是在这个设备的接口电路上，而不是在实际的设备上。</p>
<p>这三类寄存器，分别是状态寄存器（Status Register）、 命令寄存器 （Command Register）、以及数据寄存器（Data Register）。</p>
<p>除了内置在主板上的接口之外，<em>有些接口可以集成在设备上</em>。你可能都没有见过老一点儿的硬盘，我来简单给你介绍一下。</p>
<hr>
<p>上世纪 90 年代的时候，大家用的硬盘都叫作 IDE 硬盘。这个 IDE 不是像 IntelliJ 或者 WebStorm 这样的软件开发集成环境（Integrated Development Environment）的 IDE，而是代表着集成设备电路（Integrated Device Electronics）。也就是说，设备的接口电路直接在设备上，而不在主板上。我们需要通过一个线缆，把集成了接口的设备连接到主板上去。</p>
<p>把接口和实际设备分离，这个做法实际上来自于计算机走向<a href=""><strong>开放架构（Open Architecture）</strong></a>的时代。</p>
<p>当我们要对计算机升级，我们不会扔掉旧的计算机，直接买一台全新的计算机，而是可以单独升级硬盘这样的设备。我们把老硬盘从接口上拿下来，换一个新的上去就好了。   <strong>各种输入输出设备的制造商，也可以根据接口的控制协议，来设计和制造硬盘、鼠标、键盘、打印机乃至其他种种外设。正是这样的分工协作，带来了 PC 时代的繁荣。</strong></p>
<hr>
<ul>
<li>其实，在软件的设计模式里也有这样的思路。面向对象里的面向接口编程的接口，就是  Interface。如果你做 iOS 的开发，Objective-C 里面的 Protocol 其实也是这个意思。而 <em><strong>Adaptor 设计模式</strong></em>，更是一个常见的、用来解决不同外部应用和系统“适配”问题的方案。可以看到，计算机的软件和硬件，在逻辑抽象上，其实是相通的。</li>
</ul>
<p>如果你用的是 Windows 操作系统，你可以打开设备管理器，里面有各种各种的 Devices（设备）、Controllers（控制器）、Adaptors（适配器）。这些，其实都是对于输入输出设备不同角度的描述。<br><img src="/img/computer_img/设备接口.webp" alt="设备接口" style="zoom: 67%;" /></p>
<p>叫作 Devices，看重的是实际的 I&#x2F;O 设备本身。被叫作 Controllers，看重的是输入输出设备接口里面的控制电路。而被叫作 Adaptors，则是看重接口作为一个适配器后面可以插上不同的实际设备。</p>
<h2 id="CPU-是如何控制-I-x2F-O-设备的？"><a href="#CPU-是如何控制-I-x2F-O-设备的？" class="headerlink" title="CPU 是如何控制 I&#x2F;O 设备的？"></a>CPU 是如何控制 I&#x2F;O 设备的？</h2><p>无论是内置在主板上的接口，还是集成在设备上的接口，除了三类寄存器之外，<strong>还有对应的控制电路</strong>。正是通过这个控制电路，CPU 才能通过向这个接口电路板传输信号，来控制实际的硬件。</p>
<p>我们先来看一看，硬件设备上的这些寄存器有什么用。这里，我拿我们平时用的打印机作为例子。</p>
<img src="/img/computer_img/IO 设备实例.webp" alt="IO 设备实例" style="zoom:50%;" />

<ol>
<li>首先是数据寄存器（Data Register）。CPU 向 I&#x2F;O 设备写入需要传输的数据，比如要打印的内容是“GeekTime”，我们就要先发送一个“G”给到对应的 I&#x2F;O 设备。</li>
<li>然后是命令寄存器（Command Register）。CPU 发送一个命令，告诉打印机，要进行打印工作。这个时候，打印机里面的控制电路会做两个动作。第一个，是去设置我们的状态寄存器里面的状态，把状态设置成 not-ready。第二个，就是实际操作打印机进行打印。</li>
<li>而状态寄存器（Status Register），就是告诉了我们的 CPU，现在设备已经在工作了，所以这个时候，CPU 你再发送数据或者命令过来，都是没有用的。直到前面的动作已经完成，状态寄存器重新变成了 ready 状态，我们的 CPU 才能发送下一个字符和命令。</li>
</ol>
<p>当然，在实际情况中，打印机里通常不只有数据寄存器，还会有数据缓冲区。我们的 CPU 也不是真的一个字符一个字符这样交给打印机去打印的，而是一次性把整个文档传输到打印机的内存或者数据缓冲区里面一起打印的。不过，通过上面这个例子，相信你对 CPU 是怎么操作 I&#x2F;O 设备的，应该有所了解了。</p>
<hr>
<h2 id="信号和地址：发挥总线的价值"><a href="#信号和地址：发挥总线的价值" class="headerlink" title="信号和地址：发挥总线的价值"></a>信号和地址：发挥总线的价值</h2><ul>
<li>搞清楚了实际的 I&#x2F;O 设备和接口之间的关系，一个新的问题就来了。那就是，我们的 CPU 到底要往总线上发送一个什么样的命令，才能和 I&#x2F;O 接口上的设备通信呢？</li>
</ul>
<p>CPU 和 IO设备的通信，一样是通过CPU 支持的机器指令来执行的。</p>
<p>回头去看一看， MIPS 的机器指令的分类，你会发现，我们并没有一种专门的和 I&#x2F;O 设备通信的指令类型。那么，MIPS 的 CPU 到底是通过什么样的指令来和 I&#x2F;O 设备来通信呢？</p>
<p>答案就是：</p>
<p>和访问我们的主内存一样，使用“内存地址”。为了让已经足够复杂的 CPU 尽可能简单，<em><strong>计算机会把 I&#x2F;O 设备的各个寄存器，以及I&#x2F;O设备内部的内存地址，都映射到主内存地址空间里来。</strong></em> 主内存的地址空间里，会给不同的 I&#x2F;O 设备预留一段一段的内存地址。 </p>
<p>CPU 想要和这些 I&#x2F;O 设备通信的时候呢，就往这些地址发送数据。这些地址信息，就是通过上一讲的地址线来发送的，  而对应的数据信息呢，自然就是通过数据线来发送的了。</p>
<p>而我们的 I&#x2F;O 设备呢，就会<strong>监控地址线</strong>，并且在 CPU 往自己地址发送数据的时候， 把对应的数据线里面传输过来的数据，接入到对应的设备里面的寄存器 和内存里。     CPU 无论是向 I&#x2F;O 设备发送命令、查询状态还是传输数据，都可以通过这样的方式。这种方式呢，叫作 <strong>内存映射IO（</strong>Memory-Mapped I&#x2F;O，简称 MMIO）。</p>
<img src="/img/computer_img/内存映射IO.webp" alt="内存映射IO" style="zoom:50%;" />

<hr>
<p>那么，MMIO 是不是唯一一种 CPU 和设备通信的方式呢？答案是<strong>否定的</strong>。精简指令集 MIPS 的 CPU 特别简单，所以这里只有 MMIO。            而我们有 2000 多个指令的 Intel X86 架构的计算机，自然可以设计专门的和 I&#x2F;O 设备通信的指令，也就是 in 和 out 指令。</p>
<p>Intel CPU 虽然也支持 MMIO，不过它还可以通过特定的指令，来支持<strong>端口映射 I&#x2F;O</strong>（Port-Mapped I&#x2F;O，简称 <strong>PMIO</strong>）或者也可以叫独立输入输出（Isolated I&#x2F;O）。</p>
<p>其实 PMIO 的通信方式和 MMIO 差不多，核心的区别在于，<em>PMIO 里面访问的设备地址，不再是在内存地址空间里面，而是一个<strong>专门的端口</strong>（Port）。</em>这个端口并不是指一个硬件上的插口，而是和 CPU 通信的一个抽象概念。</p>
<p>无论是 PMIO 还是 MMIO , CPU 都会传送一条二进制的数据，给到IO 设备的对应地址。设备自己本身的接口电路，会去解码这个数据，解码之后的数据 ，会变成设备支持的一条指令 ，再去通过控制电路去操作实际的硬件设备。     <strong>对于 CPU 来说，它并不需要关心设备本身能够支持哪些操作。他要做的，只要在总线上传输一条条数据就好了。</strong></p>
<p>这个，其实也有点像我们在设计模式里面的 Command 模式。我们在<em>总线上传输的，是一个个数据对象，然后各个接受这些对象的设备，再去根据对象内容，进行实际的解码和命令执行。</em></p>
<img src="/img/computer_img/command.webp" alt="command" style="zoom:50%;" />
这是一张我自己的显卡，在设备管理器里面的资源（Resource）信息。你可以看到，里面既有 Memory Range，这个就是设备对应映射到的内存地址，也就是我们上面所说的 MMIO 的访问方式。同样的，里面还有 I/O Range，这个就是我们上面所说的 PMIO，也就是通过端口来访问 I/O 设备的地址。最后，里面还有一个 IRQ，也就是会来自于这个设备的中断信号了。



<h2 id="总结延伸"><a href="#总结延伸" class="headerlink" title="总结延伸"></a>总结延伸</h2><p>讲到这里，不知道，现在你是不是可以把 CPU 的指令、总线和 I&#x2F;O 设备之间的关系彻底串联起来了呢？</p>
<p>CPU 并不是发送一个特定的操作指令来操作不同的 I&#x2F;O 设备。因为如果是那样的话，随着新的 I&#x2F;O 设备的发明，我们就要去扩展 CPU 的指令集了。</p>
<p>在计算机系统里面，CPU 和 I&#x2F;O 设备之间的通信，是这么来解决的。</p>
<p>首先，在 I&#x2F;O 设备这一侧，我们把 I&#x2F;O 设备拆分成，能和 CPU 通信的接口电路，以及实际的 I&#x2F;O 设备本身。  接口电路里面有对应的状态寄存器、命令寄存器、数据寄存器、数据缓冲区和设备内存等等。  <strong>接口电路通过总线与CPU 通信，接受来自 CPU 的指令和数据。而接口电路中的控制电路， 再解码收到的指令，实际去操作对应的 硬件设备</strong>。</p>
]]></content>
      <categories>
        <category>计算机组成原理</category>
      </categories>
      <tags>
        <tag>存储器</tag>
        <tag>接口</tag>
      </tags>
  </entry>
  <entry>
    <title>理解IO_WAIT - I/O性能</title>
    <url>/posts/99a5f054/</url>
    <content><![CDATA[<h1 id="理解IO-WAIT-I-x2F-O性能到底是怎么回事儿？"><a href="#理解IO-WAIT-I-x2F-O性能到底是怎么回事儿？" class="headerlink" title="理解IO_WAIT : I&#x2F;O性能到底是怎么回事儿？"></a>理解IO_WAIT : I&#x2F;O性能到底是怎么回事儿？</h1><p>大部分程序员开发的都是应用系统。在开发应用系统的时候，我们遇到的性能瓶颈大部分都在 I&#x2F;O 上<span id="more"></span>。在第 36 讲讲解局部性原理的时候，我们一起看了通过把内存当作是缓存，来提升系统的整体性能。在第 37 讲讲解 CPU Cache 的时候，我们一起看了 CPU Cache 和主内存之间性能的巨大差异。</p>
<p>然而，我们知道，并不是所有问题都能靠利用内存或者 CPU Cache 做一层缓存来解决。特别是在这个“大数据”的时代。我们在硬盘上存储了越来越多的数据，一个 MySQL 数据库的单表有个几千万条记录，早已经不算是什么罕见现象了。这也就意味着，用内存当缓存，存储空间是不够用的。大部分时间，我们的请求还是要打到硬盘上。那么，这一讲我们就来看看硬盘 I&#x2F;O 性能的事儿。</p>
<hr>
<h2 id="IO性能、顺序访问和随机访问"><a href="#IO性能、顺序访问和随机访问" class="headerlink" title="IO性能、顺序访问和随机访问"></a>IO性能、顺序访问和随机访问</h2><p>如果去看硬盘厂商的性能报告，通常会看到两个指标，一个是响应时间（Response Time）,另一个是数据传输率（Data Transfer Rate）。</p>
<p>我们先来看一看后面这个指标，数据传输率</p>
<p>我们现在常用的硬盘有两种。一种是 HDD 硬盘，也就是我们常说的机械硬盘。另一种是 SSD 硬盘，一般也被叫作固态硬盘。现在的 HDD 硬盘，用的是 SATA 3.0 的接口。而 SSD 硬盘呢，通常会用两种接口，一部分用的也是 SATA 3.0 的接口；另一部分呢，用的是 PCI Express 的接口。</p>
<p>现在我们常用的 SATA 3.0 的接口，带宽是 6Gb&#x2F;s。这里的“b”是比特。这个带宽相当于每秒可以传输 768MB 的数据。而我们日常用的 HDD 硬盘的数据传输率，差不多在 200MB&#x2F;s 左右。</p>
<p>当我们换成 SSD 的硬盘，性能自然会好上不少。比如，我最近刚把自己电脑的 HDD 硬盘，换成了一块 Crucial MX500 的 SSD 硬盘。它的数据传输速率能到差不多 500MB&#x2F;s，比 HDD 的硬盘快了一倍不止。不过 SATA 接口的硬盘，差不多到这个速度，性能也就到顶了。因为 SATA 接口的速度也就这么快。</p>
<p>不过，实际 SSD 硬盘能够更快，所以我们可以换用 PCI Express 的接口。我自己电脑的系统盘就是一块使用了 PCI Express 的三星 SSD 硬盘。它的数据传输率，在读取的时候就能做到 2GB&#x2F;s 左右，差不多是 HDD 硬盘的 10 倍，而在写入的时候也能有 1.2GB&#x2F;s。</p>
<p>除了数据传输率这个吞吐率指标，另一个我们关心的指标响应时间，其实也可以在 AS SSD 的测试结果里面看到，就是这里面的 Acc.Time 指标。</p>
<p>这个指标，其实就是程序发起一个硬盘的写入请求，直到这个请求返回的时间。可以看到，在上面的两块 SSD 硬盘上，大概时间都是在几十微秒这个级别。如果你去测试一块 HDD 的硬盘，通常会在几毫秒到十几毫秒这个级别。这个性能的差异，就不是 10 倍了，而是在几十倍，乃至几百倍。</p>
<p>光看响应时间和吞吐率这两个指标，似乎我们的硬盘性能很不错。即使是廉价的 HDD 硬盘，接收一个来自 CPU 的请求，也能够在几毫秒时间返回。一秒钟能够传输的数据，也有 200MB 左右。你想一想，我们平时往数据库里写入一条记录，也就是 1KB 左右的大小。我们拿 200MB 去除以 1KB，那差不多每秒钟可以插入 20 万条数据呢。但是这个计算出来的数字，似乎和我们日常的经验不符合啊？这又是为什么呢？</p>
<hr>
<p>答案就来自于硬盘的读写。在<strong>顺序读写</strong>和<strong>随机读写</strong>的情况下，硬盘的性能是完全不同的。</p>
<p>我们回头看一下上面的 AS SSD 的性能指标。你会看到，里面有一个“4K”的指标。这个指标是什么意思呢？它其实就是我们的程序，去随机读取磁盘上某一个 4KB 大小的数据，一秒之内可以读取到多少数据。</p>
<p>你会发现，在这个指标上，我们使用 SATA 3.0 接口的硬盘和 PCI Express 接口的硬盘，性能差异变得很小。这是因为，在这个时候，接口本身的速度已经不是我们硬盘访问速度的瓶颈了。更重要的是，你会发现，即使我们用 PCI Express 的接口，在随机读写的时候，数据传输率也只能到 40MB&#x2F;s 左右，是顺序读写情况下的几十分之一。</p>
<p>我们拿这个 40MB&#x2F;s 和一次读取 4KB 的数据算一下。</p>
<p>​                    40MB &#x2F; 4KB &#x3D; 10,000</p>
<p>也就是说，一秒之内，这块 SSD 硬盘可以随机读取 1 万次的 4KB 的数据。如果是写入的话呢，会更多一些，90MB &#x2F;4KB 差不多是 2 万多次。</p>
<p>这个每秒读写的次数，我们称之为  <em><strong>IOPS</strong></em>，也就是每秒输入输出操作的次数。事实上，比起响应时间，我们更关注 IOPS 这个性能指标。IOPS 和 DTR（Data Transfer Rate，数据传输率）才是输入输出性能的核心指标。</p>
<p>我们在实际的应用开发当中，<em>对于数据的访问，更多的是随机读写，而不是顺序读写。</em>我们<strong>平时所说的服务器承受的“并发”，其实是在说，会有很多个不同的进程和请求来访问服务器</strong>。自然，它们在硬盘上访问的数据，是很难顺序放在一起的。这种情况下，随机读写的 IOPS 才是服务器性能的核心指标。</p>
<p>我们引出 IOPS 这个问题的 HDD 硬盘。我现在要问你了，那一块 HDD 硬盘能够承受的 IOPS 是多少呢？</p>
<p>HDD 硬盘的 IOPS 通常也就在 100 左右，而不是在 20 万次。在后面讲解机械硬盘的原理和性能优化的时候，我们还会再来一起看一看，这个 100 是怎么来的，以及我们可以有哪些优化的手段。</p>
<hr>
<h2 id="如何定位-IO-WAIT"><a href="#如何定位-IO-WAIT" class="headerlink" title="如何定位 IO_WAIT"></a>如何定位 IO_WAIT</h2><p>我们看到，即使是用上了 PCI Express 接口的 SSD 硬盘，IOPS 也就是在 2 万左右。而我们的 CPU 的主频通常在 2GHz 以上，也就是每秒可以做 20 亿次操作。</p>
<p>即使 CPU 向硬盘发起一条读写指令，需要很多个时钟周期，一秒钟 CPU 能够执行的指令数，和我们硬盘能够进行的操作数，也有好几个数量级的差异。这也是为什么，我们在应用开发的时候往往会说“性能瓶颈在 I&#x2F;O 上”。   因为很多时候，CPU 指令发出去之后，不得不去“等”我们的 I&#x2F;O 操作完成，才能进行下一步的操作 .</p>
<p>那么，在实际遇到服务端程序的性能问题的时候，我们怎么知道这个问题是不是来自于 CPU 等 I&#x2F;O 来完成操作呢？别着急，我们接下来，就通过 top 和 iostat 这些命令，一起来看看 CPU 到底有没有在等待 io 操作。</p>
<hr>
<p>你一定在 Linux 下用过 top 命令。对于很多刚刚入门 Linux 的同学，会用 top 去看服务的负载，也就是 load average。不过，在 top 命令里面，我们一样可以看到 CPU 是否在等待 IO 操作完成。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">top - 06:26:30 up 4 days, 53 min,  1 user,  load average: 0.79, 0.69, 0.65</span><br><span class="line">Tasks: 204 total,   1 running, 203 sleeping,   0 stopped,   0 zombie</span><br><span class="line">%Cpu(s): 20.0 us,  1.7 sy,  0.0 ni, 77.7 id,  0.0 wa,  0.0 hi,  0.7 si,  0.0 st</span><br><span class="line">KiB Mem:   7679792 total,  6646248 used,  1033544 free,   251688 buffers</span><br><span class="line">KiB Swap:        0 total,        0 used,        0 free.  4115536 cached Mem</span><br></pre></td></tr></table></figure>

<p>在 top 命令的输出结果里面，有一行是以 %CPU 开头的。这一行里，有一个叫作 wa 的指标，这个指标就代表着 iowait，也就是 CPU 等待IO 完成操作花费时间占 CPU 的百分比。</p>
<p>下一次，当你自己的服务器遇到性能瓶颈，load 很大的时候，你就可以通过 top 看一看这个指标。</p>
<p>知道了 iowait 很大，那么我们就要去看一看，实际的 I&#x2F;O 操作情况是什么样的。这个时候，你就可以去用  <em>iostat</em>  这个命令了。我们输入 “iostat”，就能够看到实际的硬盘读写情况。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">avg-cpu:  %user   %nice %system %iowait  %steal   %idle</span><br><span class="line">          17.02    0.01    2.18    0.04    0.00   80.76</span><br><span class="line">Device:            tps    kB_read/s    kB_wrtn/s    kB_read    kB_wrtn</span><br><span class="line">sda               1.81         2.02        30.87     706768   10777408</span><br></pre></td></tr></table></figure>

<p>你会看到，这个命令里，不仅有 iowait 这个 CPU 等待时间的百分比，还有一些更加具体的指标了，并且它还是按照你机器上安装的多块不同的硬盘划分的。</p>
<p>这里的 tps 指标，其实就对应着我们上面所说的硬盘的 IOPS 性能。而 kB_read&#x2F;s 和 kB_wrtn&#x2F;s 指标，就对应着我们的数据传输率的指标。</p>
<p>知道实际硬盘读写的 tps、kB_read&#x2F;s 和 kb_wrtn&#x2F;s 的指标，我们基本上可以判断出，机器的性能是不是卡在 I&#x2F;O 上了。那么，接下来，我们就是要找出到底是哪一个进程是这些 I&#x2F;O 读写的来源了。这个时候，你需要 <em>“iotop”</em> 这个命令。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">Total DISK READ :       0.00 B/s | Total DISK WRITE :      15.75 K/s</span><br><span class="line">Actual DISK READ:       0.00 B/s | Actual DISK WRITE:      35.44 K/s</span><br><span class="line">  TID  PRIO  USER     DISK READ  DISK WRITE  SWAPIN     IO&gt;    COMMAND                                             </span><br><span class="line">  104 be/3 root        0.00 B/s    7.88 K/s  0.00 %  0.18 % [jbd2/sda1-8]</span><br><span class="line">  383 be/4 root        0.00 B/s    3.94 K/s  0.00 %  0.00 % rsyslogd -n [rs:main Q:Reg]</span><br><span class="line"> 1514 be/4 www-data    0.00 B/s    3.94 K/s  0.00 %  0.00 % nginx: worker process</span><br></pre></td></tr></table></figure>

<p>通过 iotop 这个命令，你可以看到具体是哪一个进程实际占用了大量 I&#x2F;O，那么你就可以有的放矢，去优化对应的程序了。上面的这些示例里，不管是 wa 也好，tps 也好，它们都很小。那么，接下来，我就给你用 Linux 下，用 stress 命令，来模拟一个高 I&#x2F;O 复杂的情况，来看看这个时候的 iowait 是怎么样的。</p>
<p>通过 top、iostat 以及 iotop，一步一步快速定位服务器端的 I&#x2F;O 带来的性能瓶颈了。</p>
<ul>
<li><p>top    、iostat 查看实际的IO 操作情况</p>
</li>
<li><p>找出哪一些进程是这些 IO 读写的来源，使用 <strong>iotop</strong></p>
</li>
</ul>
<h2 id="总结延伸"><a href="#总结延伸" class="headerlink" title="总结延伸"></a>总结延伸</h2><ul>
<li><p>这一讲里，我们从硬盘的两个核心指标，响应时间和数据传输率，来理解和研究 I&#x2F;O 的性能问题。在顺序读取的情况下，无论是 HDD 硬盘还是 SSD 硬盘，性能看起来都是很不错的。不过，等到进行随机读取测试的时候，硬盘的性能才能见了真章。因为在大部分的应用开发场景下，我们关心的并不是在顺序读写下的数据量，而是每秒钟能够进行输入输出的操作次数，也就是 IOPS 这个核心性能指标 (tps)。</p>
</li>
<li><p>你会发现，即使是使用 PCI Express 接口的 SSD 硬盘，IOPS 也就只是到了 2 万左右。这个性能，和我们 CPU 的每秒 20 亿次操作的能力比起来，可就差得远了。所以很多时候，我们的程序对外响应慢，其实都是 CPU 在等待 I&#x2F;O 操作完成。</p>
</li>
<li><p>在 Linux 下，我们可以通过 top 这样的命令，来看整个服务器的整体负载。在应用响应慢的时候，我们可以先通过这个指令，来看 CPU 是否在等待 I&#x2F;O 完成自己的操作。进一步地，我们可以通过 iostat 这个命令，来看到各个硬盘这个时候的读写情况。而 iotop 这个命令，能够帮助我们定位到到底是哪一个进程在进行大量的 I&#x2F;O 操作。这些命令的组合，可以快速帮你定位到是不是我们的程序遇到了 I&#x2F;O 的瓶颈，以及这些瓶颈来自于哪些程序，你就可以根据定位的结果来优化你自己的程序了。</p>
</li>
</ul>
]]></content>
      <categories>
        <category>计算机组成原理</category>
      </categories>
      <tags>
        <tag>SSD</tag>
      </tags>
  </entry>
  <entry>
    <title>Hexo-Next添加打字特效、鼠标点击特效</title>
    <url>/posts/6a70f096/</url>
    <content><![CDATA[<h1 id="Hexo-Next-添加打字特效、鼠标点击特效"><a href="#Hexo-Next-添加打字特效、鼠标点击特效" class="headerlink" title="Hexo-Next 添加打字特效、鼠标点击特效"></a>Hexo-Next 添加打字特效、鼠标点击特效</h1><p>本文主要介绍<strong>评论框打字特效</strong>以及<strong>鼠标点击特效</strong>的实现。这里我主要介绍四种鼠标点击特效。</p>
<span id="more"></span>

<p>鼠标点击特效脚本，我们将其放在 themes&#x2F;next&#x2F;source&#x2F;js&#x2F;cursor&#x2F; 目录下，并创建fireworks.js文件，具体代码如下：</p>
<h2 id="礼花特效代码："><a href="#礼花特效代码：" class="headerlink" title="礼花特效代码："></a>礼花特效代码：</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">class Circle &#123;</span><br><span class="line">  constructor(&#123; origin, speed, color, angle, context &#125;) &#123;</span><br><span class="line">    this.origin = origin</span><br><span class="line">    this.position = &#123; ...this.origin &#125;</span><br><span class="line">    this.color = color</span><br><span class="line">    this.speed = speed</span><br><span class="line">    this.angle = angle</span><br><span class="line">    this.context = context</span><br><span class="line">    this.renderCount = 0</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  draw() &#123;</span><br><span class="line">    this.context.fillStyle = this.color</span><br><span class="line">    this.context.beginPath()</span><br><span class="line">    this.context.arc(this.position.x, this.position.y, 2, 0, Math.PI * 2)</span><br><span class="line">    this.context.fill()</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  move() &#123;</span><br><span class="line">    this.position.x = (Math.sin(this.angle) * this.speed) + this.position.x</span><br><span class="line">    this.position.y = (Math.cos(this.angle) * this.speed) + this.position.y + (this.renderCount * 0.3)</span><br><span class="line">    this.renderCount++</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">class Boom &#123;</span><br><span class="line">  constructor (&#123; origin, context, circleCount = 16, area &#125;) &#123;</span><br><span class="line">    this.origin = origin</span><br><span class="line">    this.context = context</span><br><span class="line">    this.circleCount = circleCount</span><br><span class="line">    this.area = area</span><br><span class="line">    this.stop = false</span><br><span class="line">    this.circles = []</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  randomArray(range) &#123;</span><br><span class="line">    const length = range.length</span><br><span class="line">    const randomIndex = Math.floor(length * Math.random())</span><br><span class="line">    return range[randomIndex]</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  randomColor() &#123;</span><br><span class="line">    const range = [&#x27;8&#x27;, &#x27;9&#x27;, &#x27;A&#x27;, &#x27;B&#x27;, &#x27;C&#x27;, &#x27;D&#x27;, &#x27;E&#x27;, &#x27;F&#x27;]</span><br><span class="line">    return &#x27;#&#x27; + this.randomArray(range) + this.randomArray(range) + this.randomArray(range) + this.randomArray(range) + this.randomArray(range) + this.randomArray(range)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  randomRange(start, end) &#123;</span><br><span class="line">    return (end - start) * Math.random() + start</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  init() &#123;</span><br><span class="line">    for(let i = 0; i &lt; this.circleCount; i++) &#123;</span><br><span class="line">      const circle = new Circle(&#123;</span><br><span class="line">        context: this.context,</span><br><span class="line">        origin: this.origin,</span><br><span class="line">        color: this.randomColor(),</span><br><span class="line">        angle: this.randomRange(Math.PI - 1, Math.PI + 1),</span><br><span class="line">        speed: this.randomRange(1, 6)</span><br><span class="line">      &#125;)</span><br><span class="line">      this.circles.push(circle)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  move() &#123;</span><br><span class="line">    this.circles.forEach((circle, index) =&gt; &#123;</span><br><span class="line">      if (circle.position.x &gt; this.area.width || circle.position.y &gt; this.area.height) &#123;</span><br><span class="line">        return this.circles.splice(index, 1)</span><br><span class="line">      &#125;</span><br><span class="line">      circle.move()</span><br><span class="line">    &#125;)</span><br><span class="line">    if (this.circles.length == 0) &#123;</span><br><span class="line">      this.stop = true</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  draw() &#123;</span><br><span class="line">    this.circles.forEach(circle =&gt; circle.draw())</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">class CursorSpecialEffects &#123;</span><br><span class="line">  constructor() &#123;</span><br><span class="line">    this.computerCanvas = document.createElement(&#x27;canvas&#x27;)</span><br><span class="line">    this.renderCanvas = document.createElement(&#x27;canvas&#x27;)</span><br><span class="line"></span><br><span class="line">    this.computerContext = this.computerCanvas.getContext(&#x27;2d&#x27;)</span><br><span class="line">    this.renderContext = this.renderCanvas.getContext(&#x27;2d&#x27;)</span><br><span class="line"></span><br><span class="line">    this.globalWidth = window.innerWidth</span><br><span class="line">    this.globalHeight = window.innerHeight</span><br><span class="line"></span><br><span class="line">    this.booms = []</span><br><span class="line">    this.running = false</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  handleMouseDown(e) &#123;</span><br><span class="line">    const boom = new Boom(&#123;</span><br><span class="line">      origin: &#123; x: e.clientX, y: e.clientY &#125;,</span><br><span class="line">      context: this.computerContext,</span><br><span class="line">      area: &#123;</span><br><span class="line">        width: this.globalWidth,</span><br><span class="line">        height: this.globalHeight</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;)</span><br><span class="line">    boom.init()</span><br><span class="line">    this.booms.push(boom)</span><br><span class="line">    this.running || this.run()</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  handlePageHide() &#123;</span><br><span class="line">    this.booms = []</span><br><span class="line">    this.running = false</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  init() &#123;</span><br><span class="line">    const style = this.renderCanvas.style</span><br><span class="line">    style.position = &#x27;fixed&#x27;</span><br><span class="line">    style.top = style.left = 0</span><br><span class="line">    style.zIndex = &#x27;999999999999999999999999999999999999999999&#x27;</span><br><span class="line">    style.pointerEvents = &#x27;none&#x27;</span><br><span class="line"></span><br><span class="line">    style.width = this.renderCanvas.width = this.computerCanvas.width = this.globalWidth</span><br><span class="line">    style.height = this.renderCanvas.height = this.computerCanvas.height = this.globalHeight</span><br><span class="line"></span><br><span class="line">    document.body.append(this.renderCanvas)</span><br><span class="line"></span><br><span class="line">    window.addEventListener(&#x27;mousedown&#x27;, this.handleMouseDown.bind(this))</span><br><span class="line">    window.addEventListener(&#x27;pagehide&#x27;, this.handlePageHide.bind(this))</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  run() &#123;</span><br><span class="line">    this.running = true</span><br><span class="line">    if (this.booms.length == 0) &#123;</span><br><span class="line">      return this.running = false</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    requestAnimationFrame(this.run.bind(this))</span><br><span class="line"></span><br><span class="line">    this.computerContext.clearRect(0, 0, this.globalWidth, this.globalHeight)</span><br><span class="line">    this.renderContext.clearRect(0, 0, this.globalWidth, this.globalHeight)</span><br><span class="line"></span><br><span class="line">    this.booms.forEach((boom, index) =&gt; &#123;</span><br><span class="line">      if (boom.stop) &#123;</span><br><span class="line">        return this.booms.splice(index, 1)</span><br><span class="line">      &#125;</span><br><span class="line">      boom.move()</span><br><span class="line">      boom.draw()</span><br><span class="line">    &#125;)</span><br><span class="line">    this.renderContext.drawImage(this.computerCanvas, 0, 0, this.globalWidth, this.globalHeight)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">const cursorSpecialEffects = new CursorSpecialEffects()</span><br><span class="line">cursorSpecialEffects.init()</span><br></pre></td></tr></table></figure>

<h2 id="爆炸特效"><a href="#爆炸特效" class="headerlink" title="爆炸特效"></a>爆炸特效</h2><p>explosion.min.js代码</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&quot;use strict&quot;;function updateCoords(e)&#123;pointerX=(e.clientX||e.touches[0].clientX)-canvasEl.getBoundingClientRect().left,pointerY=e.clientY||e.touches[0].clientY-canvasEl.getBoundingClientRect().top&#125;function setParticuleDirection(e)&#123;var t=anime.random(0,360)*Math.PI/180,a=anime.random(50,180),n=[-1,1][anime.random(0,1)]*a;return&#123;x:e.x+n*Math.cos(t),y:e.y+n*Math.sin(t)&#125;&#125;function createParticule(e,t)&#123;var a=&#123;&#125;;return a.x=e,a.y=t,a.color=colors[anime.random(0,colors.length-1)],a.radius=anime.random(16,32),a.endPos=setParticuleDirection(a),a.draw=function()&#123;ctx.beginPath(),ctx.arc(a.x,a.y,a.radius,0,2*Math.PI,!0),ctx.fillStyle=a.color,ctx.fill()&#125;,a&#125;function createCircle(e,t)&#123;var a=&#123;&#125;;return a.x=e,a.y=t,a.color=&quot;#F00&quot;,a.radius=.1,a.alpha=.5,a.lineWidth=6,a.draw=function()&#123;ctx.globalAlpha=a.alpha,ctx.beginPath(),ctx.arc(a.x,a.y,a.radius,0,2*Math.PI,!0),ctx.lineWidth=a.lineWidth,ctx.strokeStyle=a.color,ctx.stroke(),ctx.globalAlpha=1&#125;,a&#125;function renderParticule(e)&#123;for(var t=0;t&lt;e.animatables.length;t++)e.animatables[t].target.draw()&#125;function animateParticules(e,t)&#123;for(var a=createCircle(e,t),n=[],i=0;i&lt;numberOfParticules;i++)n.push(createParticule(e,t));anime.timeline().add(&#123;targets:n,x:function(e)&#123;return e.endPos.x&#125;,y:function(e)&#123;return e.endPos.y&#125;,radius:.1,duration:anime.random(1200,1800),easing:&quot;easeOutExpo&quot;,update:renderParticule&#125;).add(&#123;targets:a,radius:anime.random(80,160),lineWidth:0,alpha:&#123;value:0,easing:&quot;linear&quot;,duration:anime.random(600,800)&#125;,duration:anime.random(1200,1800),easing:&quot;easeOutExpo&quot;,update:renderParticule,offset:0&#125;)&#125;function debounce(e,t)&#123;var a;return function()&#123;var n=this,i=arguments;clearTimeout(a),a=setTimeout(function()&#123;e.apply(n,i)&#125;,t)&#125;&#125;var canvasEl=document.querySelector(&quot;.fireworks&quot;);if(canvasEl)&#123;var ctx=canvasEl.getContext(&quot;2d&quot;),numberOfParticules=30,pointerX=0,pointerY=0,tap=&quot;mousedown&quot;,colors=[&quot;#FF1461&quot;,&quot;#18FF92&quot;,&quot;#5A87FF&quot;,&quot;#FBF38C&quot;],setCanvasSize=debounce(function()&#123;canvasEl.width=2*window.innerWidth,canvasEl.height=2*window.innerHeight,canvasEl.style.width=window.innerWidth+&quot;px&quot;,canvasEl.style.height=window.innerHeight+&quot;px&quot;,canvasEl.getContext(&quot;2d&quot;).scale(2,2)&#125;,500),render=anime(&#123;duration:1/0,update:function()&#123;ctx.clearRect(0,0,canvasEl.width,canvasEl.height)&#125;&#125;);document.addEventListener(tap,function(e)&#123;&quot;sidebar&quot;!==e.target.id&amp;&amp;&quot;toggle-sidebar&quot;!==e.target.id&amp;&amp;&quot;A&quot;!==e.target.nodeName&amp;&amp;&quot;IMG&quot;!==e.target.nodeName&amp;&amp;(render.play(),updateCoords(e),animateParticules(pointerX,pointerY))&#125;,!1),setCanvasSize(),window.addEventListener(&quot;resize&quot;,setCanvasSize,!1)&#125;</span><br></pre></td></tr></table></figure>

<h2 id="浮出爱心特效"><a href="#浮出爱心特效" class="headerlink" title="浮出爱心特效"></a>浮出爱心特效</h2><p>love.min.js代码：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">!function(e,t,a)&#123;function n()&#123;c(&quot;.heart&#123;width: 10px;height: 10px;position: fixed;background: #f00;transform: rotate(45deg);-webkit-transform: rotate(45deg);-moz-transform: rotate(45deg);&#125;.heart:after,.heart:before&#123;content: &#x27;&#x27;;width: inherit;height: inherit;background: inherit;border-radius: 50%;-webkit-border-radius: 50%;-moz-border-radius: 50%;position: fixed;&#125;.heart:after&#123;top: -5px;&#125;.heart:before&#123;left: -5px;&#125;&quot;),o(),r()&#125;function r()&#123;for(var e=0;e&lt;d.length;e++)d[e].alpha&lt;=0?(t.body.removeChild(d[e].el),d.splice(e,1)):(d[e].y--,d[e].scale+=.004,d[e].alpha-=.013,d[e].el.style.cssText=&quot;left:&quot;+d[e].x+&quot;px;top:&quot;+d[e].y+&quot;px;opacity:&quot;+d[e].alpha+&quot;;transform:scale(&quot;+d[e].scale+&quot;,&quot;+d[e].scale+&quot;) rotate(45deg);background:&quot;+d[e].color+&quot;;z-index:99999&quot;);requestAnimationFrame(r)&#125;function o()&#123;var t=&quot;function&quot;==typeof e.onclick&amp;&amp;e.onclick;e.onclick=function(e)&#123;t&amp;&amp;t(),i(e)&#125;&#125;function i(e)&#123;var a=t.createElement(&quot;div&quot;);a.className=&quot;heart&quot;,d.push(&#123;el:a,x:e.clientX-5,y:e.clientY-5,scale:1,alpha:1,color:s()&#125;),t.body.appendChild(a)&#125;function c(e)&#123;var a=t.createElement(&quot;style&quot;);a.type=&quot;text/css&quot;;try&#123;a.appendChild(t.createTextNode(e))&#125;catch(t)&#123;a.styleSheet.cssText=e&#125;t.getElementsByTagName(&quot;head&quot;)[0].appendChild(a)&#125;function s()&#123;return&quot;rgb(&quot;+~~(255*Math.random())+&quot;,&quot;+~~(255*Math.random())+&quot;,&quot;+~~(255*Math.random())+&quot;)&quot;&#125;var d=[];e.requestAnimationFrame=function()&#123;return e.requestAnimationFrame||e.webkitRequestAnimationFrame||e.mozRequestAnimationFrame||e.oRequestAnimationFrame||e.msRequestAnimationFrame||function(e)&#123;setTimeout(e,1e3/60)&#125;&#125;(),n()&#125;(window,document);</span><br></pre></td></tr></table></figure>

<h2 id="浮出文字特效"><a href="#浮出文字特效" class="headerlink" title="浮出文字特效"></a>浮出文字特效</h2><p>text.js代码：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">var a_idx = 0;</span><br><span class="line">jQuery(document).ready(function($) &#123;</span><br><span class="line">  $(&quot;body&quot;).click(function(e) &#123;</span><br><span class="line">    var a = new Array(&quot;喜欢我&quot;, &quot;不喜欢我&quot;);</span><br><span class="line">    var $i = $(&quot;&lt;span/&gt;&quot;).text(a[a_idx]);</span><br><span class="line">    var x = e.pageX,</span><br><span class="line">    y = e.pageY;</span><br><span class="line">    $i.css(&#123;</span><br><span class="line">      &quot;z-index&quot;: 99999,</span><br><span class="line">      &quot;top&quot;: y - 28,</span><br><span class="line">      &quot;left&quot;: x - a[a_idx].length * 8,</span><br><span class="line">      &quot;position&quot;: &quot;absolute&quot;,</span><br><span class="line">      &quot;color&quot;: &quot;#ff7a45&quot;</span><br><span class="line">    &#125;);</span><br><span class="line">    $(&quot;body&quot;).append($i);</span><br><span class="line">    $i.animate(&#123;</span><br><span class="line">      &quot;top&quot;: y - 180,</span><br><span class="line">      &quot;opacity&quot;: 0</span><br><span class="line">    &#125;, 1500, function() &#123;</span><br><span class="line">      $i.remove();</span><br><span class="line">    &#125;);</span><br><span class="line">    a_idx = (a_idx + 1) % a.length;</span><br><span class="line">  &#125;);</span><br><span class="line">&#125;);</span><br></pre></td></tr></table></figure>



<p>然后我们在主题自定义布局文件 hexo&#x2F;source&#x2F;_data&#x2F;body-end.swig中，没有则创建，添加以下代码：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&#123;# 鼠标点击特效 #&#125;</span><br><span class="line">&#123;% if theme.cursor_effect == &quot;fireworks&quot; %&#125;</span><br><span class="line">  &lt;script async src=&quot;/js/cursor/fireworks.js&quot;&gt;&lt;/script&gt;</span><br><span class="line">&#123;% elseif theme.cursor_effect == &quot;explosion&quot; %&#125;</span><br><span class="line">  &lt;canvas class=&quot;fireworks&quot; style=&quot;position: fixed;left: 0;top: 0;z-index: 1; pointer-events: none;&quot; &gt;&lt;/canvas&gt;</span><br><span class="line">  &lt;script src=&quot;//cdn.bootcss.com/animejs/2.2.0/anime.min.js&quot;&gt;&lt;/script&gt;</span><br><span class="line">  &lt;script async src=&quot;/js/cursor/explosion.min.js&quot;&gt;&lt;/script&gt;</span><br><span class="line">&#123;% elseif theme.cursor_effect == &quot;love&quot; %&#125;</span><br><span class="line">  &lt;script async src=&quot;/js/cursor/love.min.js&quot;&gt;&lt;/script&gt;</span><br><span class="line">&#123;% elseif theme.cursor_effect == &quot;text&quot; %&#125;</span><br><span class="line">  &lt;script async src=&quot;/js/cursor/text.js&quot;&gt;&lt;/script&gt;</span><br><span class="line">&#123;% endif %&#125;</span><br></pre></td></tr></table></figure>

<p>然后在 NexT 的配置文件 next.yml 中取消 body-end.swig 的注释：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">custom_file_path:</span><br><span class="line">  bodyEnd: source/_data/body-end.swig</span><br></pre></td></tr></table></figure>

<p>四种特效中选择自己想要的效果，在 next.yml 中增加如下配置项：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line"># 鼠标点击特效</span><br><span class="line"># mouse click effect: fireworks | explosion | love | text</span><br><span class="line">cursor_effect: fireworks</span><br><span class="line"></span><br><span class="line"># 打字特效</span><br><span class="line"># typing effect</span><br><span class="line">typing_effect:</span><br><span class="line">  colorful: true  # 礼花特效</span><br><span class="line">  shake: false    # 震动特效</span><br></pre></td></tr></table></figure>


]]></content>
      <categories>
        <category>Hexo博客</category>
      </categories>
      <tags>
        <tag>Next</tag>
      </tags>
  </entry>
  <entry>
    <title>Hexo博客下集成Gitalk评论</title>
    <url>/posts/d37d3bf7/</url>
    <content><![CDATA[<h1 id="Hexo-Next博客下集成-Gitalk-评论功能"><a href="#Hexo-Next博客下集成-Gitalk-评论功能" class="headerlink" title="Hexo+Next博客下集成 Gitalk 评论功能"></a>Hexo+Next博客下集成 Gitalk 评论功能</h1><blockquote>
<p>Gitalk 是基于 GitHub Issues 的评论系统 ，面向程序员，不能匿名评论，需博主初始化话题，用户需登录 github 账号评论。并且支持markdown 格式。</p>
</blockquote>
<span id="more"></span>

<h2 id="Gitalk-功能集成过程："><a href="#Gitalk-功能集成过程：" class="headerlink" title="Gitalk 功能集成过程："></a>Gitalk 功能集成过程：</h2><h3 id="新建评论仓库"><a href="#新建评论仓库" class="headerlink" title="新建评论仓库"></a><strong>新建评论仓库</strong></h3><p>如果 Hexo博客是通过Github的仓库（repositories）部署到远端的话，该仓库即可以作为评论仓库，在setting中启动该仓库的Issues即可。</p>
<p> 否则的话，需要新建一个空的仓库来使用，并且启动该仓库的 Issues。</p>
<h3 id="注册OAuth-Application"><a href="#注册OAuth-Application" class="headerlink" title="注册OAuth Application"></a>注册OAuth Application</h3><ul>
<li><p>进入GitHub官网<a href="https://github.com/">https://github.com/</a> ，点击右上角头像，选择setting.</p>
</li>
<li><p>接着在setting后选择developer setting 。<img src="/img/Next/talk1.png" alt="talk1" style="zoom:67%;" /></p>
</li>
<li><p>然后 new OAuth app</p>
</li>
</ul>
<img src="/img/Next/talk2.png" alt="talk2" style="zoom:67%;" />

<ul>
<li>接着完成注册信息。<br>​                    <img src="/img/Next/talk3.png" alt="talk3" style="zoom: 50%;" /></li>
</ul>
<p>值得注意的是第四个参数信息很重要！是回调URL，这个一定不能填写错，一般填写你博客主页地址。</p>
<p><img src="/img/Next/talk4.png" alt="talk4" style="zoom:67%;" />注册成功之后会有 Client ID和 Client Secret ，需要记录下来之后会用到。</p>
<h3 id="配置文件修改"><a href="#配置文件修改" class="headerlink" title="配置文件修改"></a>配置文件修改</h3><p>第一步：新建或修改<code>/layout/_third-party/comments/gitalk.swig</code>文件，并添加内容（若该文件下载后已存在则无需进行更改，Next8.0 版本后为.njk 后缀结尾文件）。</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&#123;%- if page.comments %&#125;</span><br><span class="line">&#123;%- set gitalk_css_uri = theme.vendors.gitalk_css | default(&#x27;//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.css&#x27;) %&#125;</span><br><span class="line">&lt;link rel=&quot;stylesheet&quot; href=&quot;&#123;&#123; gitalk_css_uri &#125;&#125;&quot;&gt;</span><br><span class="line"></span><br><span class="line">&#123;%- set gitalk_js_uri = theme.vendors.gitalk_js | default(&#x27;//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js&#x27;) %&#125;</span><br><span class="line"></span><br><span class="line">&lt;script&gt;</span><br><span class="line">  NexT.utils.getScript(&#x27;&#123;&#123; gitalk_js_uri &#125;&#125;&#x27;, () =&gt; &#123;</span><br><span class="line">    var gitalk = new Gitalk(&#123;</span><br><span class="line">      clientID: &#x27;&#123;&#123; theme.gitalk.client_id &#125;&#125;&#x27;,</span><br><span class="line">      clientSecret: &#x27;&#123;&#123; theme.gitalk.client_secret &#125;&#125;&#x27;,</span><br><span class="line">      repo: &#x27;&#123;&#123; theme.gitalk.repo &#125;&#125;&#x27;,</span><br><span class="line">      owner: &#x27;&#123;&#123; theme.gitalk.github_id &#125;&#125;&#x27;,</span><br><span class="line">      admin: [&#x27;&#123;&#123; theme.gitalk.admin_user &#125;&#125;&#x27;],</span><br><span class="line">      id: &#x27;&#123;&#123; gitalk_md5(page.path) &#125;&#125;&#x27;,</span><br><span class="line">      &#123;%- if theme.gitalk.language == &#x27;&#x27; %&#125;</span><br><span class="line">        language: window.navigator.language || window.navigator.userLanguage,</span><br><span class="line">      &#123;% else %&#125;</span><br><span class="line">        language: &#x27;&#123;&#123; theme.gitalk.language &#125;&#125;&#x27;,</span><br><span class="line">      &#123;%- endif %&#125;</span><br><span class="line">      distractionFreeMode: &#x27;&#123;&#123; theme.gitalk.distraction_free_mode &#125;&#125;&#x27;</span><br><span class="line">    &#125;);</span><br><span class="line">    gitalk.render(&#x27;gitalk-container&#x27;);</span><br><span class="line">  &#125;, window.Gitalk);</span><br><span class="line">&lt;/script&gt;</span><br><span class="line">&#123;%- endif %&#125;</span><br></pre></td></tr></table></figure>

<p><strong>comments.swig</strong></p>
<p>第二步：修改<code>/layout/_partials/comments.swig</code>文件，在最后一个<code>&#123;%- endif %&#125;</code>前面加上如下内容 (十分重要)：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&#123;% elseif theme.gitalk.enable %&#125;</span><br><span class="line"> &lt;div id=&quot;gitalk-container&quot;&gt;&lt;/div&gt;</span><br></pre></td></tr></table></figure>

<p><strong>index.swig</strong></p>
<p>第三步：新建或者修改<code>layout/_third-party/comments/index.swig</code>文件，在最后添加内容</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">&#123;% include &#x27;gitalk.swig&#x27; %&#125;</span><br></pre></td></tr></table></figure>

<p><strong>gitalk.styl</strong></p>
<p>第四步：新建或者修改<code>/source/css/_common/components/third-party/gitalk.styl</code>文件，添加内容：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">if (hexo-config(&#x27;gitalk.enable&#x27;)) &#123;</span><br><span class="line">  .gt-header a, .gt-comments a, .gt-popup a &#123;</span><br><span class="line">    border-bottom: 0;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  .gt-container .gt-popup .gt-action.is--active::before &#123;</span><br><span class="line">    top: .7em;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p><strong>_config.xml</strong></p>
<p>第五步：在主题配置文件<code>next/_config.xml</code>中找到<code>gitalk处</code>修改或者添加如下内容：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">gitalk:</span><br><span class="line">  enable: true</span><br><span class="line">  githubID: github帐号  # 例：TateTang</span><br><span class="line">  repo: 仓库名称   # 例：blog-comments</span><br><span class="line">  ClientID: Client ID</span><br><span class="line">  ClientSecret: Client Secret</span><br><span class="line">  adminUser: github帐号 #指定可初始化评论账户 如：TateTang</span><br><span class="line">  distractionFreeMode: true</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>这时我们就需要用到之前记录的 Client ID和 Client Secret ，需要注意的是<strong>repo必须是你存放评论并启动Issues的仓库（repository）名称</strong>。</p>
<h3 id="初始化-Issues"><a href="#初始化-Issues" class="headerlink" title="初始化 Issues"></a>初始化 Issues</h3><p>完成上述工作后，即可在博客主目录下输入命令<code>hexo clean &amp;&amp; hexo g &amp;&amp; hexo d</code> 运行</p>
<p>进入博客后在评论会出现下图情况，这时使用注册 OAuth Application 的GitHub登录进行初始化创建即可。   <img src="/img/Next/talk5.png" alt="talk5" style="zoom:67%;" /></p>
<p>首次初始化会有如下结果，点击授权即可完成！（Gitalk Lin 是我OAuth APP注册的项目名称）<br><img src="/img/Next/talk6.png" alt="talk6" style="zoom:67%;" /></p>
<h2 id="注意事项："><a href="#注意事项：" class="headerlink" title="注意事项："></a>注意事项：</h2><ol>
<li><p>有时授权完成后可能显示 <code>Error Not Found</code> 问题，很可能的情况是没有创建或者已经存在 (在Setting中) 启动了 Issues的 GitHub 仓库，或者在 Next主题的配置文件中 <em><strong>repo</strong></em>名称输入有误（请与创建的评论仓库的名称相一致）。</p>
<p>例如： repo: blog-Comments</p>
<p>(blog-Comments 即为创建时的仓库名)</p>
</li>
<li><p>虽然已完成 Gitalk功能的集成，但是需要每篇博客都允许其他人可以评论的话，那么你要在该博客下登陆评论仓库（repo）使用的 GitHub 账号来完成 Issues的初始化！</p>
</li>
</ol>
<h2 id="参考链接："><a href="#参考链接：" class="headerlink" title="参考链接："></a>参考链接：</h2><p>gitalk 参考文档说明 （<a href="https://github.com/gitalk/gitalk/blob/master/readme-cn.md%EF%BC%89">https://github.com/gitalk/gitalk/blob/master/readme-cn.md）</a></p>
]]></content>
      <categories>
        <category>Hexo博客</category>
      </categories>
      <tags>
        <tag>Next</tag>
        <tag>gitalk</tag>
      </tags>
  </entry>
  <entry>
    <title>Hexo 主题下Next主题目录解析</title>
    <url>/posts/dc9e0770/</url>
    <content><![CDATA[<h1 id="Next-主题的-Hexo-博客配置目录结构"><a href="#Next-主题的-Hexo-博客配置目录结构" class="headerlink" title="Next 主题的 Hexo 博客配置目录结构"></a>Next 主题的 Hexo 博客配置目录结构</h1><blockquote>
<p> Next 主题的简约明亮的风格深受广大网友喜爱，其集成功能繁多，主要涉及对主站点的配置以及主题的配置文件进行定制（_config.yml）,为了对Hexo博客下各个子目录有一个大致认知，现将目录结构展示如下：</p>
</blockquote>
<span id="more"></span>

<h2 id="Hexo-博客下默认目录结构"><a href="#Hexo-博客下默认目录结构" class="headerlink" title="Hexo 博客下默认目录结构"></a>Hexo 博客下默认目录结构</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">.</span><br><span class="line">├── .deploy</span><br><span class="line">├── public</span><br><span class="line">├── scaffolds</span><br><span class="line">├── scripts</span><br><span class="line">├── source</span><br><span class="line">|   ├── _drafts</span><br><span class="line">|   └── _posts</span><br><span class="line">├── themes</span><br><span class="line">├── _config.yml</span><br><span class="line">└── package.json</span><br></pre></td></tr></table></figure>

<p>详细对应于：</p>
<ul>
<li>deploy：执行hexo deploy命令部署到GitHub上的内容目录</li>
<li>public：执行hexo generate命令，输出的静态网页内容目录</li>
<li>scaffolds：layout模板文件目录，其中的md文件可以添加编辑</li>
<li>scripts：扩展脚本目录，这里可以自定义一些javascript脚本</li>
<li>source：文章源码目录，该目录下的markdown和html文件均会被hexo处理。该页面对应repo的根目录，404文件、favicon.ico文件，CNAME文件等都应该放这里，该目录下可新建页面目录。<ol>
<li>drafts：草稿文章</li>
<li>posts：发布文章</li>
</ol>
</li>
<li>themes：主题文件目录</li>
<li>_config.yml：全局配置文件，大多数的设置都在这里</li>
<li>package.json：应用程序数据，指明hexo的版本等信息，类似于一般软件中的关于按钮</li>
</ul>
<h2 id="Next主题下的子目录结构"><a href="#Next主题下的子目录结构" class="headerlink" title="Next主题下的子目录结构"></a>Next主题下的子目录结构</h2><figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">├── .github            #git信息</span><br><span class="line">├── languages          #多语言</span><br><span class="line">|   ├── default.yml    #默认语言</span><br><span class="line">|   └── zh-Hans.yml      #简体中文</span><br><span class="line">|   └── zh-tw.yml      #繁体中文</span><br><span class="line">├── layout             #布局，根目录下的*.ejs文件是对主页，分页，存档等的控制</span><br><span class="line">|   ├── _custom        #可以自己修改的模板，覆盖原有模板</span><br><span class="line">|   |   ├── _header.swig    #头部样式</span><br><span class="line">|   |   ├── _sidebar.swig   #侧边栏样式</span><br><span class="line">|   ├── _macro        #可以自己修改的模板，覆盖原有模板</span><br><span class="line">|   |   ├── post.swig    #文章模板</span><br><span class="line">|   |   ├── reward.swig    #打赏模板</span><br><span class="line">|   |   ├── sidebar.swig   #侧边栏模板</span><br><span class="line">|   ├── _partial       #局部的布局</span><br><span class="line">|   |   ├── head       #头部模板</span><br><span class="line">|   |   ├── search     #搜索模板</span><br><span class="line">|   |   ├── share      #分享模板</span><br><span class="line">|   ├── _script        #局部的布局</span><br><span class="line">|   ├── _third-party   #第三方模板</span><br><span class="line">|   ├── _layout.swig   #主页面模板</span><br><span class="line">|   ├── index.swig     #主页面模板</span><br><span class="line">|   ├── page           #页面模板</span><br><span class="line">|   └── tag.swig       #tag模板</span><br><span class="line">├── scripts            #script源码</span><br><span class="line">|   ├── tags           #tags的script源码</span><br><span class="line">|   ├── marge.js       #页面模板</span><br><span class="line">├── source             #源码</span><br><span class="line">|   ├── css            #css源码</span><br><span class="line">|   |   ├── _common    #*.styl基础css</span><br><span class="line">|   |   ├── _custom    #*.styl局部css</span><br><span class="line">|   |   └── _mixins    #mixins的css</span><br><span class="line">|   ├── fonts          #字体</span><br><span class="line">|   ├── images         #图片</span><br><span class="line">|   ├── uploads        #添加的文件</span><br><span class="line">|   └── js             #javascript源代码</span><br><span class="line">├── _config.yml        #主题配置文件</span><br><span class="line">└── README.md          #用GitHub的都知道</span><br></pre></td></tr></table></figure>


]]></content>
      <categories>
        <category>目录集</category>
      </categories>
      <tags>
        <tag>Next</tag>
        <tag>Hexo博客</tag>
      </tags>
  </entry>
  <entry>
    <title>Java中的关键字和标识符号</title>
    <url>/posts/89799f4a/</url>
    <content><![CDATA[<h1 id="Java-的关键字和标识符"><a href="#Java-的关键字和标识符" class="headerlink" title="Java 的关键字和标识符"></a>Java 的关键字和标识符</h1><h2 id="常用关键字展示："><a href="#常用关键字展示：" class="headerlink" title="常用关键字展示："></a>常用关键字展示：</h2><span id="more"></span>
<p>汇总：<br><img src="/img/java_img/java关键字.jpg" alt="java关键字" style="zoom:100%;" /><br>*值得注意的是Java 关键字是区分大小写的哦！”</p>
<h2 id="Java-中的标识符"><a href="#Java-中的标识符" class="headerlink" title="Java 中的标识符"></a>Java 中的标识符</h2><p>标识符是用于给 Java 程序中变量、类、方法等命名的符号。<br>例如：<br><img src="/img/java_img/标识符.png" alt="标识符" style="zoom:68%;" /></p>
<p>注意的是：</p>
<ul>
<li>标识符可以由字母、数字、下划线（_）、美元符（$）组成，但不能包含 @、%、空格等其它特殊字符，不能以数字开头。譬如：123name 就是不合法滴</li>
<li>标识符不能是 Java 关键字和保留字（ Java 预留的关键字，以后的升级版本中有可能作为关键字），但可以包含关键字和保留字。如：不可以使用 void 作为标识符，但是 Myvoid 可以</li>
<li>标识符是严格区分大小写的。标识符的命名最好能反映出其作用，做到见名知意。 所以涅，一定要分清楚 imooc 和 IMooc 是两个不同的标识符哦！</li>
<li>标识符的命名最好能反映出其作用，做到见名知意。</li>
</ul>
]]></content>
      <categories>
        <category>Java</category>
      </categories>
  </entry>
  <entry>
    <title>next主题下为博客添加页面宠物</title>
    <url>/posts/9fbcb947/</url>
    <content><![CDATA[<h1 id="博客添加页面宠物"><a href="#博客添加页面宠物" class="headerlink" title="博客添加页面宠物"></a>博客添加页面宠物</h1><blockquote>
<p>为个人博客添加宠物显示来美化修饰，使用 npm命令来完成在live2d中模型的选择。</p>
</blockquote>
<span id="more"></span>
<h2 id="具体步骤"><a href="#具体步骤" class="headerlink" title="具体步骤"></a>具体步骤</h2><ol>
<li>首先在博客目录下执行<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">npm install -save hexo-helper-live2d</span><br></pre></td></tr></table></figure></li>
<li>然后在站点配置文件中加入:<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">live2d:</span><br><span class="line">  enable: true</span><br><span class="line">  scriptFrom: local</span><br><span class="line">  pluginRootPath: live2dw/</span><br><span class="line">  pluginJsPath: lib/</span><br><span class="line">  pluginModelPath: assets/</span><br><span class="line">  tagMode: false</span><br><span class="line">  model:</span><br><span class="line">    use: live2d-widget-model-wanko  #选择哪种模型</span><br><span class="line">  display: #放置位置和大小</span><br><span class="line">    position: right</span><br><span class="line">    width: 150</span><br><span class="line">    height: 300</span><br><span class="line">  mobile:</span><br><span class="line">    show: false #是否在手机端显示</span><br></pre></td></tr></table></figure></li>
<li>注意：<br>上面模型的选择可在lived2d中选择，并下载相应的模型：</li>
</ol>
<ul>
<li>例如：<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">npm install live2d-widget-model-wanko</span><br></pre></td></tr></table></figure>
<h2 id="可供模型"><a href="#可供模型" class="headerlink" title="可供模型"></a>可供模型</h2></li>
</ul>
<hr>
<p>链接地址：<a href="https://github.com/xiazeyu/live2d-widget-models">live2d</a><br>live2d-widget-model-chitose</p>
<p>live2d-widget-model-epsilon2_1</p>
<p>live2d-widget-model-gf</p>
<p>live2d-widget-model-haru&#x2F;01 (use npm install –save live2d-widget-model-haru)</p>
<p>live2d-widget-model-haru&#x2F;02 (use npm install –save live2d-widget-model-haru)</p>
<p>live2d-widget-model-haruto</p>
<p>live2d-widget-model-hibiki</p>
<p>live2d-widget-model-hijiki</p>
<p>live2d-widget-model-izumi</p>
<p>live2d-widget-model-koharu</p>
<p>live2d-widget-model-miku</p>
<p>live2d-widget-model-ni-j</p>
<p>live2d-widget-model-nico</p>
<p>live2d-widget-model-nietzsche<br>等等···</p>
<ul>
<li>模型展示<br>等等···</li>
</ul>
]]></content>
      <categories>
        <category>Hexo博客</category>
      </categories>
      <tags>
        <tag>Next</tag>
        <tag>博客定制</tag>
      </tags>
  </entry>
  <entry>
    <title>反转链表</title>
    <url>/posts/82b7a1a9/</url>
    <content><![CDATA[<h1 id="关于链表反转"><a href="#关于链表反转" class="headerlink" title="关于链表反转"></a>关于链表反转</h1><h2 id="一、整个链表的反转"><a href="#一、整个链表的反转" class="headerlink" title="一、整个链表的反转"></a>一、整个链表的反转</h2><blockquote>
<p>说明：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">输入: 1 -&gt;2 -&gt;3 -&gt;4 -&gt;5 -&gt;null</span><br><span class="line">输出：null &lt;-1 &lt;-2 &lt;-3&lt;- 4 &lt;-5 </span><br></pre></td></tr></table></figure>
</blockquote>
<span id="more"></span>
<p>先给出其代码吧</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">ListNode reverse(ListNode head)&#123;</span><br><span class="line">	if(head==null || head.next==null)      return head;</span><br><span class="line">	ListNode next=reverse(head.next);</span><br><span class="line">	head.next.next=head;</span><br><span class="line">	head.next=null;</span><br><span class="line">	return next;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><strong>释义:</strong><br><em><strong>输入一个结点<code>head</code>，将 [以<code>head</code>为起点]的链表进行反转，并且返回反转之后的头结点</strong></em><br><img src="/img/codeing/reverseListNode1.png" alt="reverseListNode1" style="zoom: 67%;"/></p>
<p>那么输入<code> reverse(head)</code> 后，会在这里进行递归：<br><code>ListNode last=reverse(head.next) </code> 可以 理解为，<strong>见下图</strong> :<br><img src="/img/coding/reverseListNode2.png" alt="reverseListNode2" style="zoom: 67%;"/></p>
<p>当<code> reverse(head.next)</code>执行完之后，整个链表就会变成:<br><img src="/img/coding/reverseListNode3.png" alt="reverseListNode3" style="zoom: 67%;"/><br>此外， <code>reverse</code> 反转之后的头结点，就会被我们用变量 last 接收了</p>
<ul>
<li>关于<code>head.next.next=head;</code><img src="/img/coding/reverseListNode4.png" alt="reverseListNode4" style="zoom: 67%;"/></li>
</ul>
<p>之后：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">head.next=null;</span><br><span class="line">  return last;</span><br></pre></td></tr></table></figure>
<img src="/img/coding/reverseListNode5.png" alt="reverseListNode5" style="zoom: 67%;"/>
这样，就完成了单链表的反转，值得注意的是***每次 `reverse()` 函数会返回反转之后的头结点！*** 
---

<h2 id="二、进阶-反转前N个节点"><a href="#二、进阶-反转前N个节点" class="headerlink" title="二、进阶 反转前N个节点"></a>二、进阶 反转前N个节点</h2><p>例如我们来实现这样的一个函数</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">//将链表的前n 个节点反转（n&lt;= 链表长度）</span><br><span class="line">  ListNode reverseN(ListNode head,int n)</span><br></pre></td></tr></table></figure>
<img src="/img/coding/reverseListNode21.png" alt="reverseListNode21" style="zoom: 67%;"/>
这需要对之前的代码稍加修改：
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">ListNode successor=null;   //后驱节点</span><br><span class="line">// 反转以 head 为起点的 n 个节点，返回 新的头节点。</span><br><span class="line">ListNode reverseN(ListNode head,int n)&#123;</span><br><span class="line">	if(n==1)&#123;</span><br><span class="line">		successor=head.next;</span><br><span class="line">		return head;</span><br><span class="line">	&#125;</span><br><span class="line">	ListNode last=reverseN(head,n-1);</span><br><span class="line">	head.next.next=head;</span><br><span class="line">	head.next=successor;</span><br><span class="line">	return last;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>之前我们直接把 <code> head.next</code> 设置为 null，因为整个链表反转后原来的 <code>head</code> 变成了整个链表的最后一个节点。但现在 <code>head</code> 节点在递归反转之后不一定是最后一个节点了，所以要记录后驱 <code>successor</code>（第 n + 1 个节点），反转之后将 <code>head</code> 连接上。<br><img src="/img/coding/reverseListNode22.png" alt="reverseListNode22" style="zoom: 67%;"/></p>
<hr>
<h2 id="三、再进阶-反转链表中的一部分"><a href="#三、再进阶-反转链表中的一部分" class="headerlink" title="三、再进阶 反转链表中的一部分"></a>三、再进阶 反转链表中的一部分</h2><p>我们现在来解决对于一个链表中，索引区间在[m,n] 之间的部分进行反转<code>ListNode reverseBetween(ListNode head,int m,int n)</code> 首先，如果 <code>m==1</code> ,就相当于反转链表开头的 n 个元素，也就是：</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">ListNode reverseBetween(ListNode head, int m, int n) &#123;</span><br><span class="line">    // base case</span><br><span class="line">    if (m == 1) &#123;</span><br><span class="line">        // 相当于反转前 n 个元素</span><br><span class="line">     	return reverseN(head,n);</span><br><span class="line">     &#125;</span><br><span class="line">     // ....</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>
<p>那么当<code>m!=1</code> 时呢？ 如果我们把<code>head</code> 的索引视为 1 ，那么是要从第m 个元       素开始反转，索引我们可以将<code>head.next</code> 的索引视为 1 时，那么反转区间相对应的就是从 第 <code>m -1</code> 个元素开始的； 同理可以继续推 <code>head.next.next</code>  ……..</p>
<p><strong>代码如下：</strong></p>
<figure class="highlight text"><table><tr><td class="code"><pre><span class="line">ListNode reverseBetween(ListNode head, int m, int n) &#123;</span><br><span class="line">    // base case</span><br><span class="line">    if(m==1)&#123;</span><br><span class="line">    	return reverseN(head,n);</span><br><span class="line">    &#125;</span><br><span class="line">    // 前进到反转的起点触发 base case</span><br><span class="line">    head.next=reverseBetween(head,m-1,n-1);</span><br><span class="line">    return head;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>LeeCode Coding</category>
      </categories>
      <tags>
        <tag>数据结构</tag>
        <tag>链表</tag>
        <tag>ListNode</tag>
      </tags>
  </entry>
  <entry>
    <title>我的第一篇博客文章</title>
    <url>/posts/b24dd347/</url>
    <content><![CDATA[<h1 id="深入浅出计算机组成原理"><a href="#深入浅出计算机组成原理" class="headerlink" title="深入浅出计算机组成原理"></a>深入浅出计算机组成原理</h1><hr>
<h2 id="入门篇"><a href="#入门篇" class="headerlink" title="入门篇"></a>入门篇</h2><hr>
<ul>
<li>冯诺依曼体系结构</li>
<li>性能与功耗<br/>
<span id="more"></span>
## 指令与运算</li>
</ul>
<hr>
<ul>
<li>指令</li>
<li>函数调用</li>
<li>静态链接与动态链接</li>
<li>二进制编码</li>
<li>电路中的加法器与乘法器</li>
<li>浮点数与定点数<br/></li>
</ul>
<h2 id="处理器"><a href="#处理器" class="headerlink" title="处理器"></a>处理器</h2><hr>
<ul>
<li>建立数据通路</li>
<li>面向流水线的指令设计</li>
<li>冒险与预测</li>
<li>异常与中断</li>
<li>指令集：CISC 和 RISC</li>
<li>GPU</li>
<li>理解虚拟机<br/></li>
</ul>
<h2 id="存储与IO系统"><a href="#存储与IO系统" class="headerlink" title="存储与IO系统"></a>存储与IO系统</h2><hr>
<ul>
<li>数据存储的金字塔结构</li>
<li>局部性原理</li>
<li>高速缓存</li>
<li>理解内存</li>
<li>总线设计</li>
<li>理解IO_WAIT</li>
<li>机械硬盘</li>
<li>SSD硬盘</li>
<li>数据完整性</li>
</ul>
<h2 id="总结与展望"><a href="#总结与展望" class="headerlink" title="总结与展望"></a>总结与展望</h2><ul>
<li>延伸</li>
<li>理解Disruptor</li>
</ul>
]]></content>
      <categories>
        <category>目录集</category>
      </categories>
      <tags>
        <tag>计算机组成</tag>
      </tags>
  </entry>
</search>
